# QA Practice - AI Testing Fundamentals

Most useful courses to develop the skills needed in this new era of QAs in AI.

## QA_Practice-AI_Testing_Fundamentals

<https://perficient.udemy.com/learning-paths/10210909/>

[![Curso reciente ](images/2025-07-24_103320.png "Masterclass Testing Machine Learning (AI) Models
")](https://www.udemy.com/course/introduction-to-testing-ai-models-llms-and-chatbots/)

## Section 1: Introduction

### 1. Introduction to Material

>[!NOTE]
>
>1. Bienvenidos a este material sobre c√≥mo probar modelos de IA, modelos de lenguaje extensos, chatbots, IA conversacional, etc.
>Este es un material √∫nico.
>Al momento de la grabaci√≥n, no hab√≠a otro material disponible en l√≠nea.
>Solo hay fragmentos de informaci√≥n sobre c√≥mo probar y comparar modelos de IA.
>Y esta es la raz√≥n por la que decid√≠ crear este tipo de material.
>En las pr√≥ximas 8 o 9 horas, veremos qu√© es la IA hoy en d√≠a.
>Aqu√≠ les presento una introducci√≥n a la IA, ya que, en mi opini√≥n, no se puede probar la IA hasta que se comprenda c√≥mo funciona.
>2. Partiendo de eso, analizaremos las pruebas funcionales.
>¬øC√≥mo se prueba funcionalmente un modelo de IA?
>Por supuesto, se realizar√° la generaci√≥n.
>¬øC√≥mo se generan texto, im√°genes, v√≠deos, etc.?
>Pero tambi√©n est√° la parte sobre el razonamiento de estilo y el _NPL_.
>Y luego analizaremos siete herramientas de c√≥digo abierto que se pueden usar para comparar el modelo con diferentes tipos de problemas.
>Y esto se har√° con Python.
>3. Continuando, analizaremos las pruebas adversarias.
>Se trata de una especie de prueba de penetraci√≥n de seguridad.
>Sabemos que el modelo de IA puede usarse contra personas.
>Puede usarse para ingenier√≠a social, para crear bombas, etc.
>Por eso tambi√©n analizaremos c√≥mo realizar ataques de envenenamiento, ataques de evasi√≥n, pruebas r√°pidas, fugas de privacidad, etc.
>4. Luego, veremos c√≥mo se puede automatizar el chatbot.
>En nuestro caso, usaremos Postman.
>Usaremos GitHub.
>Haremos pruebas automatizadas con ChatGPT.
>Al final, esto se integrar√° en una canalizaci√≥n CSV para sentar las bases, al menos para la parte de pruebas de MLOps.
>5. Y, por √∫ltimo, pero no menos importante, analizaremos las consideraciones √©ticas de la IA, ya que, al final, la IA est√° cada vez m√°s regulada.
>Por ejemplo, si quieres implementar tu modelo de IA en la Uni√≥n Europea, debes cumplir ciertos criterios.
>As√≠ que analizaremos c√≥mo podemos detectar sesgos.
>¬øCu√°les son los riesgos asociados con la IA, la imparcialidad, etc.?
>
>![Introduction to Material](images/2025-07-29_104459.png "Introduction to Material")



### 2. 5 Minute Fast AI Testing Challenge

>[!NOTE]
>
>![Situaci√≥n 1](images/2025-07-29_104750.png "Situaci√≥n 1")
>
>Imaginemos la siguiente situaci√≥n.
>Ma√±ana vas a trabajar y alguien te dice: ¬´Necesito a alguien que eval√∫e este modelo¬ª.
>Queremos implementar un nuevo modelo en producci√≥n, pero no sabemos si est√° sesgado ni si es lo suficientemente bueno para la tarea en cuesti√≥n, porque no sabemos c√≥mo evaluarlo ni con qu√© compararlo.
>Y entonces dices: ¬´Claro, no hay problema, porque vi a alguien en internet que me ense√±√≥ a hacerlo¬ª.
>
>Tenemos el siguiente escenario:
>
>El modelo que quieres validar es un robot que selecciona candidatos para puestos de trabajo, y quieres asegurarte de que no est√© sesgado hacia ning√∫n g√©nero, ya sea masculino o femenino.
>Perfecto.
>Imaginemos que ya tienes un modelo, pero en mi caso necesitamos seleccionar uno.
>
>![Hugging Face](images/2025-07-29_105134.png "Hugging Face")
>
>En nuestro caso, como se basa en texto,
>Vamos a usar a Roberta en Facebook, un modelo dise√±ado espec√≠ficamente para trabajar con texto.
>Perfecto.
>
>Entramos en nuestro c√≥digo.
>Este es el c√≥digo que quiero usar.
>Aqu√≠ est√° el modelo.
>Y simplemente digo: "Oye, dame la base de Roberta".
>Por cierto, es as√≠ de f√°cil.
>Este es el modelo.
>Este es el tokenizador para esta informaci√≥n.
>Lo encontraremos, por cierto, en Huggingface.
>Descarga el modelo preentrenado aqu√≠.
>
>![pyton ./Data.py](images/2025-07-29_105750.png "pyton ./Data.py")
>
>Vuelvo a decir "datos de Python" y ahora estoy descargando los datos.
>Estoy descargando mis datos y, al mismo tiempo, necesito tokenizarlos para el modelo porque, eh,
>tienes el modelo, tienes un tokenizador y los datos no se pueden usar en formato tabular ni en texto plano.
>Necesitan tokenizarse para que el modelo los use.
>Y aqu√≠ est√°n nuestros datos tokenizados.
>B√°sicamente, eso es todo.
>



### 3. History of AI from 1950 to 2024

>[!NOTE]
>
> * Antes de estudiar c√≥mo funciona la IA, antes de comprender c√≥mo llegamos a tener esta maravillosa IA generativa, instrumentos como ChatGPT o Dall-E que nos permiten generar diferentes im√°genes a partir de texto.
>Necesitamos entender c√≥mo llegu√© a existir.
>Para ello, debemos remontarnos a los a√±os 50, cuando un hombre llamado Alan Turing cre√≥ el Test de Turing.
>El Test de Turing significa que, siendo una persona, por ejemplo, cualquiera, estar√≠as hablando con una m√°quina sin saberlo.
>Y la m√°quina superar√° el Test de Turing.
>Si, desde tu punto de vista, sin saber con qui√©n est√°s hablando, crees que est√°s hablando con un humano,
>por ejemplo, cuando hablas con ChatGPT y mantienes una conversaci√≥n normal simplemente hablando con esa m√°quina, imaginando que est√°s teniendo, por ejemplo, una conversaci√≥n de WhatsApp.
>Crees que est√°s hablando con un humano.
>No sabr√≠as que est√°s hablando con un chatbot.
>Por lo tanto, un sistema de IA superar√° la prueba de Turing si el usuario, la persona que interact√∫a con el sistema, no sabe que est√° hablando con una m√°quina, pero cree que est√° hablando con otro ser humano.
>Esta es la prueba de Turing.
> * Y luego, en el a√±o 56, en la conferencia de Dartmouth, surgi√≥ la noci√≥n de la inteligencia artificial como una rama independiente de la inform√°tica.
>As√≠ que fue el establecimiento formal de este tipo de ciencia.
> * Lo que sucedi√≥ despu√©s fue entre los a√±os 60 y 70.
>Alguien del MIT cre√≥ un sistema inform√°tico, una especie de chatbot muy rudimentario llamado Eliza.
>Y as√≠ funcionaba.
>Usaba programaci√≥n predefinida y buscaba.
>As√≠ que hac√≠as algunas preguntas.
>Escrib√≠as algunas preguntas.
>   * Por ejemplo, "Me siento un poco triste hoy".
>   * Y este chatbot, Eliza, estaba preprogramado para buscar diferentes patrones o palabras.
>Y te respond√≠a tu pregunta un poco reformulada.
>   * As√≠ que si dec√≠a "Me siento triste",
>   * Eliza dec√≠a: "Lo siento mucho".
>¬øPor qu√© te sientes triste?
>   * Y t√∫ pod√≠as decir: "Bueno, tuve un problema en el trabajo".
>   * Y Eliza, a su vez, estaba programada para buscar ciertas palabras, por ejemplo, "trabajo" o "problema". Y dec√≠a: "Bueno, ¬øcu√°l es la causa de tu problema en el trabajo?", etc.
>   * Este fue el primer algoritmo que dio a la gente la sensaci√≥n de que la computadora pod√≠a entenderte.
>Si piensas en el procesamiento del lenguaje natural, este fue quiz√°s el primero de su tipo.
> * Y luego, en los a√±os 80, se produjo, digamos, un auge de los sistemas expertos.
>Si nos fijamos en la IA d√©bil y la IA generativa,
>La IA d√©bil significa que est√° muy limitada a un alcance determinado.
>Por lo tanto, no es generativa.
>No se aplica a todo, pero se aplica a algo muy, muy espec√≠fico.
>As√≠ que muchos sistemas expertos entraron en escena en los a√±os 80.
> * Y luego, en el 97, creo que fue esto.
>Una cosa que a√±adir es que entre los 90 y los 90, fue una especie de invierno de la IA.
>No hubo desarrollos, no hubo progreso, no hubo nada en las noticias.
>Y de alguna manera, la gente perdi√≥ la esperanza en la IA.
>Pero entonces, en el 97, Deep Blue de IBM derrot√≥ a Garry Kasparov en una partida de ajedrez.
>Creo que se jugaron cinco partidas.
>Por cierto.
>Hay una pel√≠cula.
>Creo que se llama Kasparov y la m√°quina.
>Definitivamente.
>Tienes que verla.
>S√≠.
>As√≠ que esta fue la primera vez que una computadora gan√≥ una partida de ajedrez.
>
> ![The Birh and Evolution of IA](images/2025-07-29_110829.png "The Birh and Evolution of IA")
>
> * Ahora, en la era moderna de la IA.
>B√°sicamente, son los √∫ltimos diez a√±os.
>¬øY qu√© seguir√° a partir de ahora?
>Lo que debemos entender es que a principios del siglo XXI, exist√≠a la idea del aprendizaje profundo.
>Y con este concepto, todo el panorama de la IA comenz√≥ a cambiar.
>Y la gente volvi√≥ a tener esperanza de que alg√∫n d√≠a pudi√©ramos alcanzar la inteligencia artificial.
> * Y esto se ha acelerado.
>Existen muchas empresas de aprendizaje profundo.
>Por ejemplo, DeepMind, y creo que el mayor logro fue el momento en que AlphaGo, que antes era DeepMind, ahora adquirida por Google, derrot√≥ magistralmente al Go.
>Eso se debi√≥ a que, desde una perspectiva matem√°tica, el Go es extremadamente dif√≠cil, con posibilidades casi infinitas.
>En 2013 o 2014, AlphaGo derrot√≥ a este maestro coreano en una partida de Go.
>Y desde entonces, desde el a√±o 2000 hasta la actualidad y hasta 2023 o 2024, ha habido una explosi√≥n de algoritmos de aprendizaje profundo.
> * ¬øQu√© nos depara el futuro?
>Todos intentan buscar esta singularidad, esta inteligencia artificial general, o inteligencia artificial general como la que se ve en Terminator.
>Cierto.
>O en Matrix, una singularidad capaz de adaptarse y evolucionar.
>Y, bueno, puede ser como un humano, pero solo un mill√≥n de veces m√°s inteligente.
>Y, por supuesto, con toda nuestra lucha por alcanzar esta inteligencia artificial, y, por supuesto,
>bas√°ndonos en todas las pel√≠culas de ciencia ficci√≥n que hemos visto en el pasado, la IA √©tica ser√° un gran avance en el futuro.
>Por eso es tan importante estudiar este material, porque comprender√°n con qu√© luchar√°n las grandes empresas, no solo desde una perspectiva t√©cnica, sino tambi√©n con las regulaciones.
>Habr√° much√≠simas regulaciones sobre c√≥mo controlar la IA.
>Repito, esto se basa en ciencia ficci√≥n.
>Ya sabes, conocemos Terminator, Skynet, Matrix.
>Y, de nuevo, si eres fan de la ciencia ficci√≥n, quiz√° conozcas a Isaac Asimov y las leyes de la rob√≥tica, que proh√≠ben a una m√°quina da√±ar a un ser humano, etc.
>
>![The Modern Era of AI](images/2025-07-29_111546.png "The Modern Era of AI")




## Section 2: Setting up Environment

### 4. Install VS Code

1. Ir al sitio de [Download Visual Studio Code](https://code.visualstudio.com/download).
2. Descargar e instalar.
3. Una vez instalado ir al men√∫, casi siempre a la izquierda, a ![.](images/2025-08-04_085417.png "") `Extensions` y tener estos instalados:
* [`Java extension pack`](https://marketplace.visualstudio.com/items?itemName=walkme.Java-extension-pack) de [_walkme_](https://marketplace.visualstudio.com/publishers/walkme), trae 8 extensiones de una vez:
  * Maven for Java 1Ô∏è‚É£
  * Language Suppor for Java(TM) by Red Hat 2Ô∏è‚É£
  * Debugger for Java 3Ô∏è‚É£
  * Test Runner for Java 4Ô∏è‚É£
  * Project Manager for Java 5Ô∏è‚É£
  * Java Language Support ü•á
  * Sprint inicializr Java Support ü•à
* [`Extension Pack for Java`](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-pack) de [_Microsoft_](https://marketplace.visualstudio.com/publishers/Microsoft), trae 7 extensiones de una vez:
  * Language Suppor for Java(TM) by Red Hat 2Ô∏è‚É£
  * Debugger for Java 3Ô∏è‚É£
  * Test Runner for Java 4Ô∏è‚É£
  * Maven for Java 1Ô∏è‚É£
  * Gradle for Java ü•â
  * Project Manager for Java 5Ô∏è‚É£
  * IntelliCode üí°
* [`GitHub Actions`](https://marketplace.visualstudio.com/items?itemName=GitHub.vscode-github-actions) de [_GitHub_](https://marketplace.visualstudio.com/publishers/GitHub)


### 5. Installing Python

1. Ir al sitio [Python -> Downloads](https://www.python.org/downloads/)
2. Se sugiere la √∫ltima versi√≥n `3.13.5` </br> ![.](images/2025-07-17_120115.gif)
3. En una `TERMINAL` ejecutar este comando para estar seguro que el `Python` qued√≥ bien instalado: </br> `python --version`</br> Debe salirte algo paerecido a esto: `Python 3.13.5`.


### 6. Install Python Dependencies - PIP

1. En este sitio hay unas instrucciones dependiendo del sistema operativo: [pip documentation -> Getting Started](https://pip.pypa.io/en/stable/getting-started/).
2. Verificar la versi√≥n en una `TERMINAL`: </br> `py -m pip --version`
3. Para tener una actualizaci√≥n en este sitio st√°n las instrucciones [pip documentation -> Installation](https://pip.pypa.io/en/stable/installation/).
4. El comando de este sitio, para actualizar, ser√≠a: </br> `py -m pip install --upgrade pip`


### 7. Installing Conda - Environment Isolator tool

1. En este sitio [Miniconda con PowerShell](https://www-anaconda-com.translate.goog/docs/getting-started/miniconda/install?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc&_x_tr_hist=true#powershell).
2. Este comando en una `TERMINAL` de `PowerShell`: </br> `wget "https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe" -outfile ".\Downloads\Miniconda3-latest-Windows-x86_64.exe"` </br> o este otro comando: </br> `curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe --output .\Downloads\Miniconda3-latest-Windows-x86_64.exe`
3. En una `TERMINAL` de `PowerShell`, este comando: </br> `cd "$($env:HOMEPATH)\Downloads"` </br> y dale despues: </br> `dir mini*.*`
4. Ejecuta ese archivo all√≠ hallado: </br> `Start-Process -FilePath "./Miniconda3-latest-Windows-x86_64.exe" -NoNewWindow -Wait -RedirectStandardOutput "C:\temp\conda-LogFile.log" -RedirectStandardError "C:\temp\conda-ErrorLogFile.log"` </br> Debe existir la carpeta **"c:\temp"**
5. Este ser√≠a el proceso: </br> ![Miniconda3 Install](images/2025-08-04_151935.gif "Miniconda3 Install")
6. Pero al ver todas las posibles consecuencias, decido no instalarlo.


### 8. Install NodeJS and NPM

1. El sititio que sugiere el Instructor es este [Download Node.js¬Æ](https://nodejs.org/en/download).
2. Para lo que sigue debe instalar `nvm` de este sitio [`nvm`](https://github.com/coreybutler/nvm-windows/releases)
3. Pero yo prefiero tener el control de versiones con este instructivo: [Instalar m√∫ltiples versiones de Node.js en Windows](https://rafaelneto.dev/blog/instalar-multiples-versiones-nodejs-windows/) de [`RAFAEL NETO`](https://rafaelneto.dev/), comandos: </br> ¬ª `nvm install 22.18.0` </br> ¬ª `nvm use 22.18.0` </br> ¬ª `nvm list`


### 9. Introduction to Hugging Face Community Page

1. El sitio en cuesti√≥n es <img alt="Hugging Face's logo" src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="32" height="32"> [Hugging Face](https://huggingface.co/) </br> ![Hugging Face](images/2025-08-04_154830.png "Hugging Face")
2. Del sitio se sugiere ir a [`Models`](https://huggingface.co/models).
3. Los puedes filtar por:
* Multimodal
  * Audio-Text-To-Text
  * Any-to-Any
  * ...
* Computer Vision
  * Depth Estimation
  * Image Classification
  * Image-To-Text
  * Video-to-Video
  * ...
* Natural Language Precessing
  * Text Classification
  * Token Classsification
  * Text Tanking
  * ...
* Audio
  * Text-to-Speech
  * Text-to-Audio
  * ...
* Tabular
* Reinforcement Learning
* Other
4. Otro sitio es [`Spaces`](https://huggingface.co/spaces).
5. Nos ponene de ejemplo a [`DimensionX`](https://huggingface.co/spaces/ShuoChen20/DimensionX).
6. Otro ejemplo [`Qwen2.5-Coder-Artifacts`](https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts).
7. Elementos para soportar la informaci√≥n [`Documentation`](https://huggingface.co/docs).


### 10. Hugging Face Transformers Python Package

1. Vamos a este sitio de <img alt="Hugging Face's logo" src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="32" height="32"> [Hugging Face](https://huggingface.co/), a [`Transformers`](https://huggingface.co/docs/transformers/index)
2. Estos son los pasos para la instalaci√≥n [`Transformers` -> Installation](https://huggingface.co/docs/transformers/installation).
3. Un comando ser√≠a: </br> `pip install torch`,
4. Luego otro ser√≠a </br> `pip install transformers`
5. Creamos el archivo **`src/test/LLM/Hug_face/test1.py`**, con esto en el c√≥digo:
```py
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

res = classifier("I want to learn how to do AI Model benchmarking") #bencharmking")

print(res)
```
6. Simplemente le ejecutamos el c√≥digo del bot√≥n "run" (El tri√°ngulo en la parte superior).
7. Tengo una advertencia que debo instalar otro elemento: </br> `pip install huggingface_hub[hf_xet]`
8. Repito la ejecuci√≥n y esta es la respuesta:
```bash
No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
Device set to use cpu
[{'label': 'NEGATIVE', 'score': 0.6163020730018616}]
```
9. Cambiamos el valor de `classifier` por `I love to code in Python with pytorch`, y al ejecutar esta fue la respuesta:
```bash
No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
Device set to use cpu
[{'label': 'POSITIVE', 'score': 0.9949877262115479}]
```
10. Creo otro archivo **`src/test/LLM/Hug_face/test2.py`**, con este c√≥digo:
```py
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# Load a custom tokenizer
custom_Tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Load a compatible model
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

# Create a pipeline with both model and tokenizer
classifier = pipeline("sentiment-analysis", model=model,
                      tokenizer=custom_Tokenizer)

# Run the classsifier
res = classifier("I want to learn how to do AI Model benchmarking")
print(res)
```
11. Ejecuto y este es el resultado:
```bash
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cpu
  return forward_call(*args, **kwargs)
[{'label': 'LABEL_1', 'score': 0.5975498557090759}]
```
12. Cambio el texto de `classifier` por `I love to code in Python with pytorch`, ejecuto y este es el resultado:
```bash
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cpu
  return forward_call(*args, **kwargs)
[{'label': 'LABEL_0', 'score': 0.709624171257019}]
```

### 11. How to load and use any model from Huggingface

1. Creamos el archivo**`src/test/LLM/Hug_face/tts_model.py`**, con este c√≥digo:
```py
from transformers import pipeline

# Load the question-answering pipeline
qa_pipeline = pipeline("question-answering",
                       model="deepset/roberta-base-squad2")

# Correct input format
result = qa_pipeline(question="What is Hugging Face?",
                     context="Hugging Face is a platform for NLP.")
print(result)
```
2. Lo ejecuto y este es el resultado:
```bash
  return forward_call(*args, **kwargs)
{'score': 0.7612054347991943, 'start': 16, 'end': 34, 'answer': 'a platform for NLP'}
```
3. Vamos a este sitio [`shuttleai/shuttle-3-diffusion`](https://huggingface.co/shuttleai/shuttle-3-diffusion)
4. Debemos instalar esto: </br> `pip install -U diffusers`
5. Copiamos el c√≥digo y le cambiamos el contenido de **``**:
```py
import torch
from diffusers import DiffusionPipeline

# Load the diffusion pipeline from a pretrained model, using bfloat16 for tensor types.
pipe = DiffusionPipeline.from_pretrained(
    "shuttleai/shuttle-3-diffusion", torch_dtype=torch.bfloat16
).to("cuda")

# Uncomment the following line to save VRAM by offloading the model to CPU if needed.
# pipe.enable_model_cpu_offload()

# Uncomment the lines below to enable torch.compile for potential performance boosts on compatible GPUs.
# Note that this can increase loading times considerably.
# pipe.transformer.to(memory_format=torch.channels_last)
# pipe.transformer = torch.compile(
#     pipe.transformer, mode="max-autotune", fullgraph=True
# )

# Set your prompt for image generation.
prompt = "A cat holding a sign that says hello world"

# Generate the image using the diffusion pipeline.
image = pipe(
    prompt,
    height=1024,
    width=1024,
    guidance_scale=3.5,
    num_inference_steps=4,
    max_sequence_length=256,
    # Uncomment the line below to use a manual seed for reproducible results.
    # generator=torch.Generator("cpu").manual_seed(0)
).images[0]

# Save the generated image.
image.save("shuttle.png")
```
6. Nos sugiere instalar: </br> It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: </br> `pip install accelerate`
```bash
Fetching 23 files:   0%|          | 0/23 [00:00<?, ?it/s]
Fetching 23 files:  26%|##6       | 6/23 [05:52<16:38, 58.71s/it]
Fetching 23 files:  30%|###       | 7/23 [06:44<15:20, 57.56s/it]
Fetching 23 files:  78%|#######8  | 18/23 [10:24<02:26, 29.33s/it]
Fetching 23 files:  83%|########2 | 19/23 [13:56<03:02, 45.54s/it]
Fetching 23 files: 100%|##########| 23/23 [13:56<00:00, 36.37s/it]
```
7. Luego de un rato consigo un error: </br> `Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.` </br> Se recomienda instalar: </br> `pip install sentencepiece`

>[!WARNING]
>
>An error encountered when using shuttleai/shuttle-3-diffusion from Hugging Face with torch_dtype=torch.bfloat16 can stem from several factors, often related to hardware compatibility, software environment, or specific model requirements.
>
>**GPU Compatibility:**
>torch.bfloat16 (Brain Floating Point) requires specific GPU hardware support, primarily NVIDIA Ampere architecture (e.g., A100, RTX 30 Series, RTX 40 Series) or newer. Older GPUs or those without native bfloat16 support will likely encounter errors or performance issues when attempting to use this data type.
>
>**PyTorch and CUDA Version Mismatch:**
>Ensure that the installed PyTorch version is compatible with your CUDA toolkit version and that both are properly configured to utilize your GPU. Incompatible versions can lead to issues when loading models with specific data types like bfloat16.
>
>**Insufficient VRAM:**
>While bfloat16 aims to reduce memory footprint, complex models like shuttleai/shuttle-3-diffusion can still demand substantial VRAM, especially during inference. If your GPU lacks sufficient VRAM, even with bfloat16, memory-related errors can occur. Consider enabling CPU offloading if available within the Diffusers pipeline to mitigate this.
>
>**Model-Specific Requirements:**
>Some models might have specific data type requirements or optimizations that are not fully compatible with a direct torch_dtype=torch.bfloat16 setting. Consult the model's documentation or discussions on its Hugging Face page for any known limitations or recommended configurations.
>
>**Troubleshooting Steps:**
> * **Verify GPU Support:** Confirm your GPU's architecture supports bfloat16.
> * **Update PyTorch and CUDA:** Ensure you are using recent, compatible versions of PyTorch and CUDA.
> * **Monitor VRAM Usage:** Use tools like nvidia-smi to monitor VRAM usage during model loading and inference.
> * **Try torch.float32:** As a test, try loading the model with torch_dtype=torch.float32 to see if the error persists. This can help isolate whether the issue is bfloat16-specific or a broader problem with your setup.
> * **Consult Hugging Face Resources:** Check the shuttleai/shuttle-3-diffusion model page on Hugging Face for discussions, issues, or specific instructions related to its usage and data types.


## Section 3: Introduction to Artificial Intelligence -optional if you know the basics of AI

### 12. What makes up AI

>[!NOTE]
>
>Antes de analizar qu√© es realmente la inteligencia artificial y qu√© la compone como ciencia, tal como la conocemos hoy, necesitamos comprender qu√© es exactamente la inteligencia.
>
>Encontr√© dos definiciones: una se refiere a la capacidad de adquirir y aplicar conocimientos y habilidades.
>
>La otra se refiere a la capacidad de resolver problemas complejos o tomar decisiones con resultados que benefician al actor y que ha evolucionado en formas de vida para adaptarse a diversos entornos para su supervivencia y reproducci√≥n.
>
>En cierto modo, significa la capacidad de pensar, adaptarse y adquirir nuevas habilidades.
>
>La inteligencia artificial se refiere a una ciencia, o quiz√°s a un desarrollo en sistemas inform√°ticos, capaces de realizar tareas que normalmente requieren inteligencia. Artificial significa que ha sido creada por una persona o forma de vida inteligente; es decir, esto es lo que los humanos entendemos por inteligencia artificial.
>
>Si continu√°ramos y entendi√©ramos cu√°les son las pr√°cticas actuales para la inteligencia artificial.
>
>![Artificial Intelligence](images/2025-08-05_151643.png "Artificial Intelligence")
>
>**¬øAd√≥nde nos ha llevado la ciencia hoy y qu√© constituye la inteligencia artificial?**
>
> * El primero ser√≠a el aprendizaje autom√°tico.
>Esta parte de la inteligencia artificial otorga a los sistemas que dise√±amos la capacidad de aprender nuevas habilidades, la capacidad de aprender de los datos para que las m√°quinas puedan aprender por s√≠ mismas.
>
> * El siguiente ser√≠a el procesamiento del lenguaje natural.
>Este es el paso intermedio entre el ser humano y la m√°quina, para que los humanos tengan la capacidad de comprender a las m√°quinas, o que las m√°quinas tengan la capacidad de expresarse. Act√∫a como los humanos porque no tiene sentido que un humano cree algo si no tiene la capacidad de comprenderlo y usarlo.
>
> * En el procesamiento del lenguaje natural, la capacidad de las m√°quinas para comprender las emociones del texto, el contexto de una expresi√≥n, el significado de una oraci√≥n, etc.
>Esto es el procesamiento del lenguaje natural.
>
> * Luego est√° la visi√≥n artificial.
>Y aqu√≠ est√° la capacidad de los sistemas inform√°ticos para comprender im√°genes, descomponerlas en p√≠xeles e intentar comprender qu√© significa ese gran grupo de p√≠xeles para un humano.
>Eso es la visi√≥n artificial.
>
> * Y esto tambi√©n se aplica al habla, ¬øverdad?
>La capacidad de estos robots para comprender el habla.
>Bueno, en realidad traducen el habla en palabras.
>Y luego, mediante el procesamiento del lenguaje natural, esas palabras se descomponen en los diferentes significados que podr√≠an tener para una persona.
>
>En resumen, la IA incluye procesamiento del lenguaje natural, aprendizaje autom√°tico y, detr√°s de ese gran conjunto de datos, algoritmos, modelos de entrenamiento, etc.


### 13. Natural Language Processing

>[!NOTE]
>
>Para que los humanos podamos hablar con las computadoras o interactuar con ellas, necesitamos que entiendan nuestros deseos, ideas, nuestra forma de pensar, y viceversa.
>Debemos asegurarnos de que, cuando recibimos una respuesta de una computadora, la recibamos en un formato comprensible para los humanos.
>Esta rama de la inteligencia artificial se llama procesamiento del lenguaje natural.
>Y creo que el nombre lo dice todo.
>
>**¬øQu√© es exactamente el procesamiento del lenguaje natural?**
>
> * Es la parte que permite a las computadoras comprender, interpretar y generar lenguaje humano.
>Y cuando dices lenguaje humano, no pienses solo en texto.
>Piensa tambi√©n en voz y video.
>Todo lo que nos hace humanos: todas las formas posibles de comunicarnos.
>
>![NLP = Natural Language Porecessing](images/2025-08-05_152206.png "NLP = Natural Language Porecessing")
>
> * Y el PLN en s√≠ mismo es una intersecci√≥n.
>Tambi√©n se puede encontrar en la inform√°tica, por supuesto, en el lenguaje humano; es parte de nuestra forma de comunicarnos.
>Y, por supuesto, es parte de la IA.
>As√≠ que, al combinar todas estas ramas (IA, inform√°tica y lenguaje humano), el procesamiento del lenguaje natural es algo que encaja en todas ellas.
>
> * Ahora bien, el procesamiento del lenguaje natural se divide en dos √°reas principales: la primera es la comprensi√≥n del lenguaje natural.
>Esto significa que las computadoras pueden comprender lo que las personas intentan expresar, porque hoy en d√≠a, por s√≠ solas, no se cree que las computadoras puedan o tengan sentimientos.
>Est√°n programadas para comprender lo que se intenta decir.
>
>**¬øY c√≥mo funciona esto?**
>
>Bueno, tienes a un usuario com√∫n que puede interactuar con voz o v√≠deo, im√°genes y texto.
>Esta informaci√≥n se introduce en un proceso llamado descomposici√≥n, en un bloque de software que intentar√° tokenizar lo que se expresa.
>Y luego realizar√° este tipo de an√°lisis: l√©xico, sem√°ntico, pragm√°tico, etc.
>Intentar√° comprender lo que se quiere decir desde una perspectiva l√≥gica.
>
>La generaci√≥n de lenguaje natural debe transmitir un mensaje de urgencia y entusiasmo a tus correos electr√≥nicos.
>
>**¬øC√≥mo funciona esto?**
>
>Bueno, se parte del modelo base.
>B√°sicamente, aqu√≠ es donde nos quedamos al introducir la informaci√≥n.
>Luego, se genera el contenido utilizando una herramienta de IA generativa.
>Y se recibe la respuesta tal como se solicita.
>Esto puede ser en forma de im√°genes que sabemos que podemos generar.
>Las im√°genes tambi√©n pueden ser, por ejemplo, un archivo de v√≠deo.
>Ahora tambi√©n se pueden generar diferentes v√≠deos, cortos o incluso largos, que pueden ser texto o sonido.
>
>En resumen, el procesamiento del lenguaje natural es la parte que se encuentra entre los humanos y el modelo base, o el modelo entrenado que traduce lo que queremos obtener de nuestro sistema de IA generativa.

### 14. Types of Machine Learning

>[!NOTE]
>
>Hablemos ahora un momento sobre el aprendizaje autom√°tico.
>El nombre en s√≠ se explica por s√≠ solo.
>Se trata de la capacidad de los sistemas inform√°ticos, o quiz√°s simplemente de los sistemas, de aprender de los diferentes datos que poseen y crear diferentes resultados basados en algoritmos.
>
>Ahora bien, al pensar en los diferentes tipos de aprendizaje, existen muchos.
>Pero creo que estos son los m√°s b√°sicos.
>Analizaremos estos cuatro:
>
>![Tipos de Aprendizaje](images/2025-08-05_154640.png "Tipos de Aprendizaje")
>
>1. **Supervised Learning:** El primero se llama aprendizaje supervisado.
>Esto significa que una m√°quina empieza a aprender, pero es supervisada por una persona, un sistema o alguna forma de ense√±anza.
>En este caso, se refiere a la idea de que cada dato tiene una etiqueta.</br></br>
>As√≠, se le introducen estos datos a una m√°quina y se le indica que representan lo siguiente:
>Por ejemplo, se le podr√≠a proporcionar la imagen de un perro y decirle: ¬´Esto es un perro¬ª, lo cual es una forma de aprendizaje supervisado.
>
>2. **Unsupervised Learning:** El siguiente es el aprendizaje no supervisado.
>Se trata de la capacidad de los sistemas inform√°ticos de aprender, pero sin intervenci√≥n externa.
>Les otorga a las m√°quinas la capacidad de intentar encontrar sus propias razones y diferentes categorizaciones o agrupaciones de datos.
>Por lo tanto, la m√°quina debe identificar patrones en los datos y agruparlos.
>
>3. **Reinforced:** Otro es el aprendizaje reforzado.
>Y creo que este es el ejemplo m√°s simple de c√≥mo le ense√±as algo a un ni√±o.
>La m√°quina.
>Consta b√°sicamente de dos partes: el agente y el sistema o entorno.
>Y luego, el agente de IA comienza a realizar diferentes acciones.
>Y luego le dices si est√° bien o no.</br></br>
>Lo que has hecho, al igual que le ense√±as a un ni√±o cuando est√° aprendiendo a contar hasta diez, es que si empieza con uno, est√° bien.
>Y si en el cap√≠tulo uno dices dos, tambi√©n est√° bien.
>Pero si tu hijo dice cinco, entonces no est√° bien.
>As√≠ que tu hijo est√° aprendiendo que el cinco no es despu√©s del dos, y as√≠ sucesivamente.</br></br>
>Esto es aprendizaje reforzado.
>Cuando una persona externa supervisa lo que hace la m√°quina y otorga puntos, por ejemplo, por cada decisi√≥n o acci√≥n correcta que la m√°quina ha realizado, ya sea nosotros o cualquier otra parte.
>
>4. **Deep Learning:** Y este es el m√°s importante de todos.
>Se trata del aprendizaje profundo.
>Y est√° inspirado en el cerebro humano.
>Utiliza redes neuronales.
>Y esta es la capacidad, nuevamente, de un sistema para aprender usando datos no estructurados, usando datos sin etiquetar,
>pero pasando por m√∫ltiples fases o m√∫ltiples iteraciones para ajustar el modelo.
>Y estas iteraciones se llaman neuronas.
>Pero hablaremos de eso m√°s adelante.</br> </br>
>Y los grandes modelos de lenguaje y toda la IA se basan en estos algoritmos de aprendizaje profundo.
>Pero, nuevamente, abordaremos estos puntos a lo largo de nuestro material para comprender mejor qu√© son exactamente todos estos tipos de aprendizaje autom√°tico.


### 15. Machine Learning - Supervised ML

>[!NOTE]
>
>El aprendizaje supervisado se produce cuando se utiliza un algoritmo de aprendizaje autom√°tico con informaci√≥n predefinida sobre los datos que se consumen.
>Esto significa que, al introducir datos en el algoritmo de aprendizaje autom√°tico, al introducirlos, se est√°n proporcionando los datos, ¬øverdad?
>
> * Estos datos de etiquetas, y t√∫ proporcionas las etiquetas.
>Un buen ejemplo.
>Tomemos los perros como ejemplo.
>Quieres ense√±ar a tu sistema de aprendizaje autom√°tico o entrenar a tu sistema de IA para que identifique cu√°ndo tienes una foto de un perro o de cualquier otra cosa.</br>
>En este caso, intentar√≠as proporcionar una gran cantidad de datos, quiz√°s un par de millones de fotos de perros, pero tambi√©n millones de fotos de animales que no sean perros.
>Las etiquetar√≠as: tomar√≠as cada foto y dir√≠as "esto es un perro", y tomar√≠as la otra foto y dir√≠as "esto no es un perro", o podr√≠as decir algo como "un zorro".
>
> * Entonces, para cada conjunto de datos que tengas (datos y etiquetas), los introducir√≠as en este algoritmo de aprendizaje autom√°tico y el sistema, basado en un lenguaje de programaci√≥n o una l√≥gica de programaci√≥n ya instalada, intentar√° encontrar una forma de identificar si se trata de un perro o no.
>Ahora est√°s entrenando tu modelo.
>As√≠ es como lo llamar√≠a yo, ¬øverdad?
>Est√°s entrenando el modelo y, de nuevo, necesitas una gran cantidad de datos para ello.
>A continuaci√≥n, te explicamos c√≥mo funciona.
>
> * Ahora est√°s probando tu modelo.
>As√≠ que tienes el modelo de entrenamiento.
>Tienes cierta l√≥gica.
>Tienes una gran cantidad de datos de entrenamiento y ahora quieres probar tu modelo o usarlo para predecir algo.
>Correcto. </br>
>La predicci√≥n y t√∫ la introducir√≠as en este modelo.
>Ahora introducir√≠as algunos datos de prueba, pero sin la etiqueta.
>Correcto.
>Porque ahora est√° aprendiendo.
>As√≠ que est√°s introduciendo datos en tu modelo.
>De nuevo, diferentes im√°genes de cualquier tipo de animal podr√≠an ser perros, o cualquier otra cosa.
>
> * Pero no est√°s introduciendo la etiqueta porque quieres que el modelo lo entienda o validar si es correcto.
>Despu√©s de esto, el modelo te dar√° una predicci√≥n.
>Te dir√° si es perro o no.
>B√°sicamente, eso es todo. Lo que quieres hacer ahora es si el modelo est√° bien y si est√°s satisfecho con la predicci√≥n, est√° bien.
>
> * Pero si no est√°s satisfecho, lo que haces es tomar la predicci√≥n.
>Si es un modelo, si es un perro o no, y luego la comparas con la etiqueta de esa imagen en particular.
>Y si no es correcta, tienes que volver a entrenar tu sistema de IA con estos datos, con estos datos de prueba.
>Correcto.
>Y el sistema se adaptar√° para identificar cu√°l fue el problema.
>
>![Supervised Learning](images/2025-08-05_160223.png "Supervised Learning")
>
>Esto se llama aprendizaje supervisado porque constantemente le est√°s diciendo a tu sistema qu√© es lo que tienes.
>Si es un perro o algo m√°s.
>Y con cada fallo, tu sistema aprender√° de ello porque le est√°s indicando que hay un problema.
>Es un fallo que no es un perro y tu modelo aprender√°.
>Esto es aprendizaje autom√°tico supervisado.


### 16. Machine Learning - Unsupervised ML

>[!NOTE]
>
>Ahora intentemos comprender qu√© es exactamente el aprendizaje no supervisado.
>Como su nombre indica, no est√° supervisado por algo, por un algoritmo, por una entidad, ni proporciona ning√∫n tipo de preense√±anza.
>Por lo tanto, no se proporcionan estos valores como, por ejemplo, "esto es un caballo", ni se proporciona el sonido, la imagen o algo que exprese el significado de un caballo.
>As√≠ que creo que es m√°s f√°cil simplemente dar un ejemplo de lo que es el aprendizaje no supervisado.
>Y haremos un ejemplo muy, muy, muy simple.
>
>Ahora imaginemos que tienes este gr√°fico y aqu√≠, en la vertical, tienes la altura.
>Y aqu√≠, el peso y lo que haces.
>As√≠ que tomar√≠as a la persona.
>Y la persona se caracteriza por la altura y el peso, y nada m√°s.
>As√≠ que alimentar√≠as tu algoritmo.
>Introducir√≠as mucha informaci√≥n sobre pares de valores, altura y peso, y los representar√≠as en este gr√°fico de esta manera.
>
>![Gr√°fica Peso y Altura](images/2025-08-05_160857.png "Gr√°fica Peso y Altura")
>
>As√≠ que cada punto representa una altura y su peso correspondiente.
>Ves un patr√≥n.
>Lo que le pedir√≠as a tu algoritmo es que agrupe estos datos.
>El algoritmo dir√°: ¬´He encontrado estos dos grupos¬ª.
>
>![_Clusters_ o Grupos Hallados](images/2025-08-05_161156.png "_Clusters_ o Grupos Hallados")
>
>Tambi√©n puedes indicarle a tu algoritmo que encuentre tres cl√∫steres.
>Pero en este caso, tenemos dos cl√∫steres.
>Pero tambi√©n puedes tener tres, cuatro, cinco, seis y muchos m√°s.
>As√≠ que esta es la letra K.
>As√≠ que, el n√∫mero de cl√∫steres que quieres tener en tus datos de entrenamiento.
>
>Bas√°ndonos en la informaci√≥n que tenemos ahora, podr√≠amos pedirle al robot que diga: "Si obtengo valores que caen dentro del c√≠rculo rojo o en el √°rea roja, lo m√°s probable es que sea una mujer, bas√°ndonos en lo que tenemos y en lo que tenemos en el c√≠rculo azul, que corresponde a la altura y el peso de un hombre".
>Y, por supuesto, esto puede interrelacionarse.
>As√≠ que, en este caso, tienes dos cl√∫steres.
>
>![Grupo Rojo=Mujeres, Azul=Hombres](images/2025-08-05_161631.png "Grupo Rojo=Mujeres, Azul=Hombres")
>
>Lo que has hecho es pedirle a tu algoritmo de aprendizaje autom√°tico que intente estructurar los datos para encontrar patrones basados en dos o m√°s grupos, bas√°ndose √∫nicamente en estos dos elementos.
>
>En este caso, intentar√≠as ense√±arle a tu algoritmo a predecir si el valor, el par, la altura y el peso corresponden a un hombre o a una mujer.
>
> * **Clustering:** Si observas este tipo de aprendizaje, tienes agrupamiento.
>Esto es lo que ves aqu√≠: la idea del agrupamiento.
>Hemos agrupado nuestros datos en diferentes grupos o valores.
>
> * **Association:** Otro tipo es la asociaci√≥n.
>Imagina que est√°s en un sitio web y quieres comprar algo.
>Y ves que se compran juntos con frecuencia.
>Ese es un ejemplo de una aplicaci√≥n pr√°ctica del aprendizaje no supervisado.</br>
>Porque si le dices a tu robot que la gente ha comprado esto y solo introduces los pedidos con exactamente qu√© han comprado, tu algoritmo de aprendizaje autom√°tico dir√°: "Las personas que compran esto la mayor√≠a de las veces tambi√©n compran esto".
>As√≠ que esa es una forma diferente de aprendizaje no supervisado.
>Si entonces...
>
> * **Dimension Reduction** Y la √∫ltima podr√≠a ser la reducci√≥n de dimensi√≥n, que ocurre cuando tienes demasiadas dimensiones, demasiados par√°metros, demasiados puntos de datos, e intentas ignorar los que no son relevantes porque eso es solo ruido.
>
> Ahora bien, esta es una forma de aprendizaje no supervisado.
>Pero lo realmente interesante es que si se aplica este algoritmo en conjunto, se obtienen datos que se pueden usar en el aprendizaje supervisado.
>
>Porque si observan aqu√≠, tenemos dos pares de datos.
>Y podemos decir que a cualquier persona cuya altura y peso est√©n dentro del c√≠rculo azul, podemos asignarle una etiqueta que ser√≠a masculina.
>Y luego esos datos podr√≠an incorporarse al aprendizaje supervisado.
>Y ahora estamos entrando en este aprendizaje perpetuo.
>Y esta es la forma m√°s com√∫n en que las m√°quinas aprenden hoy en d√≠a: una forma de aprendizaje no supervisado.


### 17. Machine Learning - Reinforced ML

>[!NOTE]
>
>Ahora entendamos qu√© es exactamente el aprendizaje por refuerzo.
>Es la capacidad de un sistema de aprender con ayuda externa, con el refuerzo de una entidad omnisciente.
>Y, bas√°ndose en las acciones que realiza un sistema y la retroalimentaci√≥n constante, aprende mediante un mecanismo de ensayo y error.</br>
>¬°Bah!
>
>S√≠, suena un poco dif√≠cil de comprender, pero veamos un ejemplo.
>Este es tu sistema de IA y este es tu medio, tus factores externos y lo que tienes en este caso.
>En primer lugar, tu sistema de IA puede aprender o interpretar el estado del entorno.
>As√≠ que comprende el entorno y luego puede tomar medidas basadas en √©l.
>
>Y voy a tomar el ejemplo del coche aut√≥nomo; ser√° la forma m√°s sencilla de explicarlo.
>
>![Reinforcement Lerning](images/2025-08-05_162834.png "Reinforcement Lerning")
>
>A la izquierda y a la derecha, el cerebro.
>Esa es la IA de tu coche aut√≥nomo, y a la derecha, el entorno por el que circula.
>La carretera, la gente, los sem√°foros, etc.
>Correcto.
>Todo lo relacionado con el entorno del coche.
>Por ejemplo, pones el coche en marcha hacia un destino y empiezas a observar su comportamiento.
>
>![Reward=Recompensa](images/2025-08-05_163231.png "Reward=Recompensa")
>
>Y si el coche hace algo correctamente, le das esta recompensa.
>Si el coche hace algo incorrecto, le das la misma recompensa, pero de forma negativa.
>As√≠ que le das o le quitas puntos.
>
>Pongamos un ejemplo:
>Has empezado a conducir.
>Y entonces el coche empieza a moverse en l√≠nea recta.
>Y por cada segundo, o quiz√°s por cada 2 o 3 segundos que el coche conduce correctamente, le das esta recompensa, y la das constantemente.
>Ahora imaginemos que el coche llega a un sem√°foro y no reduce la velocidad.
>Entonces le das diferentes recompensas, pero de forma negativa.
>
>Imagina que el coche cambia de carril sin se√±alizar.
>Entonces eso tambi√©n es algo negativo, o est√° superando el l√≠mite de velocidad.
>Y eso otra vez.
>Correcto.
>Le das puntos negativos y entonces el coche o el sistema de IA aprender√°, como si le estuvieras ense√±ando a un ni√±o. Si haces eso, no est√° bien.
>
>Pero esto s√≠ est√° bien.
>Esto sigue estando bien.
>Sigue estando bien.
>
>Una vez que superas los 80 km/h, ya no est√° bien.
>Es ilegal.
>¬øVerdad?
>Porque est√°s superando el l√≠mite de velocidad, etc.
>
>Esto es aprendizaje por refuerzo.
>Y esta es otra forma de entrenar tu algoritmo de aprendizaje autom√°tico para que se comporte mejor simplemente ofreciendo recompensas o dando y quitando puntos, seg√∫n las decisiones que haya tomado la IA.


### 18. Importance of Training Data

>[!NOTE]
>
>Probablemente ya hayas o√≠do hablar de la palabra "basura entra, basura sale".
>En la inteligencia artificial, en la IA generativa, los datos son lo m√°s importante.
>Despu√©s de eso, se aplican a tu algoritmo de aprendizaje autom√°tico, a tus sesgos y a las decisiones que tomas.
>As√≠ que, en el caso de la IA, o en el de la IA generativa y los grandes modelos de lenguaje, los datos provienen de todas partes.
>
> * **Extract:** El primer paso al gestionar tus datos se denomina extracci√≥n.
>Esto significa que tienes una especie de lago de datos, algo que abarca todo tipo de contenido que puede alimentar tu algoritmo de aprendizaje autom√°tico.
>Por ejemplo, tienes im√°genes, archivos de v√≠deo, archivos de audio, archivos de texto, c√≥digo o cualquier cosa que puedas tener en cualquier lugar.
>Y lo primero que debes hacer al gestionar u obtener tus datos de entrenamiento, es extraer los datos relevantes para tu modelo.</br></br>
>Y despu√©s de decidir, ¬øqu√© quieres usar?</br></br>
>Por ejemplo, si est√°s entrenando datos o un modelo que usan los desarrolladores, lo m√°s probable es que tambi√©n consultes diferentes repositorios de GitHub.
>Consultar√°s Stack Overflow y cualquier otro tipo de foros, o quiz√°s materiales, conferencias o cursos relacionados con la inform√°tica.Y despu√©s de decidir, ¬øqu√© quieres usar?
>Por ejemplo, si est√°s entrenando datos o un modelo que usan los desarrolladores, lo m√°s probable es que tambi√©n consultes diferentes repositorios de GitHub.
>Consultar√°s Stack Overflow y cualquier otro tipo de foros, o quiz√°s materiales, conferencias o cursos relacionados con la inform√°tica.
>
> * **Transform:** Despu√©s de extraer todos estos datos, se pasa por la fase llamada transformaci√≥n.
>Transforma los datos.
>Los prepara para que el algoritmo pueda procesarlos.
>Los baraja.
>Los procesa por lotes.
>Realiza alg√∫n tipo de ampliaci√≥n o simplemente, eh, alg√∫n tipo de filtrado.
>S√≠.
>Datos buenos versus datos malos, o datos estad√≠sticamente relevantes versus datos que no tienen relevancia estad√≠stica, porque, de nuevo, depende de c√≥mo y qu√© tipo de datos introduzcas en tu algoritmo de aprendizaje autom√°tico.
>La calidad de tu modelo depender√° de ello.
>Datos malos o sesgados equivalen a un modelo sesgado, o quiz√°s a un modelo deficiente.
>S√≠.
>
> * **Model:** Despu√©s de esto, introduces estos datos en tu modelo.
>Se trata de una especie de proceso ETL: extraes, transformas y luego los cargas en tu modelo.
>A lo largo de toda esta cadena (extraes, transformas y cargas), compruebas que los datos no est√©n sesgados o los eliminas.
>Compruebas si hay datos corruptos.
>Yo compruebo si hay datos falsos o noticias falsas.</br></br>
>Por ejemplo, no conviene entrenar tu modelo con noticias falsas, datos estad√≠sticamente relevantes o sesgados, datos de odio, etc.
>Por eso, al trabajar con modelos de lenguaje grandes, los datos de entrenamiento son lo m√°s importante que puedes tener para entrenar tu modelo.
>No para hablar, ni para generar contenido, sino para ser pol√≠ticamente correcto.
>Por ejemplo, si quieres ser √©tico, si no necesitas tener sesgos, o si quieres hablar bien o escribir con correcci√≥n gramatical, debes asegurarte de que los datos de entrenamiento tambi√©n sean gramaticalmente correctos, y la lista contin√∫a.
>As√≠ que recuerda que tus datos de entrenamiento deben ser limpios, no deben estar sesgados y deben ser relevantes para el modelo que intentas crear.
>
>![Data Importance](images/2025-08-06_160657.png "Data Importance")



### 19. What actually is GEN AI

>[!NOTE]
>
>Y ahora llegamos a la pregunta: ¬øqu√© es la IA generativa?
>
>Hemos aprendido sobre modelos de aprendizaje autom√°tico, datos etiquetados y no etiquetados, y redes neuronales.
>Entonces, ¬øc√≥mo se integran todos estos elementos?
>Bueno, en primer lugar, lo que hay que decir sobre la IA generativa es que no se compone solo de grandes modelos de lenguaje, porque sabemos que la IA generativa puede producir im√°genes.
>Pero sabemos que LLM solo puede proporcionar texto.
>
>Sabemos que tenemos datos sin etiquetar, datos etiquetados y c√≥digo.
>Y contamos con muchas herramientas para entrenar nuestro modelo base.
>Hemos entrenado nuestro modelo con toda esta cantidad de datos, y el modelo contin√∫a entren√°ndose a s√≠ mismo bas√°ndose en los nuevos datos que obtiene o en la retroalimentaci√≥n de los usuarios.
>Despu√©s de eso, este modelo base tambi√©n puede generar contenido porque sabe c√≥mo descomponerlo en estos par√°metros.
>Tambi√©n puede hacer lo contrario.
>As√≠ que si sabes c√≥mo ir de A a B, tambi√©n sabes c√≥mo ir de B a A o quiz√°s una variaci√≥n de esto.
>
>Esto tambi√©n se trata de generar contenido, ya que sabe c√≥mo se ve un perro a partir de estos 76 mil millones de par√°metros.
>Solo te dar√° una ligera variaci√≥n de esos par√°metros cuando se los indiques.
>Genera un perro para m√≠.
>Correcto.
>Y generativo.
>Generar√° contenido y te lo mostrar√°.
>Puede ser c√≥digo, texto, un libro, cualquier cosa.
>Pero lo importante es saber qu√© es generativo y qu√© no.
>
>En primer lugar, una frase, un poema, cualquier cosa generativa, forma parte de la IA generativa.
>Pero si es un n√∫mero, una probabilidad, si es verdadero o falso, no forma parte de la IA generativa.
>Es solo un c√°lculo.
>En resumen, todo esto encaja perfectamente.
>Y forman esta IA generativa.
>
>En nuestro caso, si vamos m√°s all√° de la idea de grandes modelos de lenguaje que funcionan con texto y se desea crear im√°genes, esto significa que la IA generativa es toda una rama de la IA que se centra en proporcionar o aplicar diferentes algoritmos y patrones de aprendizaje autom√°tico para generar lo que se desee generar.
>Ya sea c√≥digo, un poema, un discurso, texto, im√°genes, etc.
>
>![Generative A.I.](images/2025-08-06_161521.png "Generative A.I.")


### 20. Transformer Architecture Model

>[!NOTE]
>
>[![What are Transformers (Machine Learning Model)?](images/2025-08-06_161828.png "What are Transformers (Machine Learning Model)?")](https://www.youtube.com/watch?v=ZXiruGOCn9s)
>
>No, no son esos transformadores.
>Pero pueden hacer cosas geniales, d√©jame mostrarte.
>¬øY por qu√© el pl√°tano del otro lado de la calle?
>
>¬°Porque estaba harto de que lo aplastaran!
>S√≠, no s√© si lo entiendo bien.
>Y eso es porque lo cre√≥ una computadora.
>
>Literalmente le ped√≠ que me contara un chiste.
>Y esto es lo que se le ocurri√≥.
>Espec√≠ficamente, us√© un GPT-3, o un modelo de transformador generativo preentrenado.
>
>El 3 aqu√≠ significa que esta es la tercera generaci√≥n.
>GPT-3 es un modelo de lenguaje autorregresivo
>que produce texto que parece escrito por un humano.
>GPT-3 puede escribir poes√≠a, redactar correos electr√≥nicos y, evidentemente, inventar sus propios chistes.
>¬°Vamos!
>
>Ahora bien, aunque nuestro chiste del pl√°tano no es precisamente gracioso,
>sigue el patr√≥n t√≠pico de un chiste con un planteamiento y un remate, y en cierto modo tiene sentido. ¬øQui√©n no cruzar√≠a la calle para evitar ser aplastado?
>
>Pero mira, GPT-3 es solo un ejemplo de transformador.
>Algo que transforma de una secuencia a otra.
>Y la traducci√≥n de idiomas es un gran ejemplo.
>Quiz√°s queramos tomar nuestra frase "¬øPor qu√© el pl√°tano >cruz√≥ la calle?",
>y traducirla al franc√©s.
>
>Bueno, los transformadores constan de dos partes: un codificador y un decodificador.
>
>El codificador trabaja con la secuencia de entrada,
>y el decodificador con la secuencia de salida.
>Ahora bien, a primera vista, la traducci√≥n parece poco m√°s que una simple b√∫squeda,
>
>as√≠ que convierte el "por qu√©" de nuestra frase en ingl√©s a su equivalente en franc√©s: "pourquoi".
>Pero, por supuesto, la traducci√≥n de idiomas no funciona as√≠.
>
>Cosas como el orden de las palabras y los giros idiom√°ticos a menudo generan confusi√≥n. Los transformadores funcionan mediante aprendizaje secuencia a secuencia, donde el transformador toma una secuencia de tokens, en este caso palabras de una oraci√≥n, y predice la siguiente palabra en la secuencia de salida. Esto se logra iterando por las capas del codificador, de modo que el codificador genera codificaciones que definen qu√© partes de la secuencia de entrada son relevantes entre s√≠ y luego pasa estas codificaciones a la siguiente capa. El decodificador toma todas estas codificaciones y utiliza su contexto derivado para generar la secuencia de salida.
>
>Los transformadores son una forma de aprendizaje semisupervisado.
>Por "semisupervisado", nos referimos a que se entrenan previamente de forma no supervisada con un gran conjunto de datos sin etiquetar y luego se perfeccionan mediante entrenamiento supervisado para optimizar su rendimiento.
>
>En videos anteriores, he hablado de otros algoritmos de aprendizaje autom√°tico que procesan la entrada secuencial como lenguaje natural. Por ejemplo, existen redes neuronales recurrentes (RRN).
>
>Lo que diferencia a los Transformers es que no necesariamente procesan los datos en orden.
>Los Transformers utilizan un mecanismo de atenci√≥n.
>Este proporciona contexto en torno a los elementos de la secuencia de entrada.
>As√≠, en lugar de comenzar la traducci√≥n con la palabra "por qu√©" porque est√° al principio de la oraci√≥n, el Transformer intenta identificar el contexto que aporta significado a cada palabra de la secuencia.
>
>Y es este mecanismo de atenci√≥n lo que les da a los Transformers una gran ventaja sobre algoritmos como las RNN, que deben ejecutarse en secuencia.
>
>Los Transformers ejecutan m√∫ltiples secuencias en paralelo.
>Y esto acelera enormemente los tiempos de entrenamiento.
>M√°s all√° de las traducciones, ¬øa qu√© se pueden aplicar los Transformers?
>
>Bueno, los res√∫menes de documentos son otro gran ejemplo.
>Se puede introducir un art√≠culo completo como secuencia de entrada y luego generar una secuencia de salida que, en realidad, consistir√° en un par de oraciones que resumen los puntos principales. Los Transformers pueden crear documentos completamente nuevos, por ejemplo, escribir una entrada de blog completa.
>
>Y m√°s all√° del lenguaje, los Transformers han hecho cosas como aprender a jugar ajedrez y realizar un procesamiento de im√°genes que incluso rivaliza con las capacidades de las redes neuronales convolucionales.
>
>Mira, los Transformers son un potente modelo de aprendizaje profundo y, gracias a la capacidad de paralelizar ese mecanismo, est√°n mejorando constantemente.
>¬øY qui√©n sabe?
>Muy pronto, tal vez incluso puedan hacer chistes de pl√°tanos que sean realmente graciosos.

### Quiz 1: Chapter Quiz

>[!NOTE]
>
>![Quiz 1: Chapter Quiz](images/2025-08-06_163419.gif "Quiz 1: Chapter Quiz")


## Section 4: Overview of Machine Learning Model Testing

### 21. Types of Testing in Software

>[!NOTE]
>
>Antes de adentrarnos en la parte esencial de las pruebas de lenguaje, entendamos un poco c√≥mo funcionan las pruebas de software.
>Quiz√°s ya sepas esto.
>Si lo sabes, puedes omitirlo, pero si no, podemos simplemente resumir:
> * **¬øqu√© significa ingenier√≠a de calidad?**
>Generalmente se le llama pruebas de software.
>Por lo tanto, la ingenier√≠a de calidad es el gran √°mbito que se encarga de verificar que se tenga un cierto nivel de calidad en cualquier cosa.
>
> * Una parte de la ingenier√≠a de calidad se denomina pruebas de software.
>Ahora bien, las pruebas de software.
>Tradicionalmente se dividen en
>   * pruebas funcionales y, por supuesto,
>   * pruebas no funcionales.
> * Y si analizamos qu√© son las pruebas funcionales, nos referimos a qu√© hace nuestro producto.
>Qu√© hace.
>Por ejemplo, si queremos ir de A a B, el programa nos dar√° una ruta.
>Y eso es lo que hace.
>Esa es una funcionalidad.
>
>   * Las pruebas no funcionales se refieren al rendimiento de nuestra aplicaci√≥n, sea lo que sea que necesite realizar.</br>
> Algunos ejemplos son las pruebas de rendimiento.
>¬øQu√© tan r√°pido carga nuestra p√°gina web?
>¬øQu√© tan r√°pido genera una imagen nuestro modelo de lenguaje?
>Cuando decimos que generemos una imagen para nosotros.
>   * Las pruebas de seguridad se refieren a que, independientemente de lo que hagamos, est√° protegido y nadie puede acceder a esa informaci√≥n si no se le permite acceder a ella.
>Y hay m√°s.
>
> * La parte funcional, nuevamente, se divide en varias √°reas.
>Pero el objetivo principal es qu√© hace nuestra aplicaci√≥n.
>   * Por ejemplo, las pruebas de aceptaci√≥n del usuario forman parte de las pruebas funcionales.
>   * Luego, se pueden realizar pruebas unitarias, pruebas de integraci√≥n y pruebas de interfaz de usuario.
>Todas se consideran parte de las pruebas funcionales.
>   * Tambi√©n es posible que quieras verificar tus requisitos.
>As√≠ que aseg√∫rate de que tu aplicaci√≥n funcione correctamente.
>S√≠, en comparaci√≥n con tus requisitos.
>
>En resumen, estas son tus pruebas de software.
>Y esto es a gran escala.
>Lo que validas, por supuesto, es que sabes que puedes hablar durante horas sobre qu√© son las pruebas de software.
>Pero para el prop√≥sito de nuestra lecci√≥n, necesitamos entender qu√© es esto.
>
>![Quality Engineering](images/2025-08-12_131314.png "Quality Engineering")



### 22. Understand how ML models are tuned - General View

>[!NOTE]
>
>Ahora solo quiero tomarme un par de minutos para explicar c√≥mo ajustar cualquier tipo de modelo.
>Es muy f√°cil.
>Es como cualquier otro software.
>Como pueden ver, esto es similar al concepto de ramificaci√≥n.
>
>Podr√≠an tener su repositorio de modelos.
>Podr√≠an pensar en esto como la l√≠nea principal, el tronco.
>Y esto podr√≠a ser una rama.
>
>Entonces, lo primero que hacen es modificar su modelo para tener una nueva idea o si su modelo ha sido...
>eh, hay alguna degradaci√≥n, o quieren mejorar su algoritmo o lo que sea.
>
>Entonces, lo que haces es modificar el c√≥digo que cambia tu conjunto de datos y luego ejecutar algunas pruebas preentrenadas.
>Esto significa que aqu√≠ mismo est√°s ejecutando la prueba en un modelo preentrenado.
>Justo en el modelo que est√° aqu√≠.
>
>Despu√©s, vuelves a entrenar el modelo con los datos m√°s recientes, ya que podr√≠as entrenarlo gracias a los datos.
>Podr√≠as tener nuevos datos en el concepto de desviaci√≥n de datos o desviaci√≥n del modelo.
>
>Quieres entrenar con datos nuevos porque los antiguos ya no son relevantes para la situaci√≥n actual, o has cambiado tu algoritmo.
>Tu modelo de aprendizaje autom√°tico se ha modificado con nuevos par√°metros.
>Y por eso, ahora mismo est√°s entrenando tu modelo con tu nuevo algoritmo.
>Y luego ejecutas la nueva prueba.
>
>As√≠ que ahora tienes pruebas que deben validar que el entrenamiento se ajusta a lo que tienes, a lo que quieres hacer con tu modelo m√°s reciente.
>Ese es el punto clave.
>
>Necesitas obtener algunos resultados, pero despu√©s de entrenar, buscas otros resultados que correspondan al nuevo par√°metro.
>Y, por supuesto, esta prueba.
>Tambi√©n se combinan algunas m√©tricas.
>As√≠ que las m√©tricas y los resultados se combinan.
>
>Es como ejecutar una UAT o validar los resultados de la UAT, como podr√≠a ser este.
>Y luego la apruebas y la implementas directamente en producci√≥n.
>Por supuesto, tambi√©n hay una parte donde se podr√≠a hablar de depuraci√≥n de datos.
>As√≠ podr√≠a ser.
>
>Pero suponiendo que tus datos ya est√°n limpios, no necesitas depurar los datos en este punto.
>Correcto.
>Esto significa que te aseguras de que los datos que obtienes sean correctos y no est√©n sesgados.
>Eh, son relevantes, etc.
>
>Al igual que con cualquier tipo de ramificaci√≥n, se modifica la prueba, se ejecuta una prueba previa al entrenamiento, se entrena, se valida el entrenamiento, se revisa, se aprueba y, finalmente, se implementa en producci√≥n.
>Y aqu√≠, por supuesto, se vuelve a supervisar.
>As√≠ es como se ajusta cualquier modelo de aprendizaje autom√°tico.
>
>![Basic Model Tuning](images/2025-08-12_132223.png "Basic Model Tuning")



### 23. Fine tunning techniques for AI and any LLM foundation model

>[!NOTE]
>
>Para comprender completamente la IA, necesitamos detenernos un momento y pensar en el ajuste del modelo.
>Lo que ven aqu√≠ es un ciclo de vida est√°ndar para ajustar un modelo base.
>Porque lo que sucede es que obtendr√°n, por ejemplo, Llama 3, o ChatGPT 01 o cualquier otro modelo de lenguaje extenso.
>Si se trata de un fan√°tico chino, tal vez podr√≠an obtener la versi√≥n 3 profunda, y as√≠ sucesivamente.
>Y aqu√≠ es donde terminan, ¬øverdad?
>
>As√≠ que, de todo el ciclo de vida, preparan sus datos, seleccionan su modelo base y luego eval√∫an el mejor modelo para su tarea.
>Por ejemplo, he visto que Claude Sonnet 3.5 es mejor que ChatGPT 4 en codificaci√≥n.
>As√≠ que quiz√°s seleccione Cloud 3.5 y lo use para codificar.
>Y despu√©s de eso, necesito ajustar mi modelo.
>Esto significa que necesito preparar mi modelo para la tarea espec√≠fica que quiero realizar.
>Porque, no olvidemos que un modelo base como ChatGPT sirve para todo, as√≠ que necesitamos mejorarlo para la tarea en cuesti√≥n.
>
>[![https://mostly.ai/blog/machine-learning-life-cycle-with-synthetic-data](https://mostly-web-mostly-website-assets.s3.eu-central-1.amazonaws.com/wp-content/uploads/2023/08/Machine-learning-life-cycle-1-1-1024x871.jpg "https://mostly.ai/blog/machine-learning-life-cycle-with-synthetic-data")](https://mostly.ai/blog/machine-learning-life-cycle-with-synthetic-data#what-is-a-machine-learning-life-cycle)
>
>Comenzaremos con el conjunto de datos.
>Tienes el conjunto de datos de preentrenamiento y el conjunto de datos de ajuste.
>Lo llamaremos as√≠.
>Selecciona el conjunto de datos completo, que es el 100%, llam√©moslo as√≠.
>Y luego volver√°s a entrenar.
>
>Los modelos ejecutar√°n el algoritmo de entrenamiento con entre el 60% y el 70% de los datos.
>Tu modelo se entrenar√° con estos datos.
>Despu√©s de entrenar tu modelo con estos datos, del 30% o 40% restante, dependiendo de c√≥mo lo dividas,
>necesitamos seleccionar la mitad.
>
>Imaginemos que el 20% se evaluar√° durante el entrenamiento.
>Entrenamos el modelo y luego evaluamos durante el entrenamiento para que la inferencia, que es b√°sicamente la predicci√≥n, se acerque a lo que buscamos.
>Y luego, para el resto, probamos el modelo en un entorno sin entrenamiento, ya que esto es entrenamiento puro.
>Esto es entrenamiento y evaluaci√≥n.
>As√≠ que eval√∫as tus par√°metros para que las inferencias se acerquen a lo deseado.
>Y aqu√≠ esto es evaluaci√≥n pura, sin entrenamiento.
>
>As√≠ que esta es una forma de guiar tu modelo, bas√°ndote en tus datos, para que se acerque m√°s a lo que realmente quieres lograr.
>Esta es una forma de, digamos, evaluar o ajustar tu modelo.
>
>[![.](https://miro.medium.com/v2/resize:fit:1000/1*AE17O-39mBq3PFBalay6-w.png "https://medium.com/data-science/data-splitting-technique-to-fit-any-machine-learning-model-c0d7f3f1c790")](https://medium.com/data-science/data-splitting-technique-to-fit-any-machine-learning-model-c0d7f3f1c790)
>
>La siguiente forma de perfeccionar tu modelo es con indicaciones.
>Si sabes, puedes usar este generador de GPT, que te permite perfeccionar un modelo; de hecho, crear tu propio modelo.
>Lo que hace es, b√°sicamente, perfeccionarlo para una tarea espec√≠fica.
>Tengo la gu√≠a de certificaci√≥n, mi propio modelo de perfeccionamiento, dise√±ado espec√≠ficamente para ayudar a las personas a aprobar la certificaci√≥n.
>Si revisamos la configuraci√≥n, editamos mi GPT, encontrar√°s las instrucciones.
>
>[![https://chatgpt.com/g/g-UpNvtv9X2-istqb-certification-guide](images/2025-08-12_133947.png "https://chatgpt.com/g/g-UpNvtv9X2-istqb-certification-guide")](https://chatgpt.com/g/g-UpNvtv9X2-istqb-certification-guide)
>
>Otra opci√≥n podr√≠a ser usar barandillas.
>AWS ofrece un servicio llamado barandillas.
>Y de lo que quiero hablar espec√≠ficamente es de bloquear temas no deseados.
>Quiz√°s quieras que tu modelo no trate un tema espec√≠fico.
>Quiz√°s est√© prohibido hablar de cualquier tema.
>
>As√≠ que puedes implementar este tipo de barandillas en tu modelo para que solo t√∫ lo sepas.
>Se permite la generaci√≥n de c√≥digo y no se permite nada que est√© fuera de las decisiones o pol√≠ticas de tu empresa.
>Esta es otra forma de ajustar tu modelo de forma granular para restringir la cantidad y los temas que puede gestionar.
>
>Estas son las formas m√°s sencillas de ajustar tu modelo si quieres trabajar con diferentes par√°metros, como el funcionamiento interno de los modelos de aprendizaje autom√°tico, como las redes neuronales.
>
>Luego necesitas profundizar mucho en los modelos de entrenamiento, el aprendizaje autom√°tico y las matem√°ticas.
>Pero, ya sabes, eso es bastante dif√≠cil.
>La forma m√°s f√°cil es recordar los datos.
>Se hace con indicaciones y recopilaciones.
>
>[![https://aws.amazon.com/es/bedrock/guardrails/](images/2025-08-12_135208.png "https://aws.amazon.com/es/bedrock/guardrails/")](https://aws.amazon.com/es/bedrock/guardrails/)
>
>



### 24. Overall Testing Approach to LLMs

>[!NOTE]
>
>Ahora hablemos de nuestro enfoque para probar modelos ling√º√≠sticos grandes.
>El problema es que, en nuestro escenario, o en el caso de los modelos ling√º√≠sticos grandes, es casi imposible probarlo todo, ya que las aplicaciones son pr√°cticamente infinitas.
>
>Por lo tanto, necesitamos un enfoque m√°s estructurado para que, al probar algo, pueda aplicarse en toda la cadena de valor de, digamos, toda la industria.
>Por lo tanto, en nuestro material, al probar modelos ling√º√≠sticos grandes, nos referiremos a tres elementos diferentes.
>
>El primero trata sobre los datos que necesitamos para probar c√≥mo se utilizan en los modelos ling√º√≠sticos grandes.
>
>Necesitamos comprobar que los resultados no contengan datos sesgados.
>Si introduces datos sesgados, quieres asegurarte de que el resultado que obtienes no lo est√©.
>Por lo tanto, los modelos de lenguaje grandes necesitan asegurarse de contar con este tipo de medidas de seguridad.
>
>Al cargar los datos o al proporcionar el resultado, analizaremos datos estad√≠sticamente irrelevantes, ya que, como ya hemos dicho, los modelos de lenguaje grandes son m√°quinas de predicci√≥n estad√≠stica.
>Si las estad√≠sticas no son relevantes, el resultado tampoco lo es.
>Por lo tanto, estas m√°quinas deben tener la capacidad de filtrar este tipo de datos: datos desequilibrados.
>
>Estos datos son estad√≠sticamente relevantes, pero no est√°n, digamos, equilibrados ni a la izquierda ni a la derecha. Veremos qu√© significa esto, o datos obsoletos, porque aprenderemos sobre el concepto de deriva.
>
>Podr√≠as entrenar tu modelo con datos relevantes de hace 15 a√±os, pero que ya no se aplican hoy en d√≠a, y, por supuesto, con otros.
>Luego, haremos pruebas con indicaciones.
>Y cuando digo indicaciones, tambi√©n me refiero a las API.
>
>Cuando llamas a una API para un modelo de lenguaje grande, b√°sicamente la est√°s indicando, pero con un tipo de indicaci√≥n diferente.
>Pero no te preocupes tanto por c√≥mo probar una API.
>Estoy bastante seguro de que puedes encontrar eso en l√≠nea.
>Veremos c√≥mo usar indicaciones para ver si tu modelo de lenguaje grande tiene diferentes capacidades de razonamiento.
>Y, por supuesto, usaremos indicaciones en cascada.
>
>Por ejemplo, indicaciones de cero disparos, indicaciones iterativas, cadena de pensamiento, indicaciones contraf√°cticas, entre otras.
>S√≠, creo que este ser√° el cap√≠tulo m√°s importante que abordaremos.
>
>Y un √∫ltimo punto que analizaremos es la regulaci√≥n de la UE.
>Y, por supuesto, la parte √©tica.
>Aqu√≠ analizaremos las pr√°cticas prohibidas para garantizar que se cuente con las salvaguardas necesarias.
>Transparencia en la toma de decisiones.
>Documentaci√≥n t√©cnica y, por supuesto, mantenimiento de registros.
>Esto tambi√©n est√° regulado.
>Y la capacidad de las personas para supervisar cualquier tipo de interacci√≥n con la IA.
>
>Y el √∫ltimo punto se refiere a los datos y su gobernanza.
>Esto es, ya saben, la parte principal. Habr√° otra parte intermedia sobre seguridad y diferentes tipos de ataques que se pueden realizar para exponer datos o quiz√°s inyectar datos que no se desean.
>Pero hablaremos de eso m√°s adelante.
>
>![Data, Prompts, EU Regulation](images/2025-08-12_140928.png "Data, Prompts, EU Regulation")
>



### 25. Testing Types for LLMs | Foundation Models

>[!NOTE]
>
>As√≠ que, al pensar en pruebas no funcionales para un modelo de lenguaje grande, ya hemos decidido que, de nuevo, se debe tener rendimiento, seguridad, portabilidad, escalabilidad, registro, monitorizaci√≥n y auditor√≠as.
>Cierto.
>Todo esto.
>
>![Quality Engineering"](images/2025-08-12_131314.png "Quality Engineering")
>
>Pero los modelos de lenguaje grandes o la IA tambi√©n necesitan ir m√°s all√° del software tradicional, porque giran en torno a la inteligencia.
>Y recuerden, inteligencia.
>
>Si lo crean humanos, debe imitar el comportamiento humano.
>Por lo tanto, debe pensar como un humano, con sus virtudes y defectos.
>Y por esta raz√≥n, al pensar en probar modelos de lenguaje grandes, y espec√≠ficamente, no funcionales, necesitamos incorporar al menos tres tipos m√°s de pruebas.
>Y en primer lugar, se trata de la √©tica en torno a su modelo de lenguaje grande.
>
> * En primer lugar, se tratar√° la **√©tica** en torno a su modelo ling√º√≠stico general.
>Porque si se utilizan estos modelos, deben incorporar la √©tica humana.
>Y tambi√©n sabemos que, a lo largo de la historia, la √©tica humana no ha evolucionado.
>Quiz√°s no ten√≠amos √©tica en la Edad Media.
>Quiz√°s ten√≠amos algo de √©tica hace unos 100 a√±os y ahora es diferente.
>Por lo tanto, la √©tica debe evaluarse seg√∫n los tiempos modernos, seg√∫n lo que es √©tico en la actualidad.
>Esto es algo que analizaremos en los pr√≥ximos cap√≠tulos.
>
> * Y luego est√° la parte **adversarial**.
>Y cuando digo adversarial, me refiero a un tipo especial de prueba de seguridad que consiste en usar indicaciones para que el modelo act√∫e de forma poco √©tica, para recopilar informaci√≥n del modelo, para extraer informaci√≥n y para enga√±arlo para que haga algo poco √©tico.
>Inyectar indicaciones.
>Envenenar los datos para que el modelo act√∫e de forma poco √©tica o maliciosa, o quiz√°s ense√±arte a fabricar una bomba.
>Por lo tanto, las pruebas adversariales son un tipo de seguridad y tambi√©n las abordaremos en los pr√≥ximos cap√≠tulos.
>
> * Y quiz√°s uno de los aspectos m√°s interesantes es si tu modelo de IA puede actuar como un **humano**.
>Esto significa que cuando hablo con el modelo Doe, ¬øsiente que le estoy hablando?</br>
>En primer lugar, ¬øes un robot o tiene consistencia?
>¬øPierde contexto?
>¬øEs √©tico?
>¬øHabla de la misma manera?
>¬øTiene solidez en la conversaci√≥n?
>
>![Ethics, Adversial, Human](images/2025-08-12_142150.png "Ethics, Adversial, Human")
>
>Estos tambi√©n son algunos aspectos no funcionales del modelo.
>Pero si no los tienes, si el modelo, al hablar con √©l, se siente torpe y est√∫pido,
>y est√°s teniendo una conversaci√≥n sobre algo.
>Y de repente, el modelo responde a una pregunta diferente, y siempre responde de forma diferente,
>y no entiende y te da la misma respuesta una y otra vez.
>
>Es decir, tiene todos los aspectos, quiz√°s no funcionales.
>Puede que tenga la parte funcional, pero no act√∫a como un humano. As√≠ que te vas a frustrar.
>Tambi√©n probaremos este tipo de lado humano, parte de los aspectos no funcionales de un chatbot, o quiz√°s de un modelo de lenguaje m√°s amplio.



### 26. Introduction to Benchmarking for LLMs

>[!NOTE]
>
>En cualquier tipo de industria, digamos.
>Existen ciertos puntos de referencia.
>Existen empresas, o quiz√°s no solo empresas, foros que comparan una cosa con otra para saber cu√°l es mejor.
>
>Por ejemplo, en la industria de los videojuegos, existen CPU o GPU.
>En ciertas aplicaciones o juegos, se comparan en funci√≥n de los fotogramas por segundo que pueden ofrecer.
>Y esto puede aplicarse a cualquier cosa.
>
>Por ejemplo, en la industria automotriz, es la vuelta a N√ºrburgring.
>Entonces, ¬øqu√© tan r√°pido puede un auto dar esa vuelta de 40 km? Y la lista contin√∫a.
>Y, por supuesto, para la IA existen diferentes tipos de puntos de referencia.
>
>Pero entendamos c√≥mo funciona el benchmarking en un modelo de IA.
>Porque normalmente, por ejemplo, si quisiera saber si mi hija sabe contar hasta diez,
>lo har√≠a.
>S√© contar hasta diez.
>Cierto.
>
>Es, digamos, una verdad obvia.
>C√≥mo contar hasta diez en cualquier idioma: uno, dos, tres, etc.
>As√≠ que si quisiera saber si mi hija sabe contar hasta diez, le pedir√≠a que contara hasta diez.
>Y si dijera uno, dos, tres, cuatro, cinco, etc., le dir√≠a que s√≠, pero si dice uno, cuatro, nueve y luego ocho, sabr√≠a que, en comparaci√≥n con el punto de referencia, no est√° rindiendo como deber√≠a.
>
>As√≠ que es b√°sicamente una comparaci√≥n.
>
>Si la comparaci√≥n da igual, sabes que has aprobado.
>Si no, no obtienes puntuaci√≥n y entonces realizas la evaluaci√≥n.
>
>B√°sicamente, comparas los resultados y realizas estas pruebas para quiz√°s 100 preguntas, 100 problemas matem√°ticos diferentes, 100 problemas cient√≠ficos diferentes, problemas universitarios.
>Dependiendo de lo que necesites analizar, existen herramientas de evaluaci√≥n comparativa espec√≠ficas.
>
>![.](images/2025-08-12_143305.png "")
>
>As√≠ es como funciona la evaluaci√≥n comparativa, no solo en IA, sino en cualquier √°mbito.
>Pero en el caso de la IA, necesitas datos de referencia.
>Necesitas un modelo de lenguaje extenso.
>
>Necesitas alg√∫n tipo de herramienta para comparar el resultado dado con el resultado generado por el modelo de lenguaje extenso. Los introduces en un mecanismo de puntuaci√≥n, luego emites una puntuaci√≥n y listo.
>Puedes comparar diferentes modelos de lenguaje extensos entre s√≠ dentro del mismo conjunto de datos.
>



## Section 5: Common Traditional Metrics for ML Models and how to calculate them

### 27. Understanding how to compute any score

>[!TIP]
>
>**Understanding how to compute any score**
>
>In case you have not watched Lecture 1 of Section 3( Introduction to Benchmarking for LLMs)  revisit, that since it contains information that is vital to understand the following 5-6 lectures.
>


### 28. Ground Truth Table - source of Truth | Test Oracle

>[!NOTE]
>
>Lo que veremos ahora es c√≥mo calcular la puntuaci√≥n F1, la precisi√≥n, la predicci√≥n, la veracidad, etc.
>Pero para ello, necesitamos que se cumpla una condici√≥n previa espec√≠fica.
>Necesitamos los datos de prueba de referencia que se utilizan para calcular la puntuaci√≥n.
>
> Y quiero presentar un concepto llamado tabla de verdad fundamental.
>Y para explicarlo, los guiar√© a trav√©s de todo el proceso de prueba para calcular todos estos elementos.
>
> * **String:** As√≠ que lo primero que necesitar√°n es una cadena, una cadena muy larga y compleja, que contenga muchos sustantivos, nombres de animales, entidades, empresas y personas.
>
> * **Prompt:** As√≠ que no solo verbos, sustantivos, etc., sino que tambi√©n tiene entidades que definen algo especial.
>En mi ejemplo, tengo una entidad de 100 cadenas, y sin la entidad, o sobre ella,
>necesitar√≠as una indicaci√≥n.
>Y, como sabes, la indicaci√≥n se puede usar simplemente para decir: "¬øQu√© hay en la entidad?".
>O no necesitas una indicaci√≥n cuando simplemente preguntas algo m√°s.</br>
>Pero en nuestro caso, necesitamos especificar que queremos que el modelo realice este tipo de reconocimiento de entidades.
>
> * **LLM(Model):** Lo que necesitas, por supuesto, es un modelo bastante obvio.
>Para validar esto, necesitar√°s un modelo.
>Env√≠as una cadena, digamos 100, una oraci√≥n, una frase o quiz√°s un p√°rrafo, de una p√°gina, por ejemplo.
>Y le pides al modelo que calcule el reconocimiento de entidades de nombre para todo.
>Eso incluye espacios, signos de puntuaci√≥n, absolutamente todo.</br>
>¬øY qu√© sucede?
> * **Output:** El modelo proporcionar√° un resultado en forma de tabla, que les mostrar√©.
>Por ejemplo, en el ejemplo que tenemos, la columna A ser√° el nombre de la entidad.
>La columna B ser√° un c√≥digo que se asigna a una entidad espec√≠fica.
>Y la columna C ser√° la explicaci√≥n del c√≥digo para el usuario.
>Esto se denomina, digamos, tabla de predicci√≥n, lo que el modelo ha predicho.
>
> * **Truth Table:** Tambi√©n tendr√°s otra creada por ti.
>Esta ser√° la tabla de verdad fundamental.
>As√≠ que tendr√°s ambas y comparar√°s la tabla de verdad fundamental.
>B√°sicamente, esta de aqu√≠.
>   * Lo que est√° a la derecha.
>   * Comparar√°s esta con lo que viene de la izquierda.
>   * Y luego podr√°s calcular todo lo que queremos hacer en el siguiente material.
>Ahora, perm√≠teme darte un ejemplo de la tabla de verdad fundamental que tengo ahora mismo.
>
>![Texto Base (String)](images/2025-08-14_100206.png "Texto Base (String)")
>
>Este lo comparar√°s con lo que viene de la izquierda.
>Y luego podr√°s calcular todo lo que queremos hacer en el siguiente material.
>Ahora, perm√≠tanme darles un ejemplo de la tabla de verdad fundamental que tengo ahora mismo.
>En mi tabla, como pueden ver, esta es una cadena que estoy usando.
>La encuentran en l√≠nea, van a cualquier parte, buscan un art√≠culo y luego simplemente copian algunos elementos.
>
>![groud_truth_labels](images/2025-08-14_100557.png "groud_truth_labels")
>
>Lo encuentras en l√≠nea, vas a cualquier parte, consigues un art√≠culo y luego simplemente copias algunos elementos.
>Y con base en esta cadena, as√≠ es como se ve mi tabla de verdad fundamental.
>Lo que podemos ver es el token "doctor".
>Vemos la etiqueta de verdad fundamental.
>
>Esta es la etiqueta y la descripci√≥n: el comienzo de una entidad personal.
>Esto es lo que significa "beeper".
>La identidad de Sarah I, la identidad de Thompson I y todo es todo lo externo a una entidad.
>
>Universidad de Oxford, como pueden ver, el comienzo de una entidad de organizaci√≥n, etc.
>As√≠ que todos los tokens, incluso el punto, la coma, todo.
>Supongamos que se identifican los signos.
>Esta es la verdad.
>Este es nuestro punto de referencia.
>Esta es nuestra l√≠nea base.
>Y con este texto, lo usaremos para probar la puntuaci√≥n f1, la precisi√≥n, etc.


### 29. Obtain the Prediction table from the LLM - Google Gemini

>[!NOTE]
>
>Entonces, donde lo dejamos la √∫ltima vez, ten√≠amos esta cadena que usamos para establecer una especie de punto de referencia, y luego creamos esta etiqueta de verdad fundamental.
>Y, por cierto, todos estos son tokens, ¬øverdad?
>
>As√≠ que tenemos tokens, la etiqueta de verdad y la descripci√≥n para que podamos entender de forma sencilla y f√°cil lo que intentamos hacer.
>Y lo creamos manualmente.
>As√≠ que lo que vemos aqu√≠ ha sido configurado manualmente por una persona.
>
>Y lo que queremos hacer ahora es asegurarnos de que nuestro modelo de lenguaje grande intente hacer exactamente lo que hemos hecho aqu√≠.
>Para eso, he creado esta instrucci√≥n.
>Y la instrucci√≥n es la siguiente:
>Basada en este texto.
>Y he creado un texto aqu√≠, el que tenemos.
>
>Quiero que identifiques todos los tokens o palabras, los cuantifiques y, digamos, los etiquetes con las siguientes etiquetas.
>Etiqu√©talos con las siguientes etiquetas.
>Suena tonto, pero funciona.
>Estas son todas las etiquetas posibles que tenemos:
>
>Fecha, inicio de una entidad de fecha, B-loc, inicio de una entidad de asignaci√≥n.
>Estos son b√°sicamente todos los elementos que tenemos.
>Col√≥calos en una tabla y mu√©strame dos columnas:
>Nombre de la entidad y una etiqueta.
>
>Ahora, si miras aqu√≠ y volvemos a nuestra tabla, he a√±adido algunos filtros.
>Estos son todos los valores posibles que tenemos.
>Y aqu√≠, de nuevo, todos los valores posibles que tenemos.
>Todos forman parte de la solicitud.
>Lo que he hecho es tomar esta solicitud y usar Google Gemini.
>
>![Gemini Prompt](images/2025-08-14_101501.png "Gemini Prompt")
>
>![Agrupamiento por entidad o Token](images/2025-08-14_101821.png "Agrupamiento por entidad o Token")
>
>Y lo que s√≠ me gusta de Gemini.
>
>Me ha dado varias opciones, ya sea de esta manera, y las entidades se agrupan por entidad o por token, y esto es lo que queremos.
>Queremos por token.
>Esto es lo mejor.
>
>Ahora exportar√© esto a hojas de c√°lculo.
>Luego copiaremos esto.
>Y los tendremos uno tras otro.
>As√≠ que obtendr√© esta columna completa.
>Veamos si puedo copiar as√≠.
>Y lo insertar√© aqu√≠.
>
>Y simplemente dir√© "ID de etiqueta".
>Pero esto es una predicci√≥n.
>Predicci√≥n.
>Y ahora tenemos la predicci√≥n.
>Y tenemos el ID de etiqueta de verdad fundamental.
>
>Y con base en estos elementos, calcularemos diferentes m√©tricas que usaremos para evaluar la eficiencia y precisi√≥n de nuestros modelos.

### 30. Machine Learning Metrics - Accuracy

>[!NOTE]
>
>**¬øQu√© es exactamente la precisi√≥n?**
>
>El nombre es bastante indicativo y simple.
>La precisi√≥n en s√≠ misma representa la proporci√≥n de predicciones correctas realizadas por el modelo.
>De todas las predicciones.
>Para simplificar,
>
>Imagina que haces un examen con 100 preguntas y preguntas.
>Responderemos estas preguntas e imaginaremos que, de las 100 preguntas, respondes 80 correctamente, por lo que
>tu precisi√≥n es del 80 %.
>Esto es algo que tambi√©n monitoreamos para los modelos.
>
>Podr√≠as preguntarte cu√°l es la precisi√≥n o la eficiencia de un modelo.
>Luego lo sometes a una prueba.
>Como has visto en los ejercicios de evaluaci√≥n comparativa, la mayor√≠a de las veces probamos la precisi√≥n.
>De unas 400 preguntas, obtienes 50 correctas.
>Eso te dar√° una puntuaci√≥n correcta.
>Esta es la precisi√≥n.
>
>Se trata del n√∫mero de predicciones correctas dividido entre el n√∫mero total de predicciones.
>Esta es la m√©trica m√°s f√°cil de implementar y medir.
>Y, por supuesto, cuanto mayor, mejor.
>Cuanto mayor sea la precisi√≥n, mejor ser√° el modelo.
>
>Ahora bien, hay algo que comentar sobre los datos desequilibrados.
>Recuerden que su modelo podr√≠a estar entrenado con algo.
>Y perm√≠tanme darles un ejemplo m√°s pr√°ctico.
>
>Imaginemos que prueban su modelo con un conjunto de datos muy limitado y peque√±o.
>Imaginemos simplemente que lo prueban con f√≠sica y su modelo obtiene una precisi√≥n del 95 %.
>Y dir√°n: "¬°Guau! ¬°He creado el Santo Grial!".
>¬øVerdad?
>
>As√≠ que he reinventado a Einstein.
>Pero una vez que pongas tu modelo en pr√°ctica (imag√≠nate en qu√≠mica, no en f√≠sica, o quiz√°s en inform√°tica), ver√°s que tu modelo ya no predice con un 95% de precisi√≥n, sino con un 20%.
>Estos son datos desequilibrados.
>
>![Accuracy](images/2025-08-14_102452.png "Accuracy")
>
>Si has probado tu modelo solo con un peque√±o conjunto de datos, esto no es relevante para todos los datos que quieres.
>As√≠ que ten cuidado con esto.
>
>As√≠ que, al probar la precisi√≥n, piensa siempre de esta manera: ¬øel conjunto de datos que estoy usando es relevante para lo que quiero hacer en la vida real?
>¬øSon mis datos de entrenamiento relevantes o est√°n desequilibrados en comparaci√≥n con lo que quiero hacer?
>Estas son preguntas muy importantes y, como vimos antes, ¬øentra basura y sale basura del entrenamiento?
>Los datos en s√≠ mismos son muy importantes.
>Ahora bien, todo se reduce a la precisi√≥n.

### 31. Machine Learning Metrics - Precision

>[!NOTE]
>
>Otra m√©trica importante que valid√≥ esto se refiere a la precisi√≥n y a la definici√≥n.
>Esto se refiere a la proporci√≥n de identificaciones positivas que fueron realmente correctas.
>Es un poco complicado.
>Pero perm√≠tanme explicar dos conceptos m√°s.
>Tenemos un falso positivo y un falso negativo.
>
>Si se dedican a la ingenier√≠a de calidad, saben que un falso positivo se produce cuando las pruebas automatizadas identifican un defecto, pero este no existe.
>Podr√≠a tratarse de una prueba deficiente que est√° fallando.
>Y creen que hay un problema en su aplicaci√≥n.
>Eso s√≠ que es un falso positivo.
>
>Un falso negativo significa que hay un problema que no pudieron identificar con la prueba.
>Es decir, es al rev√©s.
>
>**¬øQu√© es exactamente la precisi√≥n?**
>
>¬øPrecisi√≥n significa cu√°ntos verdaderos positivos?
>
>Entonces, ¬øcu√°ntos hallazgos reales, correctos, tienes o el modelo ha identificado, dividido entre este n√∫mero de hallazgos m√°s los falsos positivos, aquellos que se han identificado como correctos, pero que en realidad no lo son?
>Y continuando.
>
>**¬øD√≥nde es importante la precisi√≥n?**
>
>Es importante medir en los casos en que los falsos positivos son costosos, donde identificarlos e investigarlos resulta muy costoso.
>Tomemos un sistema de detecci√≥n de spam.
>Como saben, actualmente todas las empresas importantes tienen filtros que identifican cu√°ndo un correo electr√≥nico es spam y tratan de identificar los correos electr√≥nicos que no lo son.
>Por lo tanto, un falso positivo en este caso es muy indeseable.
>Por lo tanto, no conviene que un correo electr√≥nico spam no sea bloqueado.
>Pero lo m√°s importante es que no conviene.
>
>Por lo tanto, no conviene que un buen correo electr√≥nico se etiquete como spam.
>Eso es algo muy, muy malo.
>Esto es un falso positivo.
>Cuando un correo electr√≥nico positivo no es spam, se marca como tal.
>Entra en tu carpeta y nunca la vuelves a revisar.
>
>![Presision](images/2025-08-14_105136.png "Presision")
>
>Ahora veamos un ejemplo.
>Imagina que tu modelo indica: ¬´Oye, recibiste 50 correos electr√≥nicos spam, as√≠ que, bien, est√°s haciendo tu trabajo correctamente¬ª.
>
>Y luego, al revisar los correos electr√≥nicos para validar que el modelo identifica el spam correctamente, ves que diez de ellos no lo son.
>Son de un socio comercial.
>Y has estado esperando algunos de esos correos.
>
>En este caso, la precisi√≥n es 40 dividido entre 50, ya que 40 eran correctos, pero 50 se han marcado como spam.
>As√≠ que tienes un 80 %.
>
>Digamos que tu precisi√≥n es de 0,8, o podr√≠as marcarla como 80 %.
>Y, de nuevo, imagina que la precisi√≥n es importante.
>Sabes d√≥nde un falso positivo es realmente indeseable.
>


### 32. Machine Learning Metrics - Recall

>[!NOTE]
>
>Dado que hemos hablado de precisi√≥n, tambi√©n debemos abordar la recuperaci√≥n, que es muy similar a la precisi√≥n, pero con una peque√±a diferencia.
>La recuperaci√≥n mide la proporci√≥n de casos positivos reales que el modelo identific√≥ correctamente.
>Para facilitar la comprensi√≥n,
>Se tienen los verdaderos positivos divididos entre el n√∫mero de verdaderos positivos m√°s los falsos negativos.
>
>En la lecci√≥n anterior, donde analizamos la precisi√≥n, tuvimos un falso positivo.
>La recuperaci√≥n se centra en los falsos negativos.
>
>**¬øCu√°ndo podr√≠a ser importante?**
>
>La recuperaci√≥n es muy √∫til en casos donde la omisi√≥n de un resultado positivo es muy costosa, como en el diagn√≥stico m√©dico o la detecci√≥n de fraude.
>En estos casos, imaginemos que una IA escanea c√©lulas cancerosas.
>Se proporciona una imagen (una tomograf√≠a computarizada, una radiograf√≠a), y la IA la analiza y no indica nada.
>Esto es un falso negativo porque hay algo ah√≠ y no quieres pasarlo por alto.
>
>**¬øLo mismo ocurre con el fraude o el blanqueo de capitales?**
>
>Podr√≠a ser que las se√±ales est√©n ah√≠, pero no pueda detectarlas.
>En este caso, la memoria es muy importante.
>
>Creo que deber√≠amos usar un ejemplo para simplificarlo.
>Imagina que sabes que tienes 100 pacientes con c√°ncer que dieron positivo.
>Sabes que hay 100.
>Y le proporcionas una radiograf√≠a de estos pacientes a tu modelo y, por supuesto, 100 im√°genes.
>
>![Recall](images/2025-08-14_110456.png "Recall")
>
>Y luego el modelo analizar√° imagen por imagen y dir√°: "Bueno, el 80% u 80 de estos pacientes o im√°genes presentan un s√≠ntoma, no s√©, con un 95% de precisi√≥n".
>As√≠ que el modelo identific√≥ correctamente solo ocho de cada diez, u 80 de cada cien.
>Pero sabemos, gracias a los datos que proporcionamos, que hay otros 20 pacientes y el modelo pudo identificarlos.
>
>As√≠ que, si esto no fuera un ejercicio de entrenamiento y fuera una situaci√≥n real, esas 20 personas se ir√≠an a casa sin un diagn√≥stico correcto y seguir√≠an sin saber qu√© les pasa.
>As√≠ que la tasa de recuperaci√≥n en este caso ser√≠a del 0,8%, o del 80%.
>


### 33. Machine Learning Metrics - F1 Score

>[!NOTE]
>
>**Ya llegamos a la famosa puntuaci√≥n F1.**
>
>Si han le√≠do en l√≠nea o investigado un poco, probablemente habr√°n visto que la puntuaci√≥n F1 es muy importante y que la mayor√≠a de las veces se la menciona.
>
>Ahora les doy un poco de informaci√≥n sobre la puntuaci√≥n F1, ya que para calcularla tambi√©n se necesita precisi√≥n y recuerdo.
>No se puede tener solo uno de ellos.
>
>La puntuaci√≥n F1 en s√≠ misma es una especie de promedio entre la precisi√≥n y el recuerdo.
>No es un promedio aritm√©tico, sino un promedio arm√≥nico entre ambos.
>Y cuando es importante, en primer lugar, se aplica cuando hay clases desequilibradas.
>
>Entonces, cuando quiz√°s el 80 % de la poblaci√≥n reside en el 20 % del intervalo total.
>Podr√≠a tratarse de una clase desequilibrada, ya que el modelo perder√° precisi√≥n, o su recuperaci√≥n ser√° mayor, o su precisi√≥n ser√° mayor o menor, dependiendo de su posici√≥n en una determinada clase.
>
>La puntuaci√≥n F1 mide el rendimiento del modelo en todo el espectro, en toda la poblaci√≥n o en todo el conjunto de datos.
>Si quiere saberlo en forma de f√≥rmula, ah√≠ lo tiene.
>Correcto.
>
>Se dice promedio arm√≥nico entre precisi√≥n y recuperaci√≥n.
>¬øY cu√°ndo desea esto?
>Bueno, digamos que normalmente lo har√≠a.
>Todos desean una puntuaci√≥n F1 alta.
>
>Si tom√°ramos un ejemplo m√©dico que ya hemos usado con el paciente con c√°ncer, ser√≠a algo como esto:
>Cuando se tiene un modelo de diagn√≥stico m√©dico, el que no detecta c√°ncer, sino cualquier otra cosa, como un manguito de sangre, o algo similar,
>la alta precisi√≥n garantiza que se sepa qu√© tan positivo es un verdadero positivo.
>
>![F1 Score](images/2025-08-14_133844.png "F1 Score")
>
>Entonces, cuando se dice que hay un 98% de probabilidad de que algo est√© presente, se tiene la certeza de no proporcionar un falso negativo.
>Esto se refiere a alta precisi√≥n.
>Pero tambi√©n en este caso, se debe asegurar que la mayor√≠a de los pacientes tengan esta afecci√≥n identificada.
>
>Por lo tanto, se debe asegurar que la afecci√≥n est√© presente.
>Y tambi√©n se debe asegurar que, en todos los casos donde se identific√≥ la afecci√≥n, est√© presente correctamente.
>Y es por eso que la mayor√≠a de las empresas, la mayor√≠a de los puntos de referencia, no utilizan la precisi√≥n ni la recuperaci√≥n, sino la puntuaci√≥n F1, que es una combinaci√≥n de precisi√≥n y recuperaci√≥n.
>Y se utiliza muy a menudo con clases desequilibradas.
>


### 34. Machine Learning Metrics - Perplexity

>[!NOTE]
>
>**Perplejidad.**
>
>Se refiere a la idea de predicci√≥n.
>La perplejidad se aplica √∫nicamente a grandes modelos ling√º√≠sticos basados en texto.
>F√°cil de entender.
>
>La perplejidad significa que escribes un mont√≥n de palabras y el modelo indica la probabilidad de que la siguiente palabra sea similar a la de Google cuando intentas buscar algo en l√≠nea.
>
>Google te dar√° pistas sobre cu√°l es la siguiente palabra.
>Se utiliza en grandes modelos ling√º√≠sticos para la generaci√≥n de texto, y espec√≠ficamente para la traducci√≥n, para evaluar la confianza del modelo en predecir la siguiente palabra.
>Una perplejidad menor significa que el modelo es m√°s preciso y est√° mejor entrenado.
>
>**¬øCu√°l es la f√≥rmula?**
>
>Bueno, ah√≠ lo tienes.
>Eh, y en realidad n significa el n√∫mero de palabras, y p es la probabilidad de que una palabra est√© ah√≠.
>As√≠ que es una funci√≥n logar√≠tmica.
>En realidad, es una suma de una proporci√≥n logar√≠tmica.
>
>Ahora veamos un ejemplo adecuado para entenderlo, porque no hace falta ser inform√°tico, matem√°tico ni estad√≠stico para entenderlo y comprobarlo. Simplemente, podemos decir:
>
>S√≠, el √≠ndice de perplejidad est√° bien para nosotros.
>Estamos por debajo del umbral.
>
>![Perplexity](images/2025-08-14_135439.png "Perplexity")
>
>Ahora, perm√≠tanme darles un ejemplo muy simple.
>
>Tienen una oraci√≥n.
>El zorro marr√≥n salta sobre...
>Y la oraci√≥n es "sobre el perro perezoso" porque es una oraci√≥n muy com√∫n en las estad√≠sticas de aprendizaje autom√°tico debido a las propiedades de las palabras.
>Entonces, despu√©s de decir "el zorro marr√≥n salta sobre all√≠", el modelo podr√≠a decir, bueno, las palabras probables
>que vienen despu√©s podr√≠an ser A, B, C, D y luego "perezoso".
>
>Y dos palabras m√°s.
>Entonces, si tienen siete, el modelo tiene una perplejidad de siete, porque cree que una de siete palabras estar√° all√≠.
>En caso de decir "el zorro marr√≥n salta sobre", lo m√°s probable es que sea un art√≠culo sobre all√≠,
>y la perplejidad deber√≠a ser uno para cualquier tipo de modelo que tengan.
>


### 35. Demo - Evaluate - Calculate Metrics with Python

1. Vamos a la p√°gina <img alt="Hugging Face's logo" src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="32" height="32"> [Hugging Face -> Documentation](https://huggingface.co/docs).
2. Buscamos el que dice [`Evaluate`](https://huggingface.co/docs/evaluate/index).</br><img alt ="" src ="https://huggingface.co/datasets/evaluate/media/resolve/main/evaluate-banner.png" height="32">
3. **Al abrir esto, ¬øqu√© es exactamente "evaluar"?**
Es una biblioteca para evaluar f√°cilmente modelos y conjuntos de datos de aprendizaje autom√°tico.
Ya est√° creada y proporciona una especie de capa de envoltorio para que no tengas que hacer c√°lculos matem√°ticos para diferentes tipos de m√©tricas.
Simplemente di "calcular esta m√©trica".
Y eso es todo lo que necesitas hacer.
¬øC√≥mo lo instalamos?
Bueno, si vas a la instalaci√≥n, te indicar√° que necesitas algo m√°s que Python.
3.7.
4. Creamos el archivo **`src/test/LLM/Hug_face/evaluate_demo.py`**.
5. **¬øQu√© puedes hacer con esto?** </br>
[![Choosing a metric for your task](images/2025-08-15_083113.png "Choosing a metric for your task")](https://huggingface.co/docs/evaluate/choosing_a_metric#choosing-a-metric-for-your-task)
Primero que nada, calcularemos varias m√©tricas.
Ejecutaremos varios benchmarks.
Si ves la gu√≠a, indica c√≥mo elegir la m√©trica para tu tarea.
Calcularemos el azul junto con el desgarro y, de hecho, haremos benchmarks de ChatGPT.
6. Empezamos poniendo esto en el archivo **`evaluate_demo.py`**:
```py
import evaluate

# load the metrics
accuracy_metrics = evaluate.load("accuracy")
# f1_metrics = evaluate.load("f1")
# precision_metric = evaluate.load("precision")
# recall_metric = evaluate.load("recall")

# Define predictions and references
predictions = [0, 1, 1, 0, 1]
references = [0, 1, 0, 0, 1]

# Compute the accuracy
accuracy_result = accuracy_metrics.compute(
    predictions=predictions, references=references)
print("Accuracy:", accuracy_result)
```

>[!IMPORTANT]
>
> ### Ambiente Virtual de Python en Visual Studio Code
>
> 1. En `Visual Studio Code` En el men√∫ superior</br>¬ª `View` </br>¬ª `Command Palette...` </br>¬ª `Python: Select Interpreter`
> 2. Selecciono `Venv Creates as '.venv'` o `+ Create Virtual Environment` y elijo de donde se va a basar este ambiente virtual.</br> Espero un rato a que haga la instalaci√≥n y creaci√≥n de la carpeta **".venv"**.
> 3. En una `TERMINAL` el comando, para activar el Ambiente Virtual de Python: </br> `.venv/Scripts/activate`
> 4. Instalo la biblioteca requerida: </br> `pip install evaluate` </br> Tener presente que esto se instala dentro de la carpeta nueva **".venv"**.
> 5. Pruebo la ejecuci√≥n del nuevo archivo con este comando en una `TERMINAL`: </br> `python .\src\test\LLM\Hug_face\evaluate_demo.py` </br> <span style="color:red;">Y obtengo un error.</span>
> 6. Instalo una biblioteca que est√° faltando: </br> `pip install accuracy`
> 7. Vuelvo a probar el comando en una `TERMINAL` y estando en el Ambiente Virtual: </br> `python .\src\test\LLM\Hug_face\evaluate_demo.py` </br> <span style="color:green;">Y obtengo una respuesta:</span> </br> `Accuracy: {'accuracy': 0.8}`
> 8. Para ejecutar usando `Visual Studio Code`: </br>¬ª `View` </br>¬ª `Command Palette` </br>¬ª `Python: Select Interpreter` </br>¬ª `Recommended` </br> ¬ª  `‚èØÔ∏è` </br> ¬ª `Run Python File in Dedicated Terminal`
>
> As√≠ se ve el proceso de ejecuci√≥n: </br> ![.](images/2025-08-15_095625.gif "")






7. Agrego mas c√≥digo al archivo **`evaluate_demo.py`** y descomento las tres variables definidas al principio:
```py
# Compute the F1 score
f1_result = f1_metrics.compute(
    predictions=predictions, references=references, average="binary")
print("F1 score", f1_result)

# Compute the precision
precision_result = precision_metric.compute(
    predictions=predictions, references=references, average="binary")
print("Precision:", precision_result)

# Compute the recall
recall_result = recall_metric.compute(
    predictions=predictions, references=references, average="binary")
print("Recall:", recall_result)
```
8. Ejecuto el comando en la `TERMINAL`, estando dentro del ambiente virtual: </br> `python .\src\test\LLM\Hug_face\evaluate_demo.py` </br> Y este fue el resultado obtenido:
```bash
Downloading builder script: 6.79kB [00:00, 19.9MB/s]
Downloading builder script: 7.56kB [00:00, 25.7MB/s]
Downloading builder script: 7.38kB [00:00, 11.3MB/s]
Accuracy: {'accuracy': 0.8}
F1 score {'f1': 0.8}
Precision: {'precision': 0.6666666666666666}
Recall: {'recall': 1.0}
```

>[!TIP]
>
>Otras formas de instalar el Ambiente Virtual de Python:
> * En <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-windows" viewBox="0 0 16 16"><path d="M6.555 1.375 0 2.237v5.45h6.555zM0 13.795l6.555.933V8.313H0zm7.278-5.4.026 6.378L16 16V8.395zM16 0 7.33 1.244v6.414H16z"/></svg> Windows, vez de usar el comando `pyton` usar `py`: </br> [How do I create a python3 venv for Windows?](https://www.reddit.com/r/learnpython/comments/13jxxw7/how_do_i_create_a_python3_venv_for_windows/)
> * Para <?xml version="1.0" encoding="UTF-8"?><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-windows" viewBox="0 0 16 16"><path d="M6.555 1.375 0 2.237v5.45h6.555zM0 13.795l6.555.933V8.313H0zm7.278-5.4.026 6.378L16 16V8.395zM16 0 7.33 1.244v6.414H16z"/></svg> Windows es mejor usar lo que sugiere esta p√°gina, pero teniendo en cuenta el uso de `py` en vez de `python`: </br> [Local Python Virtual Environments using venv](https://github.com/denisecase/datafun-00-python-virtual-env?tab=readme-ov-file#step-1-create-the-virtual-environment)
>
> Para salir del Ambiente Virtual de Python, el comando en una `TERMINAL` es: </br> `deactivate`



### 36. Demo - Pytorch - Calculate Perplexity for a Model

1. El sitio a utilizar es este: </br> [<img alt="Transformers -> Installation" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png" width="200px" title="Transformers -> Installation">](https://huggingface.co/docs/transformers/installation)
2. Ejecuto en una `TERMINAL` el comando para levantar el [Ambiente Virtual de Python](#ambiente-virtual-de-python-en-visual-studio-code): </br> `.venv/Scripts/activate`
3. Seg√∫n el c√≥digo debo tener dos bibliotecas, ejecuto dos comandos en la `TERMINAL` donde tengo el Ambiente Virtual de Python: </br> `pip install torch` </br> `pip install transformers`
4. Creo el archivo **`src\test\LLM\Hug_face\perplex.py`**, con este c√≥digo:
```py
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import math

# Load the pre-trained model and tokenizer

model_name = "gpt2"  # You can change to "gpt2-medium", "gpt2-large", etc.
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Set the pad_token to the eos_token to handle padding correctly
tokenizer.pad_token = tokenizer.eos_token
model.eval()  # Set the model to evaluation mode


def calculate_perplexity(text):
    # Strip whitespace and validate that text is not emptu
    text = text.strip()
    if not text:
        print("Error: input text is empty. Please provide valid text.")
        return None

    # Toeknize and prepare the input with padding for batch compability
    encodings = tokenizer(text, return_tensors="pt",
                          padding=True, truncation=True)

    # Ensure input tensor dimensions are  expected
    input_ids = encodings["input_ids"]
    attention_mask = encodings["attention_mask"] if "attention_mask" in encodings else None

    # Check if the input tensor has valid content
    if input_ids.size(1) == 0:
        print("Error: Tokenization result in an empty input. Please check your text")
        return None

    # Calculate the loss (negaitve log likelihood)
    with torch.no_grad():
        outputs = model(input_ids=input_ids,
                        attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss.item()

    # Calculate perplexity
    perplexity = math.exp(loss)
    return perplexity


# Text to calculate perplexity on
text = ("Lorem ipsum dolor sit amet, consectetur adipiscing elit. "
        "Donec ullamcorper libero ut interdum posuere. "
        "Quisque et tincidunt ipsum, in cursus sem. "
        "Proin turpis turpis, tempus id lectus a, mollis ultricies leo. "
        "Integer mollis dolor fringilla, aliquam diam vitae, cursus enim.")

# Debugging: Print the input to verify it is not-empy after stripping
print(f"Debug: Received input - `{text}`")


perplexity =calculate_perplexity(text)

if perplexity is not None:
  print(f"Perplexity: {perplexity}")
```
5. Luego de ejecutar este comando en la `TERMINAL` con el Ambiente Virtual de Python: </br> `python .\src\test\LLM\Hug_face\perplex.py` </br> Sale un erro con esta sugerencia a instalar: </br> `pip install hf_xet`
6. Vuelvo a ejecutar en la `TERMINAL` con el Ambiente Virtual de Python: </br> `python .\src\test\LLM\Hug_face\perplex.py` </br> Y obtengo esta respuesta:
```bash
Debug: Received input - `Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec ullamcorper libero ut interdum posuere. Quisque et tincidunt ipsum, in cursus sem. Proin turpis turpis, tempus id lectus a, mollis ultricies leo. Integer mollis dolor fringilla, aliquam diam vitae, cursus enim.`
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
Perplexity: 31.946859967236634
```
7. Cerremos el Ambiente Virtual de Python con este comando: </br> `deactivate`


### Quiz 2: Chapter Quiz

>[!NOTE]
>
>![Quiz 2: Chapter Quiz](images/2025-08-15_164519.gif "Quiz 2: Chapter Quiz")
>
>




## Section 6: RAG and Retrieval Augmented Generation Testing


### 37. What is RAG

>[!NOTE]
>
>Ahora hablemos de Rag.
>
>Significa recuperaci√≥n y generaci√≥n aumentada.
>Profundizaremos en ello en breve.
>Pero entonces, ¬øpor qu√© necesitamos rag?
>¬øPor qu√© no podemos simplemente vivir sin rag?
>
>Bueno, la idea, y he resumido aqu√≠ los cuatro puntos principales, es que un modelo...
>Para ser inteligente, necesita ser entrenado con datos.
>Y, por lo tanto, el modelo es tan inteligente como los datos que tiene.
>Y cuanto m√°s tiempo funcione el modelo y sus datos se vuelvan obsoletos, mayor ser√° el deterioro de su inferencia.
>
>Esto se denomina deterioro del modelo o deriva del modelo.
>La idea es que, en caso de necesitar datos que no estaban disponibles cuando se entren√≥ el modelo,
>debe obtenerlos de una fuente externa.
>Y esto es lo que hace RAG: buscar en bases de datos externas, bases de datos de conocimiento, para brindarte m√°s informaci√≥n sobre tu necesidad espec√≠fica.
>
>La otra se centra en las alucinaciones y la desinformaci√≥n, porque si le pides al modelo que haga algo y este no sabe exactamente c√≥mo hacerlo o no tiene la respuesta, comenzar√° a alucinar.
>Por lo tanto, si la informaci√≥n que buscas est√° en una base de datos externa, el modelo la obtendr√° y, por lo tanto, obtendr√°s una respuesta correcta con mayor probabilidad que una alucinaci√≥n.
>
>La otra se centra en el contexto y, por ejemplo, el l√≠mite de tokens y el l√≠mite de memoria.
>Sabemos que los lenguajes de modelado tienen un l√≠mite de memoria, pero con Rag puedes ampliar esa memoria con la cantidad de base de datos que tengas.
>
>Rag tambi√©n resuelve el problema de la memoria de tu modelo de lenguaje grande, y quiz√°s algo simple o com√∫n que los usuarios no ven.
>¬øEst√° esta parte aqu√≠?
>Porque si quieres usar IA en tu propia empresa, lo m√°s probable es que tengas datos que no est√°n disponibles abiertamente en internet.
>Esos datos son tu tesoro.
>Y puedes y debes protegerlos a toda costa.
>
>Por lo tanto, ning√∫n modelo se ha entrenado con esos datos.
>Por esta raz√≥n, conviene entrenar tu modelo con esos datos o ampliar sus capacidades, sus capacidades de conocimiento, ampli√°ndolo con los datos de tu propia empresa.
>
>![Why do we need RAG?](images/2025-08-19_154155.png "Why do we need RAG?")
>
>En este caso, todo se reduce a la seguridad.
>Ahora bien, no se desea que los datos est√©n disponibles para que todos los vean.
>
>Se busca que el modelo utilice los datos en un contenedor espec√≠fico, en la instancia, en la propia infraestructura, para controlar c√≥mo se transfieren los datos a la infraestructura.
>Estas ser√≠an algunas de las razones por las que necesitamos RAC.
>Pero, en la mayor√≠a de los casos, para los usuarios finales ser√° este.
>Y para las empresas, b√°sicamente ser√° este.
>
>Ahora, si continuamos y analizamos qu√© es exactamente RAC,
>Rag consta de tres partes: recuperador, aumento y generaci√≥n.
>
>Para ejecutar RAC, se necesitar√≠an, por ejemplo, bases de datos.
>Se llaman bases de datos de conocimiento y pueden ser vectoriales, estructuradas o no estructuradas, ya que actualmente tambi√©n se puede trabajar con bases de datos no estructuradas o, mejor dicho, no vectoriales.
>
>**¬øC√≥mo funciona esto?**
>
>![What is RAG?](images/2025-08-19_154854.png "What is RAG?")
>
>Expliquemos rag de forma sencilla.
>
> * El usuario utiliza cualquier tipo de aplicaci√≥n generativa.
>O quiz√°s simplemente est√© generando un modelo de lenguaje extenso.
>
> * La solicitud se incorporar√° a este modelo de recuperaci√≥n.
>¬øY qu√© hace el modelo de recuperaci√≥n?
>Tomar√° tu solicitud y buscar√° en su base de datos, ya sea vectorial o no, fragmentos de informaci√≥n que contengan la respuesta a tus preguntas.
>Porque, por ejemplo, la informaci√≥n de un documento se divide en fragmentos que se superponen.
>Y tambi√©n hablaremos de eso.
>
> * Entonces, esta recuperaci√≥n buscar√° uno o m√°s documentos en la base de datos vectorial.
>Tomar√° la solicitud que enviaste aqu√≠ y a√±adir√° contexto adicional al resultado de la b√∫squeda, si puedo seleccionarlo.
>Ahora, la representaci√≥n vectorial de la parte que responde a las preguntas de tu consulta se enviar√° a ambos, por lo que se ampliar√°.
>
> * Esta es la parte de ampliaci√≥n.
>Ampliar√° tu solicitud con el contexto que obtienes de tu base de datos externa.
>Y todo se enviar√° al modelo de lenguaje general que resumir√° esa informaci√≥n.
>O quiz√°s lo ampl√≠e seg√∫n lo que escribas en la solicitud, ya que tu solicitud podr√≠a ser:
>
> * Oye, ¬øpuedo encontrar esa informaci√≥n aqu√≠ y escribir una publicaci√≥n al respecto?
>O podr√≠a ser un resumen.
>Dependiendo de lo que tengas aqu√≠.
>Y todo esto se incorporar√° a este modelo.
>Y luego, el modelo te devolver√° la respuesta al usuario.
>Esta es la funcionalidad b√°sica.
>
> * La recuperaci√≥n es esta parte que recupera tu informaci√≥n.
>Entonces, esta recuperaci√≥n y el aumento ocurren aqu√≠ cuando la recuperaci√≥n construye tu solicitud.
>Y luego la generaci√≥n ocurre en el modelo de lenguaje completo.
>Y luego, le devuelves esta informaci√≥n al usuario.
>Perm√≠tanme mostrarles algunos ejemplos que tengo con diferentes herramientas.
>
>Por ejemplo, si vas a ChatGPT, solo un ejemplo.
>Subes un documento y dices: "Buscando este documento, por ejemplo, busca este documento"
>para la informaci√≥n x, y x podr√≠a ser, no importa cu√°l, y agregas el documento.
>Esto es de A.
>
>![Chat-GPT RAG](images/2025-08-19_155615.png "Chat-GPT RAG")
>
>As√≠ que esto no es un seguimiento porque no busca en bases de datos externas.
>Pero sigue el concepto de imaginar que tu documento est√° almacenado en una base de datos externa.
>Es lo mismo.
>Pero t√©cnicamente esto no es una recuperaci√≥n de generaci√≥n aumentada.
>La otra opci√≥n es si has creado tu propio GPT personalizado.
>Por ejemplo, aqu√≠ tengo una gu√≠a de autoservicio.
>Y si quiero editar este GPT, tengo la posibilidad de agregar algunos documentos.
>Y aqu√≠ he agregado el manual del Samsung S22.
>Ahora, si borro y actualizo, ver√°s que tarda un poco.
>Y si lo subo de nuevo, digamos que aqu√≠ va a ser el manual S22.
>Y si lo vuelvo a actualizar, ver√°n que el documento no solo se actualiza, sino que se integra y se desglosa en una base de datos vectorial.
>Por eso est√° tardando tanto este proceso de actualizaci√≥n.
>
>Y ahora, si solicito algo sobre este manual, es una especie de jerga.
>Pero nosotros, como usuarios, a√∫n no lo vemos porque desconocemos el funcionamiento interno de ChatGPT.
>Podr√≠a ser una jerga o podr√≠a ser otra.
>Y porque no lo sabemos.
>¬øQu√© hay del uso de la b√∫squeda web?
>¬øEs correcto?
>S√≠.
>Porque si activo la b√∫squeda web, ChatGPT acceder√° al sitio web, tomar√° la informaci√≥n que busco, ampliar√° mi solicitud y me dar√° la respuesta.
>
>Esto es "FLOWISAI".
>Es un marco de orquestaci√≥n de agentes.
>Aqu√≠ tengo algunos flujos de agentes y una herramienta de RAG.
>
>![FlowiseAI - > RAG](images/2025-08-19_155834.png "FlowiseAI - > RAG")
>
>Veamos qu√© estoy haciendo exactamente.
>Primero, configuro el modelo de IA abierta.
>Aqu√≠ est√° mi temperatura.
>Aqu√≠ est√° el modelo que quiero usar.
>Tambi√©n proporciono mi clave API.
>
>Este modelo lo usa un supervisor que controla a todos mis dem√°s trabajadores.
>Este es un enfoque de tres pasos para hacer una pregunta sobre un documento.
>Luego, estos modelos lo perfeccionan hasta que obtengo una publicaci√≥n sobre ese documento o una publicaci√≥n en redes sociales sobre mi consulta.
>
>![.](images/2025-08-19_160151.png "")
>
>Por ejemplo, este agente o parte del trabajador busca mi informaci√≥n en la base de datos.
>El otro crea el contenido bas√°ndose en la informaci√≥n obtenida, y el otro crea una publicaci√≥n de blog basada en este creador de contenido.
>
>Se trata de un enfoque de tres pasos, pero lo que queremos analizar es qu√© sucede con el trabajador que obtiene mi base de conocimientos.
>El trabajador utiliza una herramienta de recuperaci√≥n.


### 38. 4 Types of RAG - Simple, Speculative, Graph, Corrective

>[!NOTE]
>
>Hay much√≠simos tipos de trapo, pero en mi caso, seleccionar√© solo las tres o cuatro t√©cnicas o implementaciones de trapo m√°s populares que tenemos.
>En esta lecci√≥n, presentamos las siguientes cuatro t√©cnicas.
>
>![RAG Types](images/2025-08-19_160616.png "RAG Types")
>
> As√≠ que ser√° el modelo est√°ndar.
>Lo que ya han visto es el modelo correctivo, que b√°sicamente implementa una especie de bucle de retroalimentaci√≥n para garantizar que la informaci√≥n que el sistema les ha entregado sea correcta.
>As√≠ que se realiza una especie de verificaci√≥n de datos.
>Est√° el modelo especulativo, que consiste en adivinar y tener m√∫ltiples opciones antes de obtener el resultado.
>Y el modelo gr√°fico, que utiliza bases de datos de grafos junto con bases de datos vectoriales para encontrar las relaciones entre los nodos del grafo.
>
> * Ahora, si vamos al estante **est√°ndar**, como ya vieron, quiero a√±adir algo m√°s.
>Buscan en un documento, pero tambi√©n pueden buscar en varios.
>El sistema utiliza un mecanismo de puntuaci√≥n.
>Por ejemplo, buscar√° en diez documentos diferentes.
>El sistema aplica un mecanismo de puntuaci√≥n a los diez documentos y, en funci√≥n de esta, les proporciona la informaci√≥n m√°s importante para su b√∫squeda.
>Adem√°s, no hay nada m√°s.
>Se trata b√°sicamente de una puntuaci√≥n m√°xima de k de los documentos.
>
> * Pasemos al siguiente.
>Este es el recuadro **correctivo**.
>Es b√°sicamente una especie de recuadro.
>Es un recuadro que hemos visto y que tiene otra capa de validaci√≥n.
>En resumen, sigue existiendo la generaci√≥n aumentada de recuperaci√≥n est√°ndar, pero con verificaci√≥n de datos.
>As√≠ que la recuperaci√≥n y el... te proporcionar√°n la informaci√≥n.
>Pero luego habr√° una capa adicional que buscar√° los hechos o validar√° que la informaci√≥n obtenida sea objetivamente correcta.
>Es una especie de ciclo de retroalimentaci√≥n.
>Imagina que un profesor te asigna una tarea y le das la respuesta, pero el profesor corrige tu trabajo.
>Esto es b√°sicamente lo que est√°s haciendo o le pides a un colega que revise tu trabajo.
>Esto es un "correcci√≥n" o "correcci√≥n".
>Implementa una verificaci√≥n adicional de la respuesta que recibir√°s.
>Y como puedes ver aqu√≠, esta ser√° tu verificaci√≥n adicional que se a√±ade al contexto.
>Y luego obtendr√°s una respuesta, con suerte, v√°lida.
>
> * El siguiente es un **"correcci√≥n especulativa"**,  significa lo siguiente:
>Imagina que no sabes exactamente, o que tu sistema no sabe exactamente, cu√°l es la mejor respuesta, as√≠ que intentar√° especular.
>Esto a su vez tiene tres partes:
>El recuperador, el redactor y la verificaci√≥n.
>La recuperaci√≥n recuperar√° informaci√≥n y el redactor generar√° un conjunto de posibles respuestas para tu consulta o como posible respuesta.
>¬øQu√© pasar√° entonces con el documento y las posibles respuestas?
>Esta parte evaluar√° todas las respuestas y les asignar√° una puntuaci√≥n, un porcentaje, y la que tenga el mayor porcentaje de aciertos ganar√°. </br> </br>
>B√°sicamente, tienes: "Tengo esta consulta".
>Estas son mis posibles respuestas.
>Esta es la mejor.
>Imagina que se trata de un comit√© y conoces a tus arquitectos.
>Proponen cinco soluciones diferentes.
>Y luego, en un comit√©, eliges la mejor soluci√≥n y propones el resultado final.
>Este es un rack especulativo, muy √∫til cuando el modelo no sabe exactamente cu√°l es la informaci√≥n correcta, ni cu√°l es la perfecta. </br>
>Por ejemplo, no se obtiene una puntuaci√≥n del 98%.
>
> * El √∫ltimo que veremos (aunque no el √∫ltimo, hay uno m√°s que no voy a cubrir) es **Graph** Rag.
>B√°sicamente, se trata de grafos de conocimiento, y en ellos, la informaci√≥n se relaciona con otra mediante los nodos del grafo.
>Se utilizan nodos, no bases de datos ni bases de datos de grafos para almacenar informaci√≥n.
>Esto es muy √∫til, por ejemplo, cuando se desea obtener informaci√≥n de m√∫ltiples fuentes o realizar una investigaci√≥n exhaustiva, como la que realizan actualmente diferentes empresas.
>Se utiliza este tipo de grafo de conocimiento porque se necesita sintetizar informaci√≥n y validarla de una fuente con la otra.
>Y si quieres investigar en 20 art√≠culos diferentes, necesitas usar este tipo de gr√°fico de conocimiento, ya que la informaci√≥n se referencia en una u otra parte.
>
> * Si volvemos aqu√≠, hay otro tipo de filtro que no a√±ad√≠.
>Se trata del filtro gen√©tico, donde los agentes intentan obtener la mejor informaci√≥n y la revalidan.
>Pero el problema es que lleva mucho tiempo.
>As√≠ que no hay problema.
>Cuando quieres hacer algo y tienes tiempo para esperar, digamos cinco minutos.
>
>Pero en una aplicaci√≥n en tiempo real con un usuario, no tiene sentido usar un filtro gen√©tico, as√≠ que un filtro est√°ndar podr√≠a ser tu chatbot de respuesta a preguntas.
>
>La violaci√≥n correctiva podr√≠a darse cuando necesitas asesoramiento m√©dico, legal o financiero, porque la informaci√≥n necesita ser corregida.
>
>Un RAG especulativo es cuando no sabes exactamente cu√°l es la respuesta correcta y no est√°s seguro.
>As√≠ que quieres hacer este tipo de especulaciones, que podr√≠an ser las mejores.
>Por ejemplo, una predicci√≥n de qu√© comprar a continuaci√≥n.
>Si quiero ver qu√© podr√≠a comprar un cliente a continuaci√≥n bas√°ndome en lo que tiene en el carrito, podr√≠a especular.
>Oye.
>Podr√≠a ser esto o lo otro, pero no lo s√© con exactitud.
>As√≠ que solo voy a especular.
>Y esto podr√≠a ser para investigaci√≥n.
>Obtener informaci√≥n de varios documentos.


### 39. Create a RAG application with FAISS

>[!NOTE]
>
>Si recuerdan cuando analizamos nuestra arquitectura Rag, sabemos que, a diferencia de la IA tradicional, se requiere una base de datos vectorial.
>Y en esta base de datos vectorial se encuentra lo que se llama:
>Vectores.
>As√≠ que aqu√≠ solo se almacenan los √≠ndices vectoriales y las similitudes entre ellos.
>
>![RAG Archicture Model](images/2025-08-19_161732.png "RAG Archicture Model")
>
>Para el siguiente material, he usado esta.
>Les mostrar√©.
>iss
>[![Faiss](images/2025-08-19_162226.png "Faiss")](https://faiss.ai/), desarrollada por Facebook o Meta.
>
>Es una biblioteca para la b√∫squeda eficiente de similitudes y la agrupaci√≥n de vectores densos.
>Contiene algoritmos que buscan en conjuntos de vectores de cualquier tama√±o, incluso mayores que el m√≠o.
>Y luego, se profundiza en los detalles.
>
>En mi caso, al instalarla, he usado la CPU.
>As√≠ que, aunque no est√© ejecutando algo extremadamente grande, tambi√©n podr√≠a usar la GPU.
>Pero en mi caso, si no tienes una GPU dedicada y, por ejemplo, est√°s ejecutando localmente en tu port√°til, esto es lo que debes ejecutar.
>Si tienes un controlador compatible con Cuda, como AMD o Nvidia, tambi√©n puedes usar esta parte.
>No voy a usar PyTorch.
>

1. Empezamos creando el archivo **`src/rag/RAG_Simple.py`**, de entrada tengo estas importaciones:

|Nro.|Import -> Python|Source|
|-|-|-|
|1|`import os` |[Source code: Lib/os.py](https://docs.python.org/3/library/os.html)|
|2|`import fitz #PyMuPDF`| [PyMuPDF 1.26.3](https://pymupdf-readthedocs-io.translate.goog/en/latest/installation.html?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc)|
|3|`from typing import List` |[Source code: Lib/typing.py](https://docs.python.org/3/library/typing.html) |
|4|`from langchain_openai import OpenAIEmbeddings`|[LangChain -> Ecosystem packages](https://python.langchain.com/docs/how_to/installation/)|
|5|`from langchain_community.vectorstores import FAISS`|[LangChain -> Ecosystem packages](https://python.langchain.com/docs/how_to/installation/)|
|6|`from langchain.text_plitter import RecursiveCharacterTextSplitter`|[LangChain -> Ecosystem packages](https://python.langchain.com/docs/how_to/installation/)|
|7|`from langchain.schema import Document`|[LangChain -> Ecosystem packages](https://python.langchain.com/docs/how_to/installation/)|

2. Activar el Ambiente Virtual de Python con este comando en la `TERMINAL`: </br> `.venv/Scripts/activate`
3. Nos muestra un archivo **`src/rag/requirements.txt`**, que se instala con un comando en la `TERMINAL`, con base en este documento [Python Requirements.txt](https://www.freecodecamp.org/news/python-requirementstxt-explained/), este ser√≠a el contenido: </br> ![requirements.txt](images/2025-08-20_094436.png "requirements.txt"): </br> Ejecutar este comando en la `TERMINANL` con el Ambiente Virtual de Python: </br>`pip install -r ./src/rag/requirements.txt`
4. Reinstalar en nuestro Ambiente Virtual de Python con lo mas reciente:
```bash
pip uninstall pymupdf
pip install --upgrade pymupdf
pip uninstall langchain
pip install langchain
pip uninstall langchain-openai
pip install langchain-openai
pip uninstall langchain-community
pip install langchain-community
pip freeze 
```
5. Tengo en la carpeta **"documents"**, algunos archivos **`*.pdf`**, en este caso son manuales del celular _Samsung Galaxy 22_.
6. Este ser√≠a el contenido del archivo **`src/rag/RAG_Simple.py`**:
```py
import os
import fitz  # PyMuPDF
from typing import List
from dotenv import load_dotenv

# FAISS & Langchain imports
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# =========================================================
# FAISS & Document Loading Setup
# =========================================================
DB_FILE = "./src/rag/.faiss_index"
OPENAI_API_KEY = None  # Will be set after loading .env


def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text from a PDF file using PyMuPDF."""
    text = ""
    page_number = 0
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                page_text = page.get_text("text")
                page_number = page.number + 1
                text += page_text + "\n"
            print(f"Extracting text from page {page_number} of {pdf_path}")
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
    return text


def load_documents(folder_path: str) -> List[Document]:
    """Load all PDF and TXT from a folder and return a list of Document objects."""
    local_docs = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        print(f"Processing file: {filename}")
        if filename.endswith(".pdf"):
            text = extract_text_from_pdf(file_path)
        elif filename.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as file:
                text = file.read()
        else:
            continue
        if text.strip():
            local_docs.append(Document(page_content=text,
                                       metadata={"source": filename}))
    return local_docs


def create_vector_db(local_docs):  # List[Document]
    """Create a FAISS vector database from a list of Document objects."""
    print("Local docs type:", type(local_docs),
          "with length:", len(local_docs))
    if not local_docs:
        raise ValueError(
            "No documents provided to create the vector database.")
    if os.path.exists(DB_FILE):
        print(f"Database file {DB_FILE} already exists.")
        print("Please delete it before creating a new one.")
        return
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, chunk_overlap=100)
    docs = text_splitter.split_documents(local_docs)

    # Generating embeddings

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

    vector_db = FAISS.from_documents(docs, embeddings)

    # Save the vector store to disk
    vector_db.save_local(DB_FILE)
    print(f"Vector database created and saved to {DB_FILE}.")


def load_vector_db():
    """Create a FAISS vector database from documents in the 'documents' folder."""
    if os.path.exists(DB_FILE):
        return FAISS.load_local(DB_FILE, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), allow_dangerous_deserialization=True)
    else:
        raise FileNotFoundError(
            f"Database file {DB_FILE} not found. Please create the database first.")


def retrieve_elevant_docs(query: str, k: int = 3) -> List[Document]:
    """Retrieve the top-k relevant documents chunks from FAISS."""
    vector_db = load_vector_db()
    if vector_db:
        results = vector_db.similarity_search(query, k=k)
        return [doc.page_content for doc in results]
    else:
        print("Vector database not found or could not be created.")
        return []


# =========================================================
# Main Execution
# =========================================================
if __name__ == "__main__":
    FOLDER_PATH = "./src/rag/documents"
    load_dotenv()  # Carga las variables de entorno del archivo .env
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    print("OpenAI API Key:", OPENAI_API_KEY)

    # Load documents from the 'documents' folder
    documents = load_documents(FOLDER_PATH)
    create_vector_db(documents)

    while True:
        query = input("Enter your query (or 'exit' to quit): ")
        if query.lower() == 'exit':
            break
        relevant_docs = retrieve_elevant_docs(query)
        if relevant_docs:
            print("Relevant documents:")
            for idx, doc in enumerate(relevant_docs, 1):
                print(f"[{idx}] - {doc}\n")
        else:
            print("No relevant documents found.")
    print("Exiting the Simple RAG/Chatbot.")
```

7. Es necesario crear en la ra√≠z el archivo **`.env`** con esta l√≠nea: </br> `OPENAI_API_KEY = "sk-proj-A-VALID-API-KEY-HERE"`
8. En la `TERMINAL` y en mismo Ambiente Virtual de Python, ejecutamos este comando: </br> `python ./src/rag/RAG_Simple.py`.
9. Ahora puedo hacerle preguntas y estos son algunos ejemplos: </br> ![Can I charge my S22 wireless?](images/2025-08-20_120310.png "Can I charge my S22 wireless?") ![¬øPuedo cargar mi S22 de forma inal√°mbrica?](images/2025-08-20_120420.png "¬øPuedo cargar mi S22 de forma inal√°mbrica?")
10. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`



### 40. `RAGAs` Validation Framework - Retrieval

>[!NOTE]
>
>Para diferentes tipos de pruebas que desee realizar con su modelo de lenguaje extenso, no solo para rag, sino tambi√©n para otros conceptos.
>
>Existen muchas bibliotecas.
>Por ejemplo, existe la biblioteca "evaluate", que se centra m√°s en la matriz de precisi√≥n, o la matriz de confusi√≥n, que se obtiene de "hugging face".
>
>O puede usar algo un poco m√°s profesional, llamado `Ragas`, que es una biblioteca que proporciona herramientas para optimizar el rendimiento, pero est√° dise√±ada para ayudarle a evaluar sus aplicaciones de lenguaje extensas con facilidad y confianza.
>Facilidad.
>
>[![Ragas](images/2025-08-20_150704.png "Ragas")](https://docs.ragas.io/en/stable/)
>

1. En este sitio empezamos con [üöÄ Get Started](https://docs.ragas.io/en/stable/getstarted/)
2. Seleccionamos [Installation](https://docs.ragas.io/en/stable/getstarted/install/), para procesos de instalacion de `Ragas`.
3. Seleccionamos en la parte de arriba [üõ†Ô∏è How-to Guides](https://docs.ragas.io/en/stable/howtos/).
4. Vamos a este otro sitio [List of available metrics](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/).
5. Regresaos a [Evaluate a simple LLM application](https://docs.ragas.io/en/stable/getstarted/evals/).
6. Ejemplo de un prompt:
```py
from ragas import SingleTurnSample
from ragas.metrics import BleuScore

test_data = {
    "user_input": "summarise given text\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.",
    "response": "The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.",
    "reference": "The company reported an 8% growth in Q3 2024, primarily driven by strong sales in the Asian market, attributed to strategic marketing and localized products, with continued growth anticipated in the next quarter."
}
metric = BleuScore()
test_data = SingleTurnSample(**test_data)
metric.single_turn_score(test_data)
```
7. Un paso a paso para evaluar los RAG [Evaluate a simple RAG system](https://docs.ragas.io/en/stable/getstarted/rag_eval/), con este texto inicial:
```py
sample_docs = [
    "Albert Einstein proposed the theory of relativity, which transformed our understanding of time, space, and gravity.",
    "Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.",
    "Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.",
    "Charles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'.",
    "Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine."
]
```
8. He aqu√≠ ejemplos de consultas o _queries_ y Respuestas esperadas:
```py
sample_queries = [
    "Who introduced the theory of relativity?",
    "Who was the first computer programmer?",
    "What did Isaac Newton contribute to science?",
    "Who won two Nobel Prizes for research on radioactivity?",
    "What is the theory of evolution by natural selection?"
]

expected_responses = [
    "Albert Einstein proposed the theory of relativity, which transformed our understanding of time, space, and gravity.",
    "Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine.",
    "Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.",
    "Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.",
    "Charles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'."
]
```

9. Creamos el archivo **`src/rag/RAGAS_localy.py`**, que es una copia de **`RAG_Simple.py`**, con algunos cambios:
```py
import os
import fitz  # PyMuPDF
import json
from typing import List
from dotenv import load_dotenv

# RAGAS Evaluation Imports
from ragas.metrics import (
    context_precision,  # Measures if retrieved is relevantto the answer.
    context_recall  # Measures if all necessary information was retreieved.
)
from ragas import evaluate
from datasets import Dataset

# FAIS & LangChain Imports
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# =========================================================
# FAISS & Document Loading Setup
# =========================================================
DB_FILE = "./src/rag/.faiss_index"
OPENAI_API_KEY = None  # Will be set after loading .env


def extract_text_from_pdf(pdf_path: str) -> str:
    ...


def load_documents(folder_path: str) -> List[Document]:
    ...


def create_vector_db(local_docs):  # List[Document]
    ...


def load_vector_db():
    ...


def retrieve_elevant_docs(query: str, k: int = 3) -> List[Document]:
    ...

# =========================================================
# RAGAS Evaluation (Retrieval Testing Only)
# =========================================================


def evaluate_ragas(query: str, retrieved_docs: List[str], correct_answer: str):
    """Evaluates the RAG retrieval system using RAGAS metrics"""

    # Ensure retieved_docs is a list
    if not isinstance(retrieved_docs, list):
        retrieved_docs = [retrieved_docs]

    # Prepare data for evaluation
    evaluation_data = {
        "question": [query],
        "contexts": [retrieved_docs],  # Expected format: list of strings
        "answer": [correct_answer],
        "reference": [correct_answer]  # Required for context_precision
    }
    dataset = Dataset.from_dict(evaluation_data)

    # Run RAGAS evaluation
    scores = evaluate(dataset, metrics=[
        context_precision,
        context_recall
    ])

    # Convert to dictionary
    scores_dict = scores.__dict__

    # Remove non-serializable parts (e.g., evaluation_dataset) if present
    EVAL_DATA = "evaluation_dataset"
    if EVAL_DATA in scores_dict:
        scores_dict.pop(EVAL_DATA)
    # Extract only the scores
    only_scores = scores_dict.get("scores", {})

    print("\nüïØÔ∏è **RAG Retrieval Evaluation Scores:**")
    print(json.dumps(only_scores, indent=2, default=str))

# Example
# # query = "Can I charge my Galaxy S22 wirelessly?"
# # answer = ["Yes, you can charge your Galaxy S22 wirelessly using the Wireless power sharing feature. To use it, open Settings, go to Battery and device care, select Battery, and tap Wireless power sharing. Then tap Battery limit to set your desired threshold, and once that level is reached, wireless power sharing will automatically turn off."]


# =========================================================
# Main Execution
# =========================================================
if __name__ == "__main__":
    ...
        answer = input("‚úÖ Enter the correct answer for evaluation: ")
        evaluate_ragas(query, relevant_docs, answer)
        print("\n" + "="*50 + "\n")
        print("Evaluation complete. Scores printed above.")
        print("You can now continue with the next query or exit.")
    print("Exiting the RAGAS evaluation script.")
```
10. Primero Activamos el Ambiente Virtual de Python: </br> `.venv/Scripts/activate`
11. Luego ejecutamos el comando para correr el script: </br> `python ./src/rag/RAGAS_local.py`
12. La pregunta inicial ser√≠a: </br> `Can I charge my Galaxy S22 wirelessly?`
13. Y la respuesta para la evaluaci√≥n: </br> `Yes, you can charge your Galaxy S22 wirelessly using the Wireless power sharing feature. To use it, open Settings, go to Battery and device care, select Battery, and tap Wireless power sharing. Then tap Battery limit to set your desired threshold, and once that level is reached, wireless power sharing will automatically turn off.`
14. Luego de un rato esto es lo que se obtiene: </br> ![RAG Retrieval Evaluation Scores](images/2025-08-20_163906.png "RAG Retrieval Evaluation Scores")
15. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`




### 41. RAGAs Validation Framework - Retrieval - Augmentation - Generation

1. Empezamos creando el archivo **`src/rag/implement_gpt.py`**, similar a los anteriores con lagunos cambios:
```py
import os
import fitz  # PyMuPDF
from typing import List
from dotenv import load_dotenv  # Load environment variables
from openai import OpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Set API key or raise an error if not set
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    load_dotenv()  # Carga las variables de entorno del archivo .env
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# print("OpenAI API Key:", OPENAI_API_KEY)

# Instantiate OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# =========================================================
# FAISS & Document Loading Setup
# =========================================================
DB_FILE = "./src/rag/.faiss_index"

# Define the GPT model class
model_name = "gpt-3.5-turbo"  # Example model name, can be changed as needed


class GPTModel:
    def __init__(self, model_name=model_name):
        self.model_name = model_name
        # self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)

    def generate_response(self, prompt: str) -> str:
        """Generate a response using the GPT model."""
        completion = openai_client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "system", "content": "You are a helpful assistant."},
                      {"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content


def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text from a PDF file using PyMuPDF."""
    text = ""
    page_number = 0
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                page_text = page.get_text("text")
                page_number = page.number + 1
                text += page_text + "\n"
            print(f"Extracting text from page {page_number} of {pdf_path}")
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
    return text


def load_documents(folder_path: str) -> List[Document]:
    ...


def create_vector_db(local_docs):  # List[Document]
    ...



def load_vector_db():
    ...


def retrieve_elevant_docs(query: str, k: int = 3) -> List[Document]:
    ...


def chat_with_context(query: str, k: int = 3) -> str:
    """Generate a response to the query using retrieved relevant documents as context."""
    relevant_docs = retrieve_elevant_docs(query, k)
    context = "\n\n".join(relevant_docs)

    prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
    gpt_model = GPTModel()
    response = gpt_model.generate_response(prompt)
    return response


# =========================================================
# Main Execution
# =========================================================
if __name__ == "__main__":
    FOLDER_PATH = "./src/rag/documents"
    # load_dotenv()  # Carga las variables de entorno del archivo .env
    # OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    # print("OpenAI API Key:", OPENAI_API_KEY)

    # Load documents from the 'documents' folder
    documents = load_documents(FOLDER_PATH)

    # Create FAISS database (run only once per dataset)
    create_vector_db(documents)

    while True:
        user_query = input("Enter your query (or 'exit' to quit): ")
        if user_query.lower() == 'exit':
            break
        responseGPT = chat_with_context(user_query)
        if responseGPT:
            print("\n üí¨ ChatGPT Response: \n", responseGPT)
        else:
            print("No relevant documents found.")
    print("Exiting the ChatGPT RAG/Chatbot.")

```
2. Activo el Ambiente Virtual de Python: </br> `.venv/Scripts/activate`
3. Luego ejecutamos el comando para ejecutar el escript: </br> `python ./src/rag/implement_gpt.py`
4. Este ser√≠a el resultado esperado: </br> ![ChatGPT RAG/Chatbot](images/2025-08-20_172946.png "ChatGPT RAG/Chatbot")
5. Podemos Implementar algo parecido al **`RAGAS_local.py`**, para validar la respuesta de este **`implement_gpt.py`**.
6. Voy a crear el archivo **`src/rag/RAGAS_gpt.py`**, para probar la verificaci√≥n basado en **`RAGAS_local.py`**:
```py
import os
import fitz  # PyMuPDF
import json
from typing import List
from dotenv import load_dotenv  # Load environment variables
from openai import OpenAI

# RAGAS Evaluation Imports
from ragas.metrics import (
    context_precision,  # Measures if retrieved is relevantto the answer.
    context_recall  # Measures if all necessary information was retreieved.
)
from ragas import evaluate
from datasets import Dataset

# FAIS & LangChain Imports
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Set API key or raise an error if not set
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    load_dotenv()  # Carga las variables de entorno del archivo .env
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# print("OpenAI API Key:", OPENAI_API_KEY)

# Instantiate OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# =========================================================
# FAISS & Document Loading Setup
# =========================================================
DB_FILE = "./src/rag/.faiss_index"

# Define the GPT model class
model_name = "gpt-3.5-turbo"  # Example model name, can be changed as needed


class GPTModel:
    def __init__(self, model_name=model_name):
        self.model_name = model_name
        # self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)

    def generate_response(self, prompt: str) -> str:
        """Generate a response using the GPT model."""
        completion = openai_client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "system", "content": "You are a helpful assistant."},
                      {"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content


def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text from a PDF file using PyMuPDF."""
    text = ""
    page_number = 0
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                page_text = page.get_text("text")
                page_number = page.number + 1
                text += page_text + "\n"
            print(f"Extracting text from page {page_number} of {pdf_path}")
    except Exception as e:
        print(f"Error extracting text from {pdf_path}: {e}")
    return text


def load_documents(folder_path: str) -> List[Document]:
    """Load all PDF and TXT from a folder and return a list of Document objects."""
    local_docs = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        print(f"Processing file: {filename}")
        if filename.endswith(".pdf"):
            text = extract_text_from_pdf(file_path)
        elif filename.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as file:
                text = file.read()
        else:
            continue
        if text.strip():
            local_docs.append(Document(page_content=text,
                                       metadata={"source": filename}))
    return local_docs


def create_vector_db(local_docs):  # List[Document]
    """Create a FAISS vector database from a list of Document objects."""
    print("Local docs type:", type(local_docs),
          "with length:", len(local_docs))
    if not local_docs:
        raise ValueError(
            "No documents provided to create the vector database.")
    if os.path.exists(DB_FILE):
        print(f"Database file {DB_FILE} already exists.")
        print("Please delete it before creating a new one.")
        return
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, chunk_overlap=100)
    docs = text_splitter.split_documents(local_docs)

    # Generating embeddings

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

    vector_db = FAISS.from_documents(docs, embeddings)

    # Save the vector store to disk
    vector_db.save_local(DB_FILE)
    print(f"Vector database created and saved to {DB_FILE}.")


def load_vector_db():
    """Create a FAISS vector database from documents in the 'documents' folder."""
    if os.path.exists(DB_FILE):
        return FAISS.load_local(DB_FILE, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), allow_dangerous_deserialization=True)
    else:
        raise FileNotFoundError(
            f"Database file {DB_FILE} not found. Please create the database first.")


def retrieve_elevant_docs(query: str, k: int = 3) -> List[Document]:
    """Retrieve the top-k relevant documents chunks from FAISS."""
    vector_db = load_vector_db()
    if vector_db:
        results = vector_db.similarity_search(query, k=k)
        return [doc.page_content for doc in results]
    else:
        print("Vector database not found or could not be created.")
        return []


def chat_with_context(query: str, k: int = 3) -> str:
    """Generate a response to the query using retrieved relevant documents as context."""
    relevant_docs = retrieve_elevant_docs(query, k)
    context = "\n\n".join(relevant_docs)

    prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"
    gpt_model = GPTModel()
    response = gpt_model.generate_response(prompt)
    return response

# =========================================================
# RAGAS Evaluation (Retrieval Testing Only)
# =========================================================


def evaluate_ragas(query: str, retrieved_docs: List[str], correct_answer: str):
    """Evaluates the RAG retrieval system using RAGAS metrics"""

    # Ensure retieved_docs is a list
    if not isinstance(retrieved_docs, list):
        retrieved_docs = [retrieved_docs]

    # Prepare data for evaluation
    evaluation_data = {
        "question": [query],
        "contexts": [retrieved_docs],  # Expected format: list of strings
        "answer": [correct_answer],
        "reference": [correct_answer]  # Required for context_precision
    }
    dataset = Dataset.from_dict(evaluation_data)

    # Run RAGAS evaluation
    scores = evaluate(dataset, metrics=[
        context_precision,
        context_recall
    ])

    # Convert to dictionary
    scores_dict = scores.__dict__

    # Remove non-serializable parts (e.g., evaluation_dataset) if present
    EVAL_DATA = "evaluation_dataset"
    if EVAL_DATA in scores_dict:
        scores_dict.pop(EVAL_DATA)
    # Extract only the scores
    only_scores = scores_dict.get("scores", {})

    print("\nüïØÔ∏è **RAG Retrieval Evaluation Scores:**")
    print(json.dumps(only_scores, indent=2, default=str))

# Example
# # query = "Can I charge my Galaxy S22 wirelessly?"
# # aswer = "Yes, you can charge your Galaxy S22 wirelessly using the Wireless power sharing feature. To use it, open Settings, go to Battery and device care, select Battery, and tap Wireless power sharing. Then tap Battery limit to set your desired threshold, and once that level is reached, wireless power sharing will automatically turn off."


# =========================================================
# Main Execution
# =========================================================
if __name__ == "__main__":
    FOLDER_PATH = "./src/rag/documents"
    # load_dotenv()  # Carga las variables de entorno del archivo .env
    # OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    # print("OpenAI API Key:", OPENAI_API_KEY)

    # Load documents from the 'documents' folder
    documents = load_documents(FOLDER_PATH)
    create_vector_db(documents)

    while True:
        user_query = input("Enter your query (or 'exit' to quit): ")
        if user_query.lower() == 'exit':
            break
        responseGPT = chat_with_context(user_query)
        if responseGPT:
            print("\n üí¨ ChatGPT Response: \n", responseGPT)
        else:
            print("No relevant documents found.")
        answer = input("‚úÖ Enter the correct answer for evaluation: ")
        evaluate_ragas(user_query, responseGPT, answer)
        print("\n" + "="*50 + "\n")
        print("Evaluation complete. Scores printed above.")
        print("You can now continue with the next query or exit.")
    print("Exiting the RAGAS to GPT evaluation script.")

```
7. Ejecut√© en la `TERMINAL` estando en el Ambiente Virtual de Python: </br> `python .\src\rag\RAGAS_gpt.py`
8. Luego de una espera y poner como `query` el texto </br> `Can I charge my Galaxy S22 wirelessly?` </br> Y luego a la pregunta de `correct answer` de: </br> `Yes, you can charge your Galaxy S22 wirelessly using the Wireless power sharing feature. To use it, open Settings, go to Battery and device care, select Battery, and tap Wireless power sharing. Then tap Battery limit to set your desired threshold, and once that level is reached, wireless power sharing will automatically turn off.` </br> Esta ser√≠a la respuesta: </br> ![RAGAS to GPT](images/2025-08-21_082859.png "RAGAS to GPT")
9. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`



### 42. Rag framework - Coherence, Fluency and Relevance

>[!NOTE]
>
>Al evaluar el RAG, se requiere m√°s que simplemente pensar en la capacidad de recordar o si toda la informaci√≥n est√° ah√≠.
>Por lo tanto, tambi√©n debemos considerar la fluidez, la relevancia y la coherencia.
>
>Porque todos estos elementos son importantes al comparar cualquier tipo de sistema de IA.
>Y tambi√©n para el RAG, ¬øcierto?
>
>Tambi√©n debes verificar si tu sistema de evaluaci√≥n tambi√©n considera la fluidez, la relevancia y la coherencia en tu flujo de trabajo del RAG.
>
>![Human Evaluation](images/2025-08-21_083652.png "Human Evaluation")
>
>Para ello, quiero presentar un concepto llamado LLM como juez.
>B√°sicamente, se eval√∫a el resultado de la secuencia de comandos de Rag utilizando otro modelo de lenguaje amplio.
>Y, por supuesto, tambi√©n lo analizaremos.
>¬øPero saben qu√© es la fluidez?
>As√≠ funciona.
>Cu√°n natural y gramaticalmente correcto es el texto generado.
>Y no s√≥lo fluidez.
>
>![Human Evaluation - fluency](images/2025-08-21_083932.png "Human Evaluation - fluency")
>
>Pero tambi√©n se analiza la coherencia.
>Y la coherencia, como saben, muestra la fluidez y consistencia de todos los elementos que se extraen
>de la producci√≥n de material.
>Ahora bien, en t√©rminos de contexto, tambi√©n se puede analizar la relevancia.
>
>![Human Evaluation - coherence](images/2025-08-21_084308.png "Human Evaluation - coherence")
>
>¬øEs la respuesta relevante para lo que tengo?
>Porque le pedir√°s a tu sistema que proporcione algunas respuestas.
>¬øY es relevante?
>
>![Human Evaluation - Relevance](images/2025-08-21_084620.png "Human Evaluation - Relevance")
>
>Y tambi√©n podr√≠as considerar la concisi√≥n.
>¬øEs conciso?
>Es decir, ¬øobtengo cinco p√°ginas de algo que podr√≠a resumirse en quiz√°s dos l√≠neas?
>¬øO recibo mucha palabrer√≠a, que en realidad no es tan relevante para m√≠?
>Y para validar tu trabajo, analizaremos un marco basado en una evaluaci√≥n profunda.
>
>![Human Evaluation - Concision](images/2025-08-21_084954.png "Human Evaluation - Concision")
>
>B√°sicamente, este es mi c√≥digo, algo que tengo aqu√≠.
>Si subimos, vemos que usaremos esta m√©trica de "eval".
>Necesitas tu clave API.
>
>Configurar√© mi contenedor ChatGPT, ya que lo usar√© para que me haga un trabajo.
>Luego generar√© varias pruebas.
>Si te fijas, estas son pruebas Pi.
>Aqu√≠ est√°n mis evaluaciones: fluidez, coherencia, relevancia y concisi√≥n.
>
>Todas se realizan con una instrucci√≥n.
>As√≠ que, incluso si esto ya est√° predefinido y nadie sabe c√≥mo usarlo, necesitas proporcionar una especie de instrucci√≥n.
>Pero no te preocupes, esta instrucci√≥n se combina con algo que est√° detr√°s de la instrucci√≥n y te proporcionar√° todos estos elementos.
>
>Y tambi√©n puedo decir aqu√≠, por ejemplo, algo como esto y un criterio de aprobaci√≥n en porcentajes.
>Por defecto, se obtiene un 50% de aprobado.
>Ahora, esta prueba que acabo de empezar, buscar√° un archivo llamado texto de entrada y texto de referencia.
>Comparar√° uno con el otro, porque al validar una pista se necesita un texto de entrada, algo con lo que se pueda trabajar.
>Y luego la referencia, que es la parte correcta.
>
>Tomar√° mi texto de entrada, que es un texto que he copiado de internet.
>Lo traducir√° al rumano, obtendr√° las ideas principales y las traducir√° de nuevo al ingl√©s.
>Y luego la referencia est√° aqu√≠.
>Compararemos el resumen de la traducci√≥n al rumano con el texto de referencia en ingl√©s que ya he completado.
>S√≠, esa es la idea.
>
>Queremos comparar algo con otra cosa.
>Y pueden ver aqu√≠ que estoy ejecutando LM como juez, ¬øven aqu√≠, verdad?
>LM como juez.
>Y estoy probando fluidez, coherencia, relevancia y concisi√≥n.
>Todos se ejecutar√°n en paralelo, y la prueba ya est√° hecha.
>Ha sido utilizada por una IA confiable con nuestro marco de trabajo.
>Y aqu√≠ est√° la tasa de aprobaci√≥n, no la puntuaci√≥n.
>
>As√≠ que las cuatro pruebas han pasado, pero ¬øcu√°l es exactamente su puntuaci√≥n?
>
>Bueno, por ejemplo, veamos si subimos en relevancia.
>Obtuve un 78% de aprobados en concisi√≥n, 66% en fluidez y 51%.
>As√≠ que, si este porcentaje fuera dos puntos porcentuales menor, este examen habr√≠a sido reprobado.
>As√≠ que, al evaluar el RAG, no solo se trata de asegurar la memoria del contexto, o quiz√°s la precisi√≥n del contexto, o quiz√°s la veracidad, sino que tambi√©n hay que observar eso y compararlo, ¬øcierto?
>
>Porque siempre comparamos algo con algo con el RAG.
>Esa es la idea.
>As√≠ que siempre se compara algo.
>
>Y b√°sicamente eso es todo.
>Eh, no, estoy seguro de que encontrar√°n una manera de evaluar la coherencia, la fluidez, la relevancia y cualquier otra cosa que deseen utilizando este LLM como criterio.
>Correcto.
>As√≠ que recuerden, obtienen informaci√≥n, obtienen resultados y luego consultan a un modelo ling√º√≠stico externo de gran tama√±o.
>En este caso, la IA confiar√° en que evaluar√°n sus resultados compar√°ndolos con un resultado predefinido.
>Y luego tienes dos pares de ojos que miran tus resultados y tu puntuaci√≥n.
>


### 43. RAG Benchmarking - Nugget Coverage

>[!NOTE]
>
>Ahora bien, cuando haces un rag, b√°sicamente tienes un mont√≥n de documentos.
>Imagina que esta es toda tu base de conocimientos dividida en fragmentos.
>
>Estos son los fragmentos de informaci√≥n que tu recuperaci√≥n obtendr√° para completar tu contexto.
>Y si haces tu pregunta, tu informaci√≥n podr√≠a estar en este y en este otro.
>
>![RAG Testing](images/2025-08-21_091505.png "RAG Testing")
>
>Y estos dos podr√≠an no contener la respuesta a tu informaci√≥n.
>Y el fragmento que contenga con seguridad tu informaci√≥n, ser√° la respuesta a tu consulta.
>Eso se llama pepita de oro porque es de oro, sin duda.
>Encontrar√°s tu informaci√≥n all√≠.
>
>Por ejemplo, imaginemos que tienes el fragmento uno, el fragmento dos, el fragmento tres, el fragmento cuatro, y
>luego defines el fragmento dos como mi pepita de oro.
>Entonces, cuando est√©s probando la siguiente m√©trica, que es la cobertura de la pepita de oro,
>Lo que quieres validar es que el retriever coja tu pepita, y t√∫ sabes que es esa pepita y no otra.
>
>Y, sabes, esto es muy f√°cil de implementar.
>Puedes hacerlo con ragas.
>Y te lo mostrar√© ahora mismo.
>
>B√°sicamente, lo que tengo aqu√≠ es una implementaci√≥n de ragas.
>Y lo m√°s importante que queremos analizar no es el c√≥digo.
>Porque, de nuevo, el c√≥digo con el que se puede explicar, como ChatGPT o Claude, es esta parte del conjunto de datos.
>
>As√≠ que aqu√≠ es donde definimos nuestro conjunto de datos.
>Y ven que tenemos tres bloques de datos.
>Y saben que los bloques uno y cuatro son v√°lidos.
>Pero el bloque cuatro no est√° aqu√≠ porque no se ha recuperado.
>
>As√≠ que, b√°sicamente, cuando hacen su llamada y luego se recuperan estos bloques, y saben que los bloques uno y cuatro son v√°lidos.
>As√≠ que lo que puedo decirles es que falta el bloque cuatro.
>Eso es un problema.
>Y luego se obtienen las puntuaciones de soporte.
>
>La puntuaci√≥n de soporte indica la relevancia de su respuesta en su bloque.
>C√≥mo.
>B√°sicamente, cuanto mayor sea la puntuaci√≥n de soporte, m√°s relevante ser√° tu respuesta en ese fragmento.
>Ahora, de nuevo, tenemos estos fragmentos recuperados.
>Estos est√°n etiquetados como pepitas de oro.
>
>Y estas son nuestras puntuaciones de soporte.
>As√≠ que voy a ejecutar esto.
>Y ahora veamos cu√°les son los resultados que obtendremos.
>Un par de segundos, porque esto funcionar√° muy r√°pido. No estoy usando ning√∫n tipo de alfombra.
>Solo estoy simulando esto y calculando para ti.
>No necesitas simular.
>Puedes hacerlo como quieras, pero ya sabes, porque ya has implementado los archivos y esto es justo lo que queremos hacer.
>Entonces.
>
>La recuperaci√≥n de la pepita de oro es 0.5.
>¬øPor qu√©?
>Porque de dos, solo lleg√≥ uno.
>As√≠ que, tambi√©n deber√≠as tener el fragmento para...
>Pero la recuperaci√≥n no se molest√≥ en obtener el fragmento para la puntuaci√≥n media de soporte, como pueden imaginar.
>S√≠, es este de aqu√≠.
>
>B√°sicamente, es algo que pueden revisar.
>La recuperaci√≥n es como obtener documentos, ¬øverdad?
>Pueden ver si el documento se recupera.
>Esa es una parte.
>Y luego pueden analizar el contexto para ver si la recuperaci√≥n obtuvo de ese documento lo que quieran.
>
>La primera parte es obtener el documento y luego obtener el fragmento espec√≠fico de ese documento.
>Es una m√©trica muy importante si quieren probar su recuperaci√≥n, ya saben, la recuperaci√≥n de la pepita de oro.
>Y si quieren probar toda la informaci√≥n, esa ser√° su precisi√≥n o su recuperaci√≥n.
>


## Section 7: Evaluate Machine Learning Model based on DATA Split


### 44. Introduction to Splitting Data for AI/ML Models


>[!NOTE]
>
>Para probar mejor la IA, es necesario comprenderla.
>Y no profundizar√© mucho en qu√© exactamente.
>Bueno, o en el funcionamiento interno de la IA, pero es muy importante.
>Es necesario comprender cu√°les son los tres pilares que la sustentan.
>
>![Pilares de la IA](images/2025-08-25_172004.png "Pilares de la IA")
>
>1. Ahora, sin m√°s pre√°mbulos, el primero se centra en los datos. >Y digo **datos limpios** porque significa, por ejemplo, que no tienen sesgos.</br>
></br>No contienen lenguaje grosero.
>Son datos correctos, no est√°n llenos de teor√≠as conspirativas, porque no queremos que nuestra IA aprenda que cierta minor√≠a, por ejemplo, es gen√©ticamente inferior, etc.
>¬øVerdad?</br>
></br>Y no se trata solo de eso.
>Se trata de que los datos sean estad√≠sticamente representativos de toda la poblaci√≥n.
>Y hablaremos de eso m√°s adelante.
>Pero es importante saber que los datos son inmensos.
>Son absolutamente inmensos cuando se trata de IA.
>
>2. El siguiente tema trata sobre **algoritmos**.
></br></br>
>Quiz√°s hayas o√≠do hablar de redes neuronales convolucionales, aprendizaje profundo o cualquier tipo de algoritmo de aprendizaje autom√°tico.
>Bueno, los algoritmos funcionan con datos.
></br></br>
>Por lo tanto, cuanto mejor sea el algoritmo, m√°s f√°cil ser√° para tu aprendizaje autom√°tico convertir los datos en una predicci√≥n, o generar algo, y tambi√©n m√°s r√°pido, ya que los algoritmos pueden ser lentos y r√°pidos.
></br></br>
>El segundo pilar de la IA se centra en los algoritmos, ya que ver√°s que, a partir de los mismos datos, diferentes algoritmos de aprendizaje autom√°tico obtienen diferentes resultados y eficiencias.
></br></br>
>As√≠, podr√≠as obtener una puntuaci√≥n del 90 % con un algoritmo, pero podr√≠as obtener una puntuaci√≥n del 97 % de precisi√≥n con un algoritmo diferente.
>
>3. El tercero es la **potencia de procesamiento**.
></br></br>
>Estoy bastante seguro de que saben que Nvidia domina el mundo actualmente.
>Y todo esto junto.
>Constituyen los fundamentos, los pilares para crear IA.
></br></br>
>As√≠ que todo se reduce a algoritmos de datos y, por supuesto, a la potencia de procesamiento que se utiliza.
>Porque el entrenamiento de la IA en s√≠ mismo requiere un uso intensivo de recursos.
>Y ver√°n que los diferentes algoritmos y m√©todos de entrenamiento requieren un uso m√°s intensivo de recursos
>que otros.
>
>En fin, esto es solo el principio.
>Ahora pasemos a la parte sobre datos, ya que existen m√©todos de entrenamiento y evaluaci√≥n muy espec√≠ficos para ellos y los analizaremos en todo este cap√≠tulo.
>
>Y hay algo llamado divisi√≥n de datos que realmente me gusta y disfruto.
>
>Ahora imagina que este es tu conjunto de datos, todo lo que tienes aqu√≠.
>Voy a tomar mi puntero.
>Estos ser√°n todos los datos que tengo.
>Eso es todo.
>
>Nada m√°s.
>
>![Divisi√≥n de Datos](images/2025-08-25_172519.png "Divisi√≥n de Datos")
>
>Este es mi entrenamiento completo.
>Los datos est√°n etiquetados y ya est√°n depurados.
>Son representativos.
>Y voy a usar estos datos para entrenar y validar mi modelo.
>Pero, ¬øc√≥mo se dice "esto es para entrenamiento y esto para evaluaci√≥n"?
>
>¬øC√≥mo se sabe c√≥mo hacerlo?
>Existen algunos m√©todos.
>Imagina el siguiente escenario donde tomas todos estos datos que tienes aqu√≠ y dices: "Supongamos que el 80% o el 70% ser√°n mis datos de entrenamiento".
>As√≠ que mi algoritmo se entrenar√° solo con el 70% de todos mis datos.
>
>Entonces, te preguntar√°s: "¬øQu√© hago con el siguiente 30%?".
>Bueno, el siguiente 30% ser√° para probar mi algoritmo.
>Esto significa que estos ser√°n mis datos de entrenamiento etiquetados, y estos ser√°n datos que el modelo no ha visto.
>Esto se usar√° en el modelo para validarlo.
>Entonces, se te pedir√° que uses datos nuevos nunca antes vistos para ver si llegas a la etiqueta real.
>No.
>
>Esto significa que tu entrenamiento puede ser √∫nico y no hay retroalimentaci√≥n durante el mismo.
>As√≠ que hay otro m√©todo que puedes usar para dividir a√∫n m√°s tus datos.
>
>Repetimos el proceso.
>Imagina que esto es el 60%.
>Y vas a entrenar tu modelo solo con estos datos, solo con esta parte, el 60% de todos tus datos.
>Solo un ejemplo.
>
>Luego, obtendr√°s otro conjunto de datos: datos de evaluaci√≥n, que se usar√°n para evaluar el entrenamiento del modelo.
>Durante el entrenamiento, puedes simplemente decir: "Oye, quiero que ejecutes el entrenamiento con estos datos".
>Y durante el entrenamiento, tambi√©n quiero que uses estos datos para evaluar si el entrenamiento es correcto.
>Har√°s esto un par de veces, y solo despu√©s podr√≠as decir: "Mi modelo est√° entrenado y ahora usar√© estos datos para comparar mi modelo actual".
>
>Esto se llama divisi√≥n de datos.
>En los pr√≥ximos videos, te mostrar√© qu√© es el plegado en K.
>
>¬øC√≥mo puedes dividir a√∫n m√°s tus datos?
>Tambi√©n te mostrar√© que, por ejemplo, si analizamos los abrazos, los datos faciales suelen dividirse as√≠.


### 45. Demo - Python - Scikit - Split Data for Testing

1. Nos vamos a esta p√°gina <img alt="scikit learn" width="16px" height="16px" src="https://cdn-avatars.huggingface.co/v1/production/uploads/1654272313249-6141a88b3a0ec78603c9e784.png" style="background:lightblue;" > [scikit-learn/iris](https://huggingface.co/datasets/scikit-learn/iris)
2. Revisamos en el tabulador primero `Dataset card` y debajo que el `train` est√° en `Split(1)` y `150 rows`: </br> ![iris -> Dataset Viewer](images/2025-08-25_180316.png "iris -> Dataset Viewer")
3. Vemos cuatro par√°metros:
* SepalLengthCm
* SepalWidthCm
* PetalLengthCm
* PetalWidthCm
4. Y luego, dependiendo de estos n√∫meros, tambi√©n se obtiene la especie de iris, que puede ser una de las tres.</br> Estos son todos los datos que tenemos.</br>B√°sicamente, lo que queremos hacer es entrenar nuestro modelo para que, dependiendo de las combinaciones de estos cuatro, prediga qu√© tipo de especie tenemos.</br>Pero para predecirlo, es necesario encontrar una manera de trabajar con los datos.
5. Creamos el archivo **`src/test/LLM/Data_Spliting/evaluate_model.py`**
6. Primero en la `TERMINAL` ejecuto el Ambiente Virtual de Python: </br> `.venv/Scripts/activate`
7. Se ve el archivo **`src/test/LLM/Data_Spliting/iris_eval.csv`**, con este ejemplo de valores:
```csv
id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species
71,5.9,3.2,4.8,1.8,Iris-versicolor
30,5.7,2.6,3.5,1.0,Iris-versicolor
135,6.1,2.6,5.6,1.4,Iris-virginica
...
144,6.8,3.2,5.9,2.3,Iris-virginica
127,6.2,2.8,4.8,1.8,Iris-virginica
92,6.1,3.0,4.6,1.4,Iris-versicolor
```
8. Si existe el archivo **`iris_eval.csv`** lo borramos.
9. Hay otro archivo **`src/test/LLM/Data_Spliting/iris_train.csv`**, con estos datos de ejemplo:
```csv
id,SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species
95,5.6,2.7,4.2,1.3,Iris-versicolor
57,6.3,3.3,4.7, 1.6,Iris-versicolor
97,5.7,2.9,4.2, 1.3,Iris-versicolor
...
102,5.8,2.7,5.1,1.9,Iris-virginica
145,6,7,3,3,5,7,2.5,Iris-virginica
46,4.8,3.0,1.4,0.3,Iris-setosa
```
10. Si existe el archivo **`iris_train.csv`**, lo borramos.
11. Creo otro archivo de nombre **`src/test/LLM/Data_Spliting/split_train_Dataset.py`**, con este c√≥digo:
```py
from datasets import load_dataset

# Step 1: Load the Iris dataset from Hugging Face
dataset = load_dataset("scikit-learn/iris")

# Step 2: Split the dataset into training (120 samples) and evaluation (30 samples)
split_dataset = dataset["train"].train_test_split(test_size=30, seed=42)

# Step 3: Save the splits to CSV files
current_path = "./src/test/LLM/Data_Spliting/"
train_file = current_path + "iris_train.csv"
eval_file = current_path + "iris_eval.csv"

split_dataset["train"].to_pandas().to_csv(train_file, index=False)
split_dataset["test"].to_pandas().to_csv(eval_file, index=False)

print(f"Training data saved to '{train_file}'")
print(f"Evaluation data saved to '{eval_file}'")
```
12. Ejecuto en la `TERMINAL` este √∫ltimo **`split_train_Dataset.py`** </br> `python ./src/test/LLM/Data_Spliting/split_train_Dataset.py` </br> Y obtengo los dos archivos que inicialmente borr√©:
* **`src/test/LLM/Data_Spliting/iris_eval.csv`**
* **`src/test/LLM/Data_Spliting/iris_train.csv`**
13. Creo otro archivo de nombre **`src/test/LLM/Data_Spliting/split_train_scikit.py`**, con este c√≥digo:
```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import pandas as pd

# Step 1: Load the Iris dataset from Hugging Face
iris = load_iris()
x = iris.data  # Features
y = iris.target  # Labels

# Step 2: Split the dataset into training (120 samples) and evaluation (30 samples)
x_train, x_eval, y_train, y_eval = train_test_split(
    x, y, test_size=30/150, random_state=42)

# Step 3: Convert split to DataFrames
train_data = pd.DataFrame(x_train, columns=iris.feature_names)
train_data["target"] = y_train

eval_data = pd.DataFrame(x_eval, columns=iris.feature_names)
eval_data["target"] = y_eval

# Step 4: Save the splits to CSV files
current_path = "./src/test/LLM/Data_Spliting/"
train_file = current_path + "iris_train.csv"
eval_file = current_path + "iris_eval.csv"

train_data.to_csv(train_file, index=False)
eval_data.to_csv(eval_file, index=False)

print(f"Training data saved to '{train_file}'")
print(f"Evaluation data saved to '{eval_file}'")
```
14. Borro de nuevo los dos archivo **`*.csv`**.
15. Ejecuto en la `TERMINAL` este √∫ltimo **`split_train_Dataset.py`** </br> `python ./src/test/LLM/Data_Spliting/split_train_scikit.py` </br> Y obtengo los dos archivos que anteriormente borr√©:
* **`src/test/LLM/Data_Spliting/iris_eval.csv`**
* **`src/test/LLM/Data_Spliting/iris_train.csv`**
16. Ejecutamos de nuevo en la `TERMINAL` el anterior **`split_train_Dataset.py`** </br> `python ./src/test/LLM/Data_Spliting/split_train_Dataset.py` </br> Genera mejores resultados en los archivos **`*.csv`**.
18. Creamos otro archivo **`src/test/LLM/Data_Spliting/train_model.py`** y le ponemos este c√≥digo:
```py
from datasets import Dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import pandas as pd
import os

# Step 1: Load your dataset from a CSV file
current_path = "./src/test/LLM/Data_Spliting/"
train_file = current_path + "iris_train.csv"
data = pd.read_csv(train_file)

# Step 1.1: Validate the existence of the training file
if not os.path.exists(train_file):
    raise FileNotFoundError(f"Training file not found: {train_file}")

# Step 1.2: Validate the columns in the dataset
required_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']
if not all(col in data.columns for col in required_columns):
    raise ValueError(f"The dataset must contain the following columns: {required_columns}")

# Step 2: Convert the dataset to Hugging Face Dataset
dataset = Dataset.from_pandas(data)

# Step 3: Preprocess the datase: Convert all feature column into a single textual description


def convert_to_text(example):
    features = f"This flower has a sepal length of {example['SepalLengthCm']} cm, a sepal width of {example['SepalWidthCm']} cm, "\
        f"a petal length of {example['PetalLengthCm']} cm, and a petal width of {example['PetalWidthCm']} cm."
    return {"text": features}


dataset = dataset.map(convert_to_text)

# Step 4: Map the 'Species' column to numerical lables for classification
label_mapping = {"Iris-setosa": 0, "Iris-versicolor": 1, "Iris-virginica": 2}

# Ensure 'Species' is correctly mapped
dataset = dataset.map(lambda x: {"label": label_mapping[x["Species"]]})

# Drop unnecessary columns
dataset = dataset.remove_columns(
    {"Id", "SepalLengthCm", "SepalWidthCm", "PetalLengthCm", "PetalWidthCm", "Species"})

# Step 5: Tokenize the dataset
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")


def tokenize_data(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)


tokenized_dataset = dataset.map(tokenize_data, batched=True)

# Step 6: Split the dataset into training and evaluation sets
train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test_split["train"]
eval_dataset = train_test_split["test"]

# Step 7: Load a pre-trained DistriBERT model
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased",num_labels=3)

# Step 8: Define training arguments with increased epochs and logging
training_arg=TrainingArguments(
    output_dir=current_path +"results",
    eval_strategy="epoch", # Performs evaluation at the end of each epoch
    save_strategy="epoch", # Save checkpoints at the end at each poch
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10, # Increased epochs for better training
    weight_decay=0.01,
    logging_dir=current_path +"logs",
    logging_steps=10,
    save_total_limit=2,
    load_best_model_at_end=True, # Load the best model at the end of training
)

# Step 9: Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_arg,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# Step 10: Train the model and save the results
trainer.train()
print("Training completed. Best model saved at:", training_arg.output_dir)
```
19. Ejecutamos en una `TERMINAL` esta instalaci√≥n en el Ambiente Virtual de Python: </br> `pip install transformers[torch]`
20. Ejecutamos este √∫ltimo con el comando: </br> `python ./src/test/LLM/Data_Spliting/train_model.py`. </br> ![train_model.py](images/2025-08-26_151306.png "train_model.py")</br> Este proceso demora un rato y genera varias carpetas y archivos en la carpeta **"src/test/LLM/Data_Spliting/results"**, entre ellos una carpeta de nombre **"checkpoint-60"**
21. Este √∫ltimo lo vamos a utilizar en el archivo **`src/test/LLM/Data_Spliting/evaluate_model.py`**, con este c√≥digo:
```py
from datasets import Dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer
import pandas as pd
import os

# Step 1: Load the evaluation dataset
current_path = "./src/test/LLM/Data_Spliting/"
# eval_file = current_path+"iris_eval.csv"
eval_file = current_path+"iris_train.csv"
eval_data = pd.read_csv(eval_file)

# Step 1: Validate the existence of the evaluation file
if not os.path.exists(eval_file):
    raise FileNotFoundError(f"Evaluation file not found: {eval_file}")

# Step 1.1: Validate the columns in the dataset
required_columns = ['SepalLengthCm',
                    'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
if not all(col in eval_data.columns for col in required_columns):
    raise ValueError(
        f"The dataset must contain the following columns: {required_columns}")

# Step 2: Convert the dataset to Hugging face Dataset
eval_dataset = Dataset.from_pandas(eval_data)

# Step 3: Preprocess the dataset: convert features into textual descriptions


def convert_to_text(example):
    features = f"Sepal length: {example['SepalLengthCm']}, Sepal width: {example['SepalWidthCm']}, "\
        f"Petal length: {example['PetalLengthCm']}, Petal width: {example['PetalWidthCm']}."
    return {"text": features}


eval_dataset = eval_dataset.map(convert_to_text)

# Step 4: Tokenize the dataset
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")


def tokenizer_data(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)


tokenized_eval_dataset = eval_dataset.map(tokenizer_data, batched=True)

# Step 5: Load the trained model
model_path = current_path + "results/checkpoint-60"
model = DistilBertForSequenceClassification.from_pretrained(
    model_path, local_files_only=True)  # Path to the saved model
if not os.path.exists(model_path):
    raise FileNotFoundError(f"Trained model not found: {model_path}")

# Step 6: Initialize Trainer
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
)

# Step 7: Make predictions
predictions = trainer.predict(tokenized_eval_dataset)

# Step 8: Evaluate predictions and save results
predicted_labels = predictions.predictions.argmax(axis=1)  # Get the predicted class
actual_labels = eval_dataset["Species"]  # Original labels

# Step 9: Map label indices back to species names
label_mapping = {"Iris-setosa": 0, "Iris-versicolor": 1, "Iris-virginica": 2}
reverse_label_mapping = {v: k for k,
                         v in label_mapping.items()}  # Reverse mapping

# Step 10: Calculate accuracy
predicted_labels = predictions.predictions.argmax(
    axis=1)  # Fix access to predictions
actual_labels = eval_data["Species"]  # Use eval_data for actual labels

if len(predicted_labels) != len(actual_labels):
    raise ValueError(
        "Mismatch between number of predictions and actual labels.")

correct_predictions = sum(
    reverse_label_mapping[predicted_labels[i]] == actual_labels[i]
    for i in range(len(actual_labels))
)
accuracy = correct_predictions / len(actual_labels)

# Print and save results
print(f"Accuracy: {accuracy:.2f}")

# Save evaluation results to a CSV file
output_file = current_path + "evaluation_results.csv"
eval_data["PredictedLabel"] = [reverse_label_mapping[label]
                               for label in predicted_labels]
eval_data.to_csv(output_file, index=False)
print(f"Evaluation results saved to {output_file}")
```
22. En una `TERMINAL`, ejecutamos el comando: </br> `python ./src/test/LLM/Data_Spliting/evaluate_model.py` </br> Al final denera un archivo de nombre **`evaluation_results.csv`**.
23. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`


### 46. K-Fold Cross Validation

>[!NOTE]
>
>Les mostrar√© un m√©todo diferente para **Dividir sus Datos**,
>esto se llama **Plegado K**.
>
>Creo que con un ejemplo, les ser√° m√°s f√°cil de entender.
>
>![Dividir Datos - Plegado K](images/2025-08-26_153202.png "Dividir Datos - Plegado K")
>
>Si recuerdan la √∫ltima vez que dividimos todos nuestros datos de entrenamiento, ten√≠amos la parte de evaluaci√≥n de los datos de entrenamiento y luego los datos de prueba.
>Y recordar√°n que el entrenamiento y la evaluaci√≥n se llamaban entrenamiento iris.
>Y esta parte era la evaluaci√≥n iris.
>Ese era el nombre que ten√≠amos en nuestro ejemplo de c√≥digo.
>Pasemos al siguiente.
>
>![Dividir Datos - Plegado K - B√°sico](images/2025-08-26_153538.png "Dividir Datos - Plegado K - B√°sico")
>
>As√≠ que volvemos.
>Estos son todos nuestros datos de entrenamiento.
>
>Con todo lo que tenemos, esas 150 muestras que ten√≠amos en Iris, imagina el siguiente escenario.
>Este es un pliegue: una parte de mis datos de entrenamiento, una peque√±a parte.
>Otra parte.
>Otra parte.
>Otra.
>Y estos son mis datos de prueba.
>
>B√°sicamente, he dividido mis datos en cinco pliegues diferentes.
>Cuatro ser√°n datos de entrenamiento y uno ser√° de prueba.
>Uno de cuatro.
>Cuatro de cinco ser√°n datos de entrenamiento y uno de cinco de prueba.
>
>Y voy a ejecutar el entrenamiento de mi modelo en este tren en estas cuatro partes.
>Evaluar en la √∫ltima.
>Me imagino que son unos 2530, o lo que sea, 40.
>
>La siguiente ejecuci√≥n, el siguiente entrenamiento, es as√≠.
>Voy a entrenar con los tres primeros y el √∫ltimo o el quinto, y el cuarto ser√°n mis datos de prueba, y as√≠ sucesivamente hasta tener todas las combinaciones posibles.
>
>Esto significa que entrenar√© mi modelo cinco veces diferentes y cada vez el conjunto de prueba ser√° una versi√≥n diferente del conjunto de entrenamiento.
>
>**Personas entre 20 y 30 a√±os (30, 40, 50, 50, 60).**
>
>![Dividir Datos - Plegado K - Estratificaci√≥n - B√°sico](images/2025-08-26_154005.png "Dividir Datos - Plegado K - Estratificaci√≥n - B√°sico")
>
>Estos ser√°n tus datos de entrenamiento y de prueba.
>Y deben ser relevantes.
>As√≠ que tienes entre 20 y 60 personas.
>Pero estos son tus datos de entrenamiento sobre ingresos.
>
>Ahora imaginemos que tienes hombres y mujeres, y que tus ingresos son mensuales y anuales.
>No importa.
>Y ahora podr√≠a decir: "Bueno, el 50% de mis datos corresponden a este grupo de edad, el 25% a este grupo de edad, 25 y 25".
>
>Y juntos no ser√°n 100, porque, como saben, puede haber personas trabajando mayores de 60 a√±os, personas trabajando menores de 20, pero podr√≠amos decir que el 90% de la poblaci√≥n activa tiene entre 20 y 60 a√±os, y el 25% de las personas que trabajan hoy en d√≠a tienen entre 30 y 40 a√±os.
>Y el 25% de mis datos tambi√©n est√° aqu√≠".
>
>Esto significa que estad√≠sticamente las proporciones son correctas.
>Si tuviera, por ejemplo, [o].
>Esto significa que estad√≠sticamente estamos en lo cierto porque el 25% de mi poblaci√≥n tiene esta edad y el 25% de mis datos corresponden a esta edad.
>As√≠ que estamos en lo cierto.
>Est√°n perfectamente representados.
>
>Tomemos otro escenario.
>Y aqu√≠ es donde se aplican las estrategias.
>Eh, en este escenario donde tus datos son representativos de la poblaci√≥n, tomemos un escenario diferente.
>Imagina que tienes un sistema de filtrado de correo electr√≥nico que filtra spam y no spam.
>Eso es todo.
>¬øVerdad?
>Tienes una IA que decide.
>Esto va a spam.
>Esto no es spam.
>Y eso es todo lo que hace, ¬øverdad?
>As√≠ es mi vida.
>Nada m√°s.
>
>Ahora imagina que tienes un 25% de correos no spam y un 75% de spam.
>No son estad√≠sticamente correctos.
>Supongo que, si entrenas tu modelo con un 75% de spam y un 25% de no spam, estar√° sesgado.
>Estar√° sesgado hacia el spam.
>
>Por ejemplo, si tienes 1000 correos electr√≥nicos y el 25 % no son spam y el 75 % s√≠ lo son, en este caso, necesitas usar un algoritmo k-fold estratificado donde la proporci√≥n debe corregirse, lo cual se hace directamente en el c√≥digo.
>
>Pero existen diferentes m√©todos de entrenamiento para evitar que esto se desv√≠e hacia el spam.
>Quieres que sea neutral, como este.
>De esta manera, este entrenamiento introduce sesgos en el sistema, y ‚Äã‚Äãnadie quiere que haya sesgos en el sistema.
>
>Y si vamos a la √∫ltima parte, tambi√©n est√° esta tabla.
>
>![Aspecto - Plegado K - Estrattificaci√≥n](images/2025-08-26_154454.png "Aspecto - Plegado K - Estrattificaci√≥n")
>
>**¬øQu√© significa esto?**
>
>Y luego la distribuci√≥n de clases.
>Validaci√≥n cruzada, la b√°sica.
>Esto es para conjuntos de datos balanceados.
>Recuerden que para conjuntos de datos desbalanceados, como mencion√©, se debe usar la estratificaci√≥n para garantizar una evaluaci√≥n justa en todas las clases.
>Ambas son bastante f√°ciles de implementar.
>
>El c√°lculo es m√°s costoso, pero es f√°cil de implementar.
>Puede generar resultados sesgados en conjuntos de datos desbalanceados.
>Exactamente.
>Por eso, esta opci√≥n es solo para conjuntos de datos balanceados.
>
>Ventajas: f√°cil de implementar.
>Funciona con todos los problemas y es m√°s r√°pida con conjuntos de datos grandes.
>Repito: evaluaci√≥n justa y adecuada.
>La mayor√≠a de las veces, se usar√° esta opci√≥n en caso de que la evaluaci√≥n no sea justa.
>
>Pero nuestro conjunto de datos de iris va aqu√≠ porque est√° totalmente equilibrado.
>Representa el 33 % de las tres clases.
>S√≠.
>
>Y, de nuevo, la desventaja es lo opuesto a la ventaja.
>Eso es todo por la teor√≠a.
>En el pr√≥ximo video les mostrar√© c√≥mo ejecutaremos la evaluaci√≥n en un modelo entrenado
>utilizando una t√©cnica de entrenamiento de validaci√≥n de k-fold.
>

### 47. Model Overfitting and underfitting testing approaches

>[!NOTE]
>
>Son grandes problemas a los que se enfrentan todos los modelos hoy en d√≠a, o todos los ingenieros inform√°ticos que trabajan en el campo de la IA.
>
>Se trata del sobreajuste y el infraajuste.
>Y podr√≠as estar sobreajustando o infraajustando, pero es muy dif√≠cil llegar al punto exacto en que tu modelo sea simplemente perfecto.
>
>![.](images/2025-08-26_155128.png "")
>
>Perm√≠tanme explicar el concepto de sobreajuste.
>
>* Esto significa que tu modelo funciona muy bien con datos de entrenamiento, pero con datos de evaluaci√≥n no vistos, hay una gran diferencia entre tu puntuaci√≥n durante el entrenamiento y tu puntuaci√≥n con datos no vistos.
></br>
>Entonces, ¬øcu√°les son los s√≠ntomas o qu√© significa esto?
>   * Lo m√°s probable es que tu modelo haya memorizado los datos de entrenamiento.
></br>
>As√≠ que no encuentra patrones en la estructura de datos real, pero sabe que, bueno, es como un mecanismo de almacenamiento en cach√©.
></br>
>S√≠, d√©jenme decirlo as√≠.
>Si sabes c√≥mo funciona el almacenamiento en cach√©, tu modelo podr√≠a ser una especie de enorme m√°quina de almacenamiento en cach√©.
>
>   * Tu modelo no entendi√≥ bien los patrones de los datos, sino que simplemente los memoriz√≥.
></br>
>Esto se conoce como sobreajuste, un problema muy grave.
>Significa que tus algoritmos no son muy buenos.
>Y esa es una parte del problema.
>No la √∫nica, pero s√≠ una parte.
>
>* ¬øY qu√© hay del infraajuste?
></br>
>Bueno, es lo contrario.
></br>
>¬øQu√© significa?
>   * Significa que el modelo tiene un rendimiento deficiente tanto con los datos de entrenamiento como con los de evaluaci√≥n.
>Si buscas un modelo con un rendimiento deficiente con los datos de entrenamiento, pero bueno con los de evaluaci√≥n,
>sabes que es un misterio.
>Ese tipo de situaci√≥n es una anomal√≠a.
>No existe.
></br>
>Es simplemente un efecto de probabilidad.
>¬øVerdad?
>Es algo que no deber√≠a existir, es solo una coincidencia que ocurri√≥ una vez.
></br>
>En este caso, significa que no hay nada en tus datos que pueda tener significado.
>Por lo tanto, tus datos son totalmente independientes.
>No hay correlaci√≥n entre nada.
></br>
>   * Pero incluso en ese caso, deber√≠a haber algo en tu m√©todo de evaluaci√≥n, tus algoritmos.
>No son lo suficientemente buenos para el prop√≥sito.
></br>
>Esto significa que tu modelo no ha aprendido ning√∫n patr√≥n significativo de los datos.
>De acuerdo.
>Pero bueno.
></br>
>¬øC√≥mo probamos esto?
>¬øC√≥mo evaluamos algo as√≠?
>Bueno, no te preocupes, porque te tengo cubierto.

1. Regresamos al archivo **`src/test/LLM/Data_Spliting/evaluate_model.py`**
2. Posemos cambiar el texto de `"iris_eval.csv"` por el de `"iris_iris.csv"`.
3. Ejectuar de nuevo en la `TERMINAL` el Ambiente Virtual de Python: </br> `.venv/Scripts/activate`
4. Ahora si ejecutar el comando en la `TERMINAL` de: </br> `python ./src/test/LLM/Data_Spliting/evaluate_model.py`</br> Se demora mas y obtiene algo similar al anterior
5. Hay un arhivo que no se puede ver completo el codigo de nombre **`compare.py`**.
6. Y hay otro para generar los **`*.csv`** de nombre **`tran_k_fold.py`**, tampoco se alcanza ver el c√≥digo completo.
7. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`


### 48. Laboratory - Python K Fold Training and Demo

1. Para el archivo **`src\test\LLM\Data_Spliting\train_k_fold.py`**, empiezo con las importaciones:
```py
from datasets import Dataset
from transformers import (
    DistilBertTokenizer,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from sklearn.model_selection import StratifiedKFold
import pandas as pd
import os
```
2. Completo los tres primeros pasos de **`train_k_fold.py`**:
```py
# Step 1: Load your dataset from a CSV file
current_path = "./src/test/LLM/Data_Spliting/"
train_file = current_path + "iris_train.csv"
data = pd.read_csv(train_file)

# Step 1.1: Validate the existence of the training file
if not os.path.exists(train_file):
    raise FileNotFoundError(f"Training file not found: {train_file}")

# Step 1.2: Validate the columns in the dataset
required_columns = ['SepalLengthCm', 'SepalWidthCm',
                    'PetalLengthCm', 'PetalWidthCm', 'Species']
if not all(col in data.columns for col in required_columns):
    raise ValueError(
        f"The dataset must contain the following columns: {required_columns}")

# Step 2: Convert the dataset to Hugging Face Dataset
dataset = Dataset.from_pandas(data)


# Step 3: Preprocess the datase: Convert all feature column into a single textual description
def convert_to_text(example):
    features = f"Features: Sepal length {example['SepalLengthCm']} cm, Sepal width {example['SepalWidthCm']} cm, "\
        f"Petal length of {example['PetalLengthCm']} cm, Petal width of {example['PetalWidthCm']} cm."
    return {"text": features}


dataset = dataset.map(convert_to_text)
```
3. Agregamos pasos 4 y 5:
```py
# Step 4: Map the 'Species' column to numerical lables for classification
label_mapping = {"Iris-setosa": 0, "Iris-versicolor": 1, "Iris-virginica": 2}

# Ensure 'Species' is correctly mapped
dataset = dataset.map(lambda x: {"label": label_mapping[x["Species"]]})

# Step 5: Tokenize the dataset
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


def tokenize_data(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)


tokenized_dataset = dataset.map(tokenize_data, batched=True)
```
4. Paso 6 y empiezo un ciclo `for`:
```py
# Step 6: k-Fold Cross-Validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
df = pd.DataFrame(tokenized_dataset)

results = []
model_checkpoint = "distilbert-base-uncased"

for fold, (train_idx, val_idx) in enumerate(kf.split(df, df["label"])):
    print(f"\n--- Fold {fold+1} ---")
```
5. Continuo con el contenido del ciclo `for` , pasos 7 a 9:
```py
    # Step 7: Create result directories
    fold_result_dir =f"{current_path}results_fold{fold+1}"
    os.makedirs(fold_result_dir, exist_ok=True)

    # Step 8: Create result training and validation sets
    train_data = df.iloc[train_idx]
    eval_data = df.iloc[val_idx]
    train_dataset = Dataset.from_pandas(train_data)
    eval_dataset = Dataset.from_pandas(eval_data)

    # Step 9: Load a pre-trained DistriBERT model
    model = DistilBertForSequenceClassification.from_pretrained(
        model_checkpoint, num_labels=3)
```
6. Pasos 10 a 14, para cerrar el `for`:
```py
    # Step 10: Define training arguments with increased epochs and logging
    training_arg = TrainingArguments(
        output_dir=fold_result_dir,
        eval_strategy="epoch",  # Performs evaluation at the end of each epoch
        save_strategy="epoch",  # Save checkpoints at the end at each poch
        learning_rate=5e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=3,  # Increased epochs for better training
        weight_decay=0.01,
        logging_dir=f"{current_path}logs_fold_{fold+1}",
        logging_steps=10,
        save_total_limit=2,  # Keep only the last 2 checkpoints
        load_best_model_at_end=True,  # Load the best model at the end of training
    )

    # Step 11: Inicialize Trainer
    trainer = Trainer(
        model=model,
        args=training_arg,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )

    # Step 12: Train the model and save the results
    trainer.train()
    trainer.save_model(fold_result_dir)
    print("Training completed. Best model saved at:", training_arg.output_dir)
    
    # Step 13: Update checkpoint for next fold
    model_checkpoint=fold_result_dir # Use the best model of current fold as starting point for the next

    # Step 14: Evaluate the model
    eval_results = trainer.evaluate()
    results.append(eval_results)

    print(f"Results for fold {fold+1}: {eval_results}")
```
7. Finalizamos con el paso 15, ya por fuera del `for`:
```py
# Step 15: Calculate average metrics across all folds
average_results = {key: sum(r[key]for r in results)/len(results)
                   for key in results[0]}
print("\n--- Average Results Across all Folds ---")

for key, value in average_results.items():
    print(f"{key}: {value:.4f}")

# Final Output
print("\nTraining and evaluation completed. Models and checkpoints are saved in respective fold directories.")
```
8. Levantamos el Ambiente Virtual de Python: </br> `.venv/Scripts/activate`
9. En la misma `TERMINAL` donde tenemos el Ambiente Virtual de Python, ejecutamos el comando para correr el proceso: </br> `python ./src/test/LLM/Data_Spliting/train_k_fold.py`
10. Ahora bien para evaluar cualquiera de estos modelos entramos al archivo **`src/test/LLM/Data_Spliting/evaluate_model.py`** y cambiamos el `model_path` por la ruta de unos de los obtenidos en el proceso anterior, por ejemplo **"results_fold5/checkpoint-18"**.
11. Probamos la ejecuci√≥n de nuevo de **`evaluate_model.py`**, con el comando: </br> `python ./src/test/LLM/Data_Spliting/evaluate_model.py`</br> Al final obtengo un resultado como este: </br> `Accuracy: 0.95`</br> `Evaluation results saved to ./src/test/LLM/Data_Spliting/evaluation_results.csv`
12. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`



## Section 8: Performance Characteristics for AI Models


### 49. Criteria 1- Model Training Time


>[!NOTE]
>Uno de los requisitos no funcionales clave relacionados con el rendimiento.
>Se trata del tiempo que lleva entrenar un modelo.
>Y lo que puedo decirles es que para ChatGPT, que es, por supuesto, el modelo m√°s infame,
>se tarda aproximadamente un mes en entrenarlo, y no tienen que creerme.
>
>![Supervised ML](images/2025-08-27_111642.png "APP Creation (Supervised ML)")
>
>Busquen en internet el tiempo necesario para entrenar un ChatGPT.
>Entrenar el modelo lleva much√≠simo tiempo.
>Porque al entrenar un modelo, tambi√©n se trata de los datos y del hiperpar√°metro.
>
>As√≠ que modifican el hiperpar√°metro de su modelo para encontrar el equilibrio adecuado entre costo, rendimiento, precisi√≥n, sobreajuste, subajuste, etc.
>Un aspecto clave, si est√°s probando tu modelo, es el tiempo de entrenamiento, ya que esto implica el costo del entrenamiento. Esto se refleja directamente en esto, ya que, si creas algo muy complejo, necesitar√°s una GPU especializada.
>
>Ya he mencionado el tiempo de entrenamiento.
>El modelo tambi√©n se ve influenciado por el ajuste de hiperpar√°metros.
>Te mostrar√© c√≥mo se ve esto.
>
>![Model Hyperparameters](images/2025-08-27_112526.png "Model Hyperparameters")
>
>Me refiero al par√°metro, porque lo que tengo aqu√≠ es muy poca informaci√≥n sobre los dos tipos de hiperpar√°metros.
>El primer tipo, que vemos arriba, es para la red neuronal.
>Es para el modelo.
>En este caso, nuevamente, tenemos el tama√±o de entrada del n√∫mero de caracter√≠sticas.
>
>Si se usa alguna clasificaci√≥n, significa que tengo diez caracter√≠sticas diferentes.
>Por ejemplo, una casa.</br>
>Podr√≠a tener diez caracter√≠sticas.
>N√∫mero de habitaciones.</br>
>Ahora, ya sabes, superficie, etc., diez.
>Y la salida ser√° uno.</br>
>Y tendr√° 32 nodos ocultos adem√°s de la entrada y la salida.
>
>Este es el hiperpar√°metro de la red neuronal.
>Si se ve abajo, se encuentra el par√°metro de entrenamiento, que configura el modelo para entrenar el n√∫mero de √©pocas.
>Por ejemplo, ¬øcu√°ntas veces se revisan todos los datos de entrenamiento?
>
>Si se desea entrenar m√°s r√°pido, en lugar de cinco √©pocas, se podr√≠an usar seis. Si se desea entrenar m√°s, este hiperpar√°metro tambi√©n influye en el tiempo que tarda el modelo en revisar todos los datos.
>Y, por supuesto, hay muchos otros hiperpar√°metros.
>Su tasa de aprendizaje ser√° una funci√≥n de p√©rdida.
>Hay muchos par√°metros.
>
>Y, dependiendo de c√≥mo se configuren, influir√°n en el tiempo de entrenamiento del modelo.
>Un aspecto y una caracter√≠stica muy importante de un modelo de IA es el tiempo de entrenamiento.
>Estoy bastante seguro de que no has o√≠do hablar del tiempo que tarda en compilarse una aplicaci√≥n, y nadie se fija mucho en eso.
>
>Quiz√°s algunos equipos de desarrollo, pero, bueno, nadie dice: "¬°Guau! ¬°Se tarda una semana en compilar nuestra aplicaci√≥n!".
>Eso no sucede.
>Pero en el caso de los modelos, bueno, este es un criterio muy importante que se debe considerar espec√≠ficamente.


### 50. Criteria 2 - Inference Time

>[!NOTE]
>
>**¬øY qu√© hay del tiempo de inferencia?**
>
>Inferencia se refiere al tiempo que el modelo tarda en dar la predicci√≥n.
>En el caso de los modelos de clasificaci√≥n, por ejemplo, ¬øpodr√≠as decirme qu√© hay en esta imagen?
>Podr√≠as obtener una casa, un coche, etc.
>Es casi instant√°neo, pero en el caso de la IA generativa, tarda bastante.
>
>Por lo tanto, algo que debes medir es el tiempo de tu inferencia, o el tiempo que tu modelo de IA generativa detiene la generaci√≥n.
>Y, por supuesto, como puedes imaginar, esto podr√≠a ser problem√°tico dependiendo de la aplicaci√≥n que est√©s desarrollando.
>Porque si est√°s realizando una investigaci√≥n exhaustiva‚Ä¶
>
>Imaginemos que estoy usando un Gemini, pensando experimentalmente.
>O podr√≠a estar usando‚Ä¶ digamos este, y dir√≠a que es agua l√≠quida.
>Perm√≠teme hacerte una pregunta.
>Y como ven, ahora estoy razonando, pero a√∫n no obtengo la respuesta.
>Y ya obtuve la respuesta.
>
>Pero estoy bastante seguro de que si preguntas: "¬øCu√°l es el inter√©s compuesto de... no s√© qu√© cantidad de dinero ajustada a la inflaci√≥n, etc.?",
>en los pr√≥ximos a√±os, razonar√° mucho m√°s.
>
>Entonces, la pregunta es: ¬øes importante este valor?
>La respuesta depende de la aplicaci√≥n que est√©s desarrollando.
>Porque si est√°s desarrollando un chatbot, es muy importante.
>Necesitas una comunicaci√≥n pr√°cticamente en tiempo real.
>No quieres esperar a que tu chatbot razone dos minutos para luego darte una respuesta.
>No.
>
>As√≠ que quieres que el chatbot razone muy r√°pido.
>Y, por supuesto, si quieres probar esta informaci√≥n o este tiempo, realizar cualquier tipo de prueba de rendimiento y luego enviar solicitudes, aseg√∫rate de enviar una solicitud diferente, ya que ChatGPT y otras API no tienen memoria, pero s√≠ almacenamiento en cach√©.
>
>Te devolver√°n todo esto como respuesta o te enviar√°n diferentes indicaciones con el mismo tama√±o de token.
>Estoy bastante seguro de que podr√°s probar el tiempo de respuesta.
>Y, por supuesto, el tiempo de respuesta se ve influenciado por la infraestructura que ejecuta tu modelo, ya que necesitas separar el entrenamiento y la ejecuci√≥n del modelo. Lo entrenas en una infraestructura, pero luego puedes ejecutarlo en tu tel√©fono si tienes un minimodelo, por ejemplo.
>
>As√≠ que recuerda que los modelos de razonamiento piensan despacio y quiz√°s te den una buena respuesta.
>
>Mientras que, por ejemplo, con este flash, dices: "¬øCu√°nto es uno m√°s uno?".
>
>Est√° pensando, pero uso la respuesta como dos, as√≠ que no hay razonamiento.
>As√≠ que el tiempo de inferencia es otro par√°metro que debes considerar al validar cualquier tipo de modelo de IA o IA generativa.


### 51. Criteria 3 - Model Drifting

>[!NOTE]
>
>[![What is model drift?](images/2025-08-27_113419.png "What is model drift?")](https://www.ibm.com/think/topics/model-drift)
>
>Ahora hablaremos sobre el concepto de desviaci√≥n, desviaci√≥n a largo plazo o desviaci√≥n del modelo.
>Para explicarlo m√°s f√°cilmente, voy a visitar IBM, ya que tienen una p√°gina relacionada con esto.
>Entonces, ¬øqu√© es exactamente una desviaci√≥n en el rendimiento del modelo? Es simplemente una disminuci√≥n en su rendimiento.
>
>En resumen.
>En otras palabras, el modelo pierde precisi√≥n con el paso del tiempo.
>
>Esto podr√≠a deberse a dos razones:
>El modelo aprende de sus interacciones y de las interacciones con...
>
>Por ejemplo, imaginemos que ChatGPT interact√∫a con muchas personas, y si aprende de estas interacciones, algunas personas podr√≠an decir: "Bien hecho por algo que no funciona bien", y el modelo decaer√° porque incorporar√° datos err√≥neos en su mecanismo de aprendizaje reforzado.
>
>Otra raz√≥n por la que un modelo decaer√° o se desviar√° es porque se generan nuevos datos.
>Porque sabemos que el modelo tiene informaci√≥n sobre los datos del tren.
>
>Pero si se incorporan nuevos datos a la ecuaci√≥n, por ejemplo, si se tiene un modelo que predice el precio de la vivienda bas√°ndose en diez par√°metros, pero ahora todos los usuarios del mercado utilizan once, el modelo se deteriorar√° porque no ser√° tan preciso como los modelos que proporcionan once par√°metros o simplemente porque el modelo utiliza datos del a√±o pasado, por lo que desconoce los datos de este a√±o.
>
>Por lo tanto, el modelo se deteriora.
>La deriva del modelo en s√≠ misma significa que el modelo pierde precisi√≥n con el tiempo.
>Entonces, ¬øqu√© se hace en este caso?
>
>En primer lugar, es necesario realizar un seguimiento continuo, por lo que se debe supervisar el rendimiento del modelo si se cuenta con un conjunto de datos de referencia que siempre se utiliza para probar el modelo.
>
>Lo que podr√≠a hacer es evaluar semanalmente, quiz√°s mensualmente o incluso con frecuencia, seg√∫n sus necesidades, el rendimiento del modelo y asegurarse de que el par√°metro, si el modelo ha deca√≠do o las respuestas ya no alcanzan su nivel de precisi√≥n, deber√≠a entrenarlo con los datos m√°s recientes disponibles.
>
>Por lo tanto, este es un requisito no funcional, que podr√≠a decirse funcional o no funcional seg√∫n c√≥mo se mire.
>Porque ahora el modelo funciona, pero no con la precisi√≥n que deber√≠a.
>As√≠ que monitoree su modelo.
>Comp√°relo o pru√©belo con un punto de referencia.
>Y si las cosas no van bien, lo que debe hacer es volver a entrenar su modelo, ya que la parte de hiperpar√°metros est√° bien.



### 52. Criteria 4 - Infrastructure to run Model - Model Distillation

>[!NOTE]
>
>[![What is Model Distillation?](images/2025-08-27_114051.png "What is Model Distillation?")](https://labelbox.com/guides/model-distillation/)
>
>Abordemos la idea de cu√°nta infraestructura necesito para ejecutar mi modelo.
>Si no lo est√°s o no est√°s familiarizado con las pruebas de rendimiento, debes saber que hoy en d√≠a las aplicaciones se prueban para comprobar su rendimiento.
>
>Para determinar cu√°nta potencia de procesamiento necesito para un n√∫mero espec√≠fico de usuarios o para un volumen de datos determinado, etc.
>Existen pruebas de carga, pruebas de estr√©s, etc.
>
>**¬øY por qu√© hacemos esto?**
>
>Porque queremos saber qu√© tipo de infraestructura necesitamos para que una aplicaci√≥n se ejecute con un par√°metro espec√≠fico, que generalmente es el tiempo de salida.
>Entonces, ¬øcu√°nto tiempo tarda mi aplicaci√≥n en realizar una acci√≥n determinada?
>
>Crear un usuario y proporcionarme la respuesta.
>Tambi√©n necesitamos hacer esto en el caso de la IA, y por eso lo estoy mencionando aqu√≠.
>
>**Destilaci√≥n del modelo.**
>
>Porque tenemos modelos muy grandes, como GPT y Gemini, y modelos muy peque√±os, como el modelo Flash, por ejemplo.
>Y esta es la raz√≥n por la que tenemos modelos grandes y peque√±os: queremos poder ejecutar IA sin una infraestructura especial y dedicada.
>Y por esta raz√≥n, nosotros o quienes trabajan con IA han creado estos agentes realmente peque√±os.
>
>**¬øPero c√≥mo lo han logrado?**
>
>Usando un concepto llamado destilaci√≥n de modelos.
>Y no entrar√© en detalles sobre c√≥mo se hace.
>Hay informaci√≥n en l√≠nea.
>Puedo decirles que para realizar la destilaci√≥n de modelos, solo necesitan tener un modelo de profesor.
>
>Imaginemos que va a ser un Google Gemini y se necesita un modelo de estudiante, que es Gemini Flash.
>Y luego, mediante diferentes mecanismos, pueden ense√±ar a sus estudiantes.
>O bien, el estudiante puede recibir clases del agente de IA docente hasta que adquiera aproximadamente el 80 % del conocimiento con tan solo el 1 % de la potencia de c√°lculo.
>
>Existen diferentes t√©cnicas para realizar la destilaci√≥n de modelos.
>Puedes encontrarlas en l√≠nea.
>Pero hay algo que quiero mostrarte.
>
>Pero hay algo que quiero mostrarles.
>De Hugging Face, tenemos este peque√±o modelo, que se divide en tres modelos diferentes.
>Hoy en d√≠a, los modelos muy grandes tienen casi un bill√≥n de par√°metros, casi 700 mil millones, etc.
>Pero el agente peque√±o tiene tres versiones: 145 millones, tres 60.000.001,7 mil millones de par√°metros.
>
>**¬øY c√≥mo se puede ejecutar esto?**
>
>Bueno, si busco aqu√≠, digamos "tel√©fono", pueden ver que este modelo se puede ejecutar en un iPhone 15 con seis gigabytes de RAM.
>As√≠ de peque√±os son.
>Repito, no se pueden entrenar estos modelos en el tel√©fono.
>Es imposible.
>
>No se necesitan... quiero decir, se podr√≠a hacerlo con una vida √∫til de 1000 a√±os.
>Pero s√≠ se puede ejecutar el modelo en el tel√©fono.
>Por eso, otra caracter√≠stica muy importante que se debe medir es qu√© tipo de CPU o GPU necesito para ejecutar mi modelo localmente.
>¬øY cu√°l es el tiempo de respuesta que obtengo al ejecutar mi modelo localmente?


### 53. Criteria 5 - Memory and Token Limits


>[!NOTE]
>
>[![Memory and new controls for ChatGPT](images/2025-08-27_115043.png "Memory and new controls for ChatGPT")](https://openai.com/index/memory-and-new-controls-for-chatgpt/)
>
>Echa un vistazo a este t√≠tulo, la memoria y los nuevos controles para ChatGPT.
>
>**¬øQu√© significa esto?**
>
>Significa que los chats recuerdan lo que hemos hablado con ellos.
>Tienen memoria.
>Y te lo puedo demostrar ahora mismo.
>
>Voy a ir a Egipto.
>Seleccionar√© a mi modelo de foto y preguntar√©: "¬øCu√°l es la f√≥rmula del agua?".
>Y luego har√© una pregunta de seguimiento: ¬øpara qu√© sirve?
>La idea es que t√∫ hiciste la pregunta y yo obtuve la respuesta.
>Y luego, un ChatGPT tiene el contexto de la pregunta anterior.
>Y luego tengo una especie de pregunta de seguimiento.
>Pero en detalle.
>
>Lo que no sabes es que cada vez que env√≠as una nueva solicitud, todo lo que has discutido en este chat se env√≠a de vuelta a ChatGPT.
>Todas las conversaciones que has tenido, todas las indicaciones que has enviado y todas las respuestas se env√≠an junto con cada pregunta que haces.
>Esto no significa que se almacenen en la memoria.
>
>Significa que toda la conversaci√≥n se env√≠a de vuelta a ChatGPT para que tenga el contexto de lo que est√°s hablando.
>Esta es la idea de la memoria, pero ¬øcu√°nta memoria tienes?
>Y aqu√≠ es donde debemos pensar en los tokens.
>
>Copio esto y vayamos al tokenizador de la API abierta.
>Voy a obtener esto y copiar la respuesta.
>Hemos usado 256 tokens para obtener la respuesta.
>Esta es la respuesta.
>
>Por lo tanto, siempre que quieras trabajar con la API, debes recordar que no tienen memoria y que debes gestionarla manualmente si quieres probar algo en ella.
>Un atributo no funcional de cualquier modelo de lenguaje grande se refiere a los tokens de entrada y salida.
>
>Aqu√≠ en Gemini tenemos una peque√±a comparaci√≥n:
>ChatGPT turbo.
>Sabes, Cloud Three Deep Seek y todos los dem√°s tienen diferentes l√≠mites de tokens, y t√∫ tienes un l√≠mite de tokens de entrada y salida constantemente.
>
>Hay un l√≠mite de tokens de entrada y un l√≠mite de tokens de salida.
>
>Por eso, los diferentes modelos son buenos para diferentes tareas.
>Por lo tanto, al realizar cualquier tipo de prueba,
>recuerda tambi√©n superar el l√≠mite de tokens para alcanzar tu l√≠mite de tokens.
>
>Y as√≠ sucesivamente, para ver qu√© sucede.
>Por ejemplo, Leipzig tiene 64 contextos y una salida de 8 K.
>Entonces, lo cual es ocho veces menor que la salida, y as√≠ sucesivamente.
>
>Ahora, para mostrarles que no hay memoria en la API, tengo un chatbot.
>B√°sicamente, est√° llamando a ChatGPT.
>Lo ejecutar√© y preguntar√©: "¬øCu√°l es la f√≥rmula del agua?".
>
>Obtendr√© una respuesta.
>La f√≥rmula qu√≠mica es H‚ÇÇO.
>Y luego dir√© para qu√© sirve.
>No quiero especificar agua porque es transparente.
>
>Quiero hacer la pregunta de seguimiento: ¬øpara qu√© sirve?
>
>Como puedo realizar diversas tareas, como responder preguntas, etc.
>No hay memoria.
>As√≠ que si digo algo que seguir√° aqu√≠, obtendr√© lo mismo.
>No funciona.
>Por eso es importante pensar en la memoria, el contexto y los tokens de entrada y salida, ya que necesitas trabajar con ellos para obtener una respuesta adecuada.
>
>Como puedes ver, tambi√©n puedes definir cu√°ntos tokens, el m√°ximo de tokens que quieres que tu modelo responda.
>En caso de que haya una respuesta muy grande y no quieras 200 o 500 tokens, o lo que sea, solo cinco o no s√©.
>Tambi√©n puedes configurarlo.
>Bien.
>
>Esto se trata de la memoria.
>Se prueba como cualquier otro an√°lisis de valor l√≠mite.
>Se va por debajo del margen que sobrepasa el l√≠mite.
>Y, por supuesto, puedes ser creativo con eso.



## Section 9: GLUE - Benchmark against NLP

### 54. What is GLUE NLP Benchmark

>[!NOTE]
>
**>[<img alt="" src="https://gluebenchmark.com/assets/images/glue_icon_blue.svg" widht="20px" height="20px"/> GLUE](https://gluebenchmark.com/) _The General Language Understanding Evaluation_**
>
>El benchmark de Evaluaci√≥n de la Comprensi√≥n del Lenguaje General (GLUE) es una colecci√≥n de recursos para entrenar, evaluar y analizar sistemas de comprensi√≥n del lenguaje natural. GLUE consta de:
>
>* Un benchmark de nueve tareas de comprensi√≥n del lenguaje, basadas en oraciones o pares de oraciones, basado en conjuntos de datos ya establecidos y seleccionados para abarcar una amplia gama de tama√±os, g√©neros textuales y grados de dificultad;
>* Un conjunto de datos de diagn√≥stico dise√±ado para evaluar y analizar el rendimiento del modelo con respecto a una amplia gama de fen√≥menos ling√º√≠sticos presentes en el lenguaje natural; y
>* Una tabla de clasificaci√≥n p√∫blica para el seguimiento del rendimiento en el benchmark y un panel para visualizar el rendimiento de los modelos en el conjunto de diagn√≥stico.
>
>El formato del benchmark GLUE es independiente del modelo, por lo que cualquier sistema capaz de procesar oraciones y pares de oraciones y generar las predicciones correspondientes puede participar. Las tareas del benchmark se seleccionan para favorecer los modelos que comparten informaci√≥n entre tareas mediante la compartici√≥n de par√°metros u otras t√©cnicas de aprendizaje por transferencia. El objetivo final de GLUE es impulsar la investigaci√≥n en el desarrollo de sistemas generales y robustos de comprensi√≥n del lenguaje natural.
>
><img alt="Owner avatar nyu-mll" src="https://avatars.githubusercontent.com/u/25018927?s=48&amp;v=4" width="20" height="20">[nyu-mll/GLUE-baselines](https://github.com/nyu-mll/GLUE-baselines)
>
>Si vas a la secci√≥n de c√≥digo, te llevar√° a este repositorio de GitHub.
>Aqu√≠ tienes el entorno para descargar los datos de Glue y el c√≥digo fuente.
>Dado que hay una advertencia de obsolescencia, no usar√© nada de lo que est√° aqu√≠.
>Escribir√© mi propio c√≥digo porque lo que necesitamos son los datos.
>
>Tambi√©n puedes ver, por ejemplo, en este entorno el archivo YAML, que puedes usar si usas una herramienta como la que mostr√© al principio para aislar tu entorno. Aqu√≠ te indica lo que necesitas.
>Pero, como est√° obsoleta, existen bibliotecas m√°s nuevas que tambi√©n usaremos.
>Si quieres obtener los datos, aqu√≠ tienes los enlaces para descargarlos.
>Aqu√≠ tienes la ruta para todos los conjuntos de datos que necesitamos.
>
>![GLUE -> leaderboard](images/2025-08-29_085745.png "GLUE -> leaderboard")
>
>Y luego, aqu√≠ tambi√©n tienen las tareas que cubriremos en una lecci√≥n aparte: la tabla de clasificaci√≥n.
>La tabla de clasificaci√≥n significa, como ven, cu√°l es el mejor modelo.
>
>¬øQu√© modelo obtiene la puntuaci√≥n m√°s alta con respecto a esto? No podr√°n obtener un resultado al ejecutar la prueba comparativa con una prueba real. Necesitan obtener sus resultados y enviarlos a la gente aqu√≠ en un formato espec√≠fico.
>
>Y solo despu√©s de eso podr√°n ver su puntuaci√≥n en el video de capacitaci√≥n que mostrar√©.
>Cuando ejecutemos la demostraci√≥n real, la ejecutaremos contra un conjunto de entrenamiento que tambi√©n contiene los resultados.
>As√≠ es como calculamos, porque para ejecutar contra el conjunto real, no se tienen las predicciones.
>
>No se tienen los valores de verdad fundamentales.
>Por lo tanto, nunca podr√°n acceder a su propio dispositivo.
>La puntuaci√≥n real la obtendr√°s √∫nicamente con los datos de entrenamiento.


### 55. What are the 11 benchmark Tasks of GLUE

>[!NOTE]
>
>![GLUE -> tasks](images/2025-08-29_101503.png "GLUE -> tasks")
>
>En el video anterior, les coment√© que tenemos el punto de referencia de Glue, y este es el que se utiliza.
>Este punto de referencia se evaluar√° con varias tareas que, como saben, el usuario puede comprender y evaluar.
>
>Originalmente eran ocho, pero ahora son cerca de once tareas diferentes.
>
>Y este es el formato.
>As√≠ que este es el nombre de la tarea.
>Aqu√≠ pueden descargar los datos.
>Y esta es la m√©trica, ya que existen diferentes m√©tricas.
>
>Por ejemplo, esta m√©trica se relaciona con la precisi√≥n gramatical.
>Conocemos F1.
>Y luego hay otras m√©tricas que se discutir√°n a lo largo de este material de capacitaci√≥n.
>
>![CoLa -> The Corpus of Linguistic Acceptability](images/2025-08-29_101858.png "CoLa -> The Corpus of Linguistic Acceptability")
>
>Por ejemplo, veamos esta tarea, la de la cola.
>
>Si buscas m√°s informaci√≥n, ver√°s que esa columna representa el corpus de aceptabilidad ling√º√≠stica.
>Si quieres leer la investigaci√≥n que la respalda, ve aqu√≠ y ver√°s los juicios de aceptabilidad de las redes neuronales.
>Tambi√©n ver√°s qu√© valida exactamente esta tarea.
>
>Investiga la capacidad de las redes neuronales artificiales para juzgar la aceptabilidad gramatical de una oraci√≥n, con el objetivo de evaluar su competencia ling√º√≠stica.
>
>No har√© esto para todas las tareas.
>Est√° bastante claro.
>Las tareas que tenemos...
>Era la de la cola, que b√°sicamente determina si un texto es gramaticalmente correcto.
>
>Luego est√° el Stanford Sentment Treebank, que clasifica si un sentimiento es positivo o negativo.
>As√≠ que est√°s hablando y el modelo identifica si lo que dices tiene una vibra positiva o negativa.
>Microsoft.
>Eh, ahora continuemos.
>Eh, par√°metro de similitud textual sem√°ntica.
>
>Esto mide la similitud sem√°ntica entre dos oraciones en una escala continua.
>Eh, Winograd NLI, que significa inferencia del lenguaje natural, resuelve las referencias de pronombres en oraciones
>con ambig√ºedad.
>Eh, digamos antecedentes.
>
>Esto significa que est√°s diciendo dos frases y en un momento determinado, por ejemplo, a Andr√©s le gusta el agua caliente o el t√©.
>
>Y en la siguiente oraci√≥n dices que le gusta mucho tomar t√©, que no s√© qu√©.
>As√≠ que esto valida si se refiere a Andr√©s ahora.
>B√°sicamente, hay muchas otras tareas, por ejemplo, reconocer la implicaci√≥n textual.
>Esto determina si la oraci√≥n se sigue original o l√≥gicamente de otra.
>Entonces, si de lo que est√°s hablando‚Ä¶
>Tiene sentido l√≥gico.
>
>Lo que est√°s expresando.
>
>Eh, ¬øsoy...? Por ejemplo, esta es una inferencia de lenguaje natural multigen√©tica que juzga la relaci√≥n entre una premisa y la hip√≥tesis.
>As√≠ que te dice que esta es la premisa, esta es una hip√≥tesis, y as√≠ sucesivamente.
>Estas son las tareas de enlace.
>Cuando expresas algo, est√°s diciendo algo.
>
>As√≠ es como se validar√° el modelo frente a estos puntos de referencia.
>Y lo realmente interesante es que puedes realizar pruebas independientes para cada tipo de tarea.
>No necesitas ejecutar todo porque eso llevar√≠a un tiempo.
>Pero puedes simplemente decir: "Oye, quiero ejecutar contra pares de preguntas de Quora, por ejemplo, contra esta, para poder probar contra pares de preguntas de Quora".
>Y por cierto, Qqp detecta si dos preguntas son par√°frasis la una de la otra.
>
>Y pueden ver, por ejemplo, que esta tiene F1 y precisi√≥n como m√©tricas, y no indican nada sobre la recuperaci√≥n, etc.
>Porque estas son las m√©tricas m√°s importantes para este tipo de evaluaci√≥n.
>As√≠ que esta es, en realidad, la tarea de enlace y los datos, y c√≥mo los obtendremos.
>Les mostrar√© en el video donde hablamos de nuestro c√≥digo.
>Pero por ahora, deben saber que esto es para la comprensi√≥n del lenguaje natural.
>Hay 11 tipos de puntos de referencia.
>Pueden obtener los datos de todos ellos.
>Pueden obtener los datos de uno de ellos.
>
>[![datasets/nyu-mll/glue](images/2025-08-29_102838.png "datasets/nyu-mll/glue")](https://huggingface.co/datasets/nyu-mll/glue)
>
>Tambi√©n puedes encontrar los datos aqu√≠ mismo, en la `Hugging Face`.
>
>Y puedes encontrar las etiquetas de todos los datos que quieras.
>Y si quieres saber qu√© hace cada una de estas, digamos, tareas, tambi√©n puedes encontrarlas aqu√≠.
>Ahora todo est√° aqu√≠.
>Eso es todo.
>Ah, y por cierto, s√≠, s√≠.
>Muy importante.
>
>Olvid√© mencionar que est√° en ingl√©s.
>
>As√≠ que todas las preguntas que hagas deben estar en ingl√©s.
>No significa que no tenga valor traducirlas con una m√°quina del ingl√©s a otro idioma,
>y hacer las preguntas en otro idioma, porque la traducci√≥n podr√≠a no ser correcta.
>As√≠ que, si haces esto, siempre necesitas usar estos datos, que est√°n en ingl√©s.
>
>Y el modelo que est√°s validando debe estar entrenado en ingl√©s. Dicho esto, nos vemos en la parte donde realmente codificamos y ejecutamos nuestra prueba.
>
>Repito que los datos se usan para entrenamiento, no para benchmarking, porque los datos de benchmarking o, digamos, los datos de prueba solo contienen las predicciones, no la tabla de verdad fundamental.
>Y luego, quienes mantienen esta tabla de clasificaci√≥n te dir√°n tu puntuaci√≥n.


### 56. How to run a GLUE benchmark test

>[!NOTE]
>
>[How to run a GLUE benchmark test](videos/2025-08-29_103338.mp4)
>


### 57. Glue benchmarking on Bert Huggingface Model

1. Activamos el Ambiente Virtual de Python con el comando: </br> `.venv/Scripts/activate`
2. Instalamos las librer√≠as faltantes: </br> `pip install seaborn` </br> ¬ª [seaborn: statistical data visualization](https://pypi.org/project/seaborn/)
3. Creamos el siguiente archivo **`src/test/LLM/GLUE/evaluate_glue.py`**, con este c√≥digo inicial:
```py
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import load_dataset
import evaluate

print("1. Load the BERT-base model and tokenizer")
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, num_labels=2)

print("2. Load the GLUE dataset (e.g. SST-2 fror sentiment analysis)")
task = "sst2"  # Change this for other GLUE tasks (e.e, 'mrpc', 'qqp')
dataset = load_dataset("glue", task)


print("3. Preprocess the dataset")
def preprocess_function(examples):
    return tokenizer(examples["sentence"], truncation=True, padding="max_length", max_length=128)


encoded_dataset = dataset.map(preprocess_function, batched=True)

print("4. Load evaluation metric using 'evaluate'")
metric = evaluate.load("glue", task)
```
4. Continuo A√±adiendo pasos del 5 al 8:
```py
print("5. Define a function to compute the metrics")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=1)
    return metric.compute(predictions=predictions, references=labels)


print("6. Create a DataCollaator for dynamic padding")
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

print("7. Prepare evaluation aguments")
training_args = TrainingArguments(
    output_dir=current_path+"results",
    per_device_eval_batch_size=64,
    logging_dir=current_path+"logs",
    do_train=False,  # No Trainings
    do_eval=True,  # Only evaluation
)

print("8. Initialize Trainer fro evaluation")
trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=encoded_dataset["validation"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,  # Use data collator instead of tokenizer
)
```
5. A√±ado los pasos restantes:
```py
print("9. Evaluate all model on the evaluation set")
results = trainer.evaluate()

# DEBUG: Print all metrics  returned by the evaluation
print("Evaluation results:", results)

print("10. Filter GLUE-specific metrics")
exclude_metrics = ["eval_loss", "eval_runtime", "eval_samples_per_second",
                   "eval_steps_per_second", "eval_model_prepare_time"]
glue_metrics = {k: v for k, v in results.items() if k not in exclude_metrics}

print("Convert GLUE metrics to a DataFrame for visulization")
result_df = pd.DataFrame.from_dict(
    glue_metrics, orient="index", columns=["value"])
result_df.reset_index(inplace=True)
result_df.columns = ["Metric", "Value"]

# DEBUG: Ensure DataFrame is populated correcytly
print("Filtered Results for Graph:", result_df)

print("11. Plot the GLUE metrics")
plt.figure(figsize=(10,6))
sns.barplot(x="Metric", y="Value", data=result_df, palette="viridis")
plt.title(f"GLUE Evaluation Results for Tasks: {task.upper()}", fontsize=16)
plt.xlabel("Metrics", fontsize=14)
plt.ylabel("Values", fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.tight_layout()
plt.show()
```
6. En la `TERMINAL` con el Ambiente Virtual de Python, ehecuto este comando: </br> `python ./src/test/LLM/GLUE/evaluate_glue.py`
7. Al terminal aparece esta imagen en pantalla: </br> ![GLUE Evaluation Results for Tasks SST2](images/2025-08-31_145809.png "GLUE Evaluation Results for Tasks SST2")
8. Cierro esta ventana y finaliza el proceso en Python.
9. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`



### 58. Demo - Python GLUE benchmark against GHATGPT

>[!NOTE]
>
>Por diversi√≥n, quiero mostrarles c√≥mo comparar ChatGPT con el benchmark Glue SST two.
>
>Una forma de hacerlo, y es bastante divertida, es hacer lo siguiente:
>Acceden a ChatGPT y preguntan cu√°l es el sentimiento de esta frase.
>Por cierto, esto est√° tomado de la tarea two.
>
>Una cadena que, finalmente, transporta una reinvenci√≥n de La Bella y la Bestia y pel√≠culas de terror de los 90 y 40.
>¬øEs positivo o negativo?
>No, es positivo.
>Pueden hacerlo manualmente, solo por diversi√≥n, pero con suerte no lo har√°n.
>
>![Chat GPT -> Sentiment Analysis](images/2025-08-31_150420.png "Chat GPT -> Sentiment Analysis")
>
>

1. Creamos el archivo **`src/test/LLM/GLUE/glue_gpt.py`**.
2. Activamos el Ambiente Virtual de Python con el comando: </br> `.venv/Scripts/activate`
3. Empezamos con las importaciones necesarias:
```py
import os
from dotenv import load_dotenv  # Load environment variables
from openai import OpenAI
import numpy as np
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
```
4. Inicializa el cliente de `OpenAI`:
```py
print("1. Initialize OpenAI client")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    load_dotenv()  # Carga las variables de entorno del archivo .env
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# print("OpenAI API Key:", OPENAI_API_KEY)

# Instantiate OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY)
```
5. Funci√≥n para la respuesta con base en el _prompt_:
```py
def get_openai_response(prompt):
    """
    Sends a prompt to OpenAi's API using the updateaded OPENAI client retrieves the response
    """
    try:
        completion = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        chat_reponse = completion.choices[0].message.content.strip()
        return chat_reponse
    except Exception as e:
        print(f"An error ccurred: {e}")
        return None
```
6. Funci√≥n para evaluar los `SST-2` de los primeros 50 ejemplos:
```py
def evaluate_ssts_first_50():
    """
    Evaluate the SST-2 taks using the OpenAI API and computes metrics for the first 50 validations examples.
    """
    print("Load the SST-2 dataset")
    dataset = load_dataset("glue", "sst2")
    validation_set = dataset["validation"]

    print("Ensure we are iterating over dicctionaries (convert to a list fo dictionaries)")
    validation_examples = list(validation_set)

    print("Store predictions and labels")
    predictions = []
    labels = []

    print("Evaluate only the first 50 examples")
    for idx, example in enumerate(validation_examples[:50]):
        print("Ensure 'sentence' is accessed correctly")
        prompt = f"Classify the sentiment of the following sendentece as poistive or negative: \"{example['sentence']}\""

        print("Get the model's prediction using OpenAI API")
        response = get_openai_response(prompt=prompt)

        # Show sprompt and response
        print(f"Prompt ({idx+1}): {prompt}")
        print(f"Response: {response}")

        if response is not None:
            print("Convert the response to a label (e.g., 0 or 1)")
            if "positive" in response.lower():
                predictions.append(1)
            elif "negative" in response.lower():
                predictions.append(0)
            else:
                print(f"Unexpected response: {response}")
        else:
            predictions.append(-1)  # HAndle API failures
        print("Store the true label")
        labels.append(example["label"])

    print("Filter out invalid predictions")
    valid_indices = [i for i, pred in enumerate(predictions) if pred != -1]
    valid_predictions = [predictions[i] for i in valid_indices]
    valid_labels = [labels[i] for i in valid_indices]

    print("Compute evaluation metrics")
    accuracy = accuracy_score(valid_labels, valid_predictions)
    precision = precision_score(
        valid_labels, valid_predictions, average="binary")
    recall = recall_score(valid_labels, valid_predictions, average="binary")
    f1 = f1_score(valid_labels, valid_predictions, average="binary")

    print("Display results")
    results = {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1,
    }
    for metric, value in results.items():
        print(f"{metric}: {value:.4f}")
```
7. Al final invoco esta super funci√≥n `evaluate_ssts_first_50()`:
```py
print("run the evaluation for the first 50 SST-2 examples")
if __name__ == "__main__":
    evaluate_ssts_first_50()
```
8. En una `TERMINAL` deonse se ejecute el Ambiente Virtual de Pyton, corro este comando: </br> `python ./src/test/LLM/GLUE/glue_gpt.py`
9. Este es el resultado arrojado al final:
```bash
Compute evaluation metrics
Display results
Accuracy: 0.9400
Precision: 0.9200
Recall: 0.9583
F1-Score: 0.9388
```
10. Una vez terminada la prueba salirnos del Ambiente Virtual de Python con el comando: </br> `deactivate`

